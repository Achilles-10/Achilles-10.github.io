---
title: "百面深度学习：卷积神经网络"
date: 2023-07-26T18:50:49+08:00
lastmod: 2023-07-26T18:50:49+08:00
author: ["Achilles"]
# keywords: 
# - 
categories: # 没有分类界面可以不填写
- 
tags: ["面试","学习笔记"] # 标签
description: "卷积神经网络常见面试题"
weight:
slug: ""
draft: true # 是否为草稿
comments: true # 本页面是否显示评论
# reward: true # 打赏
mermaid: true #是否开启mermaid
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示路径
math: true
cover:
    image: "" #图片路径例如：posts/tech/123/123.png
    zoom: # 图片大小，例如填写 50% 表示原图像的一半大小
    caption: "" #图片底部描述
    alt: ""
    relative: false
---

## 一、卷积基础知识

关键词：卷积操作、卷积核、感受野（Receptive Field）、特征图（feature map）、卷积神经网络

### 1. 简述卷积的基本操作，并分析其与全连接层的区别

在卷积操作中，卷积核与输入图像上**滑动**，进行**点乘**操作，然后求和得到一个单个值，这个值作为输出特征图中的一个像素，每个卷积核都会产生一个对应的特征图。

<div align=center><img src="2.png" style="zoom:70%;" /></div>

全连接层的输出层的每个节点与输出层的每个节点都有权重连接，而卷积层具有**局部连接**、**权值共享**和**输入/输出数据结构化**的特点，**参数量**和**计算复杂度**远小于全连接层，并且与生物视觉传导机制有一定的相似性。

* **局部连接**：卷积核尺寸小于输入特征图尺寸，输出层上的每个节点都只有输入层的**部分节点**连接；而全连接层中节点之间的连接是稠密的；
* **权值共享**：由于卷积核的**滑动窗口**机制，输出层不同位置的节点与输入层的连接权值是一样的（卷积核参数）；全连接层中不同节点之间的权值是不同的；
* **输入/输出数据的结构化**：卷积操作不会改变输入数据的结构信息；而全连接层的的输出数据会被展成（flatten）扁平的一维数组；
* **平移不变性**：即对于输入图像中的某个特征，如果在图像中移动它，卷积层仍然可以检测到相同的特征；全连接层通常不具备这种平移不变性；
* **输入尺寸**： 卷积层对于不同尺寸的输入图像都可以处理；全连接层输入的大小是固定的。

### 2. 在卷积神经网络中，如何计算各层的感受野大小

感受野的定义是，对于某层输出特征图上的某个点，在原始输入数据上能影响到这个点的取值的区域。

* 若第 i 层为卷积层或池化层，不考虑padding，则有：
  $$
  F(i)=[F(i-1)-1]\times s+K\\\\ K=(k-1)\times \text{dilation}+1=(r-1)*(k-1)+k
  $$
  其中，s 表示 stride，k 表示 kernel_size，$F(0)=1$。

* 若第 i 层为激活层、归一化层，则步长为1，感受野大小 $F(i)=F(i-1)$。

* 若第 i 层为全连接层，则感受野为整个输入数据全域，即 $F(i)=L_{input}$

<div align=center><img src="1.png" style="zoom:65%;" /></div>

### 3. 卷积层的输出尺寸、参数量和计算量

* 输出尺寸：
  $$
  H_{out}=\lfloor\frac{H_{in}+2\times p-K}{s}\rfloor+1
  $$
  其中，p 为 padding，s 为 stride，$K=(k-1)\times\text{dilation}+1$，k 为 kernel_size。

  向下取整是放弃了左上侧的部分数据，使得卷积核滑动窗能够恰好到达右下角的点。

  * padding=same：在特征图上下/左右共填充 k-1 行/列，此时输出 $H_{out}=\lfloor\frac{H_{in}-1}{s}\rfloor+1$
  * padding=valid：不对输入特征图进行边界填充，直接放弃右下侧卷积核无法滑动到的区域。

  > **padding 的作用**：
  >
  > 1. 避免边缘信息丢失
  > 2. 保持特征图大小
  > 3. 控制输出大小

* 参数量：

  每个卷积核的参数量为 $C_{in}K_wK_h+1$ ，1 表示偏置，若不考虑偏置则可以忽略。有 $C_{out}$ 个卷积核，则参数量为：
  $$
  C_{out}(C_{in}K_wK_h+1)
  $$

* 计算量：

  卷积层的计算量主要取决于三个因素：输出特征图的尺寸、卷积核的尺寸和输出通道数。

  卷积操作的计算量大约为 $C_{in}K_wK_h$，卷积核滑动的次数为输出特征图的数据个数，即 $C_{out}L_wL_h$，因此整体的计算量为：
  $$
  C_{in}C_{out}L_w^{(o)}L_h^{(o)}K_wK_h
  $$

## 二、卷积的变种

关键词：分组卷积（Group Convolution）、转置卷积（Transposed Convolution）、空洞卷积（Dilated Convolution）、可变形卷积（Deformable Convolution）

### 1. 简述分组卷积及其应用场景

在普通卷积中，由于一个卷积核对应输出特征图的一个通道，而每个卷积核会作用到输入特征图的所有通道上，因此普通卷积在“通道”这个维度上是“**全连接**”的。

分组卷积是将输入通道和和输出通道划分同样的组数，仅让处于相同组号的输入输出通道相互进行“全连接”。若记 g 为划分的组数，则分组卷积能降低计算量和参数量为原来的 1/g。

<div align=center><img src="3.png" style="zoom:80%;" /></div>

分组卷积最初在 AlexNet 网络中引入，用于解决单个 GPU 无法处理含有较大计算量和存储需求的卷积层这个问题，用分组卷积将计算和存储分配到多个GPU上**并行计算**。目前分组卷积更多用于构建**小型网络**模型，例如**深度可分离卷积**（depthwise ）。

分组卷积潜在的问题：

* 效率提升不如理论上明显：对内存的访问频繁程度并未降低，且现有 GPU 加速库（cuDNN）对其优化程度有限；

* 信息交互弱：因为它独立地处理每个组，可能无法充分利用输入通道之间的相关性。

> **深度可分离卷积（Depthwise Separable Convolution）**：
>
> 将标准的卷积操作分解为两个步骤：深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）：
>
> 1. **深度卷积（Depthwise Convolution）：** 深度卷积独立地对输入的每个通道进行卷积操作，对于输入通道数为C的输入特征图，它使用C个卷积核进行卷积。参数量为 $C_{in}\times K\times K$。
> 2. **逐点卷积（Pointwise Convolution）：** 在深度卷积之后，使用1x1的卷积改变深度卷积输出的通道数。参数量为 $C_{in}\times C_{out}$。
>
> 总的参数量为 $C_{in}\times K\times K + C_{in}\times C_{out}$。
>
> 深度可分离卷积能够**减少参数量和计算量**，**加速推理过程**，**实现轻量级模型和移动端应用**。但同样存在**信息交互弱**的问题，可能对模型的性能产生一定影响。
>
> <div align=center><img src="4.png" style="zoom:50%;" /></div>

### 2. 1x1 卷积的作用

1x1 卷积，也称为逐点卷积（Pointwise Convolution），卷积核大小为 1x1，不考虑输入数据局部信息之间的关系，而把关注点放在不同通道间。

<div align=center><img src="10.png" style="zoom:60%;" /></div>

作用：

1. **降维和增加通道数：** 1x1 卷积可以降低特征图的通道数，从而减少网络的参数量和计算复杂度。同时，它也可以增加通道数，引入更多的特征表示。
2. **特征融合：** 1x1 本质是对每个像素点在不同通道上进行线性组合，可以用于特征的融合，从而提高模型的表征能力。
3. **非线性映射：** 尽管1x1卷积只是在通道上进行线性组合，但之后可以通过非线性激活函数进行非线性映射，增加网络的表达能力。

### 3. 简述转置卷积的主要思想以及应用场景

转置卷积（Transposed Convolution），也称为反卷积（Deconvolution）。主要思想是将卷积操作中的输入和输出交换，从而将低维特征映射扩展到高维特征映射。

转置卷积是通过卷积核的翻转和补零操作来实现的，具体实现时，以二维卷积为例，一个卷积核尺寸为 $k_w\times k_h$，滑动步长 $(s_w,s_h)$，边界填充尺寸为 $(p_w,p_h)$ 的普通卷积：

1. 如果 $s>1$，对输入特征图进行扩张（上采样）：相邻数据点之间填充 $s_{w/h}-1$ 个零；
2. 对输入特征图进行边界填充：对左右/上下两侧分别填充 $\hat{p}_{w/h}=k_{w/h}-p_{w/h}-1$ 个零列/行；
3. 变换后再输入特征图上做卷积核大小为 $k_w\times k_h$，滑动步长为 $(1,1)$ 的普通卷积。

输出特征图大小为 $H_{out} = s\times(H_{in}-1)+k$

<div align=center><img src="7.gif" style="zoom:60%;" /></div>

**转置卷积的应用场景：**

1. **图像分割和语义分割：** 转置卷积可用于将低分辨率的特征图上采样到原始输入图像大小，以生成像素级的分割结果。
2. **图像生成和超分辨率重建：** 在图像生成任务中，例如GAN（生成对抗网络）中的生成器部分，转置卷积用于将输入的随机噪声扩展成高分辨率的图像。同样，转置卷积在超分辨率重建中也能够将低分辨率图像上采样成高分辨率图像。
3. **卷积神经网络中的上采样：** 使用转置卷积进行上采样，提高特征图的尺寸，以便更好地捕捉图像中的细节和全局信息。

### 4. 简述空洞卷积的设计思路

空洞卷积（Dilated Convolution），也称为膨胀卷积或扩张卷积。通过在卷积核中引入**扩张率**（dilation rate）这个超参数来指定相邻采样点之间的间隔：扩张率为 r 的空洞卷积，卷积核上相邻数据点之间有 r-1 个空洞，如下图所示，这是一个卷积核大小为 3，扩张率为 2 的空洞卷积。

<div align=center><img src="6.png" style="zoom:40%;" /></div>

空洞卷积的感受野为 $F = (r-1)*(k-1)+k$，当卷积核大小为 3，扩张率为 2 时，感受野计算方式如下图所示。经过一层空洞卷积感受野为 $3\times 3$，两层空洞卷积后为 $5\times 5$。

<div align=center><img src="8.png" style="zoom:40%;" /></div>

空洞卷积利用空洞结构扩大了卷积尺寸，不经过下采样操作即可增大感受野，同时还保留了输入数据的内部结构，能够捕捉更广阔的上下文信息，从而增强卷积神经网络的感知能力。

### 5. 可变形卷积旨在解决哪类问题

可变形卷积（Deformable Convolution）是一种改进的卷积操作，旨在解决传统卷积在处理不规则形状和变形目标时的不足。可变形卷积通过引入可学习的偏移量来动态调整卷积核的采样位置，从而适应不同目标的变形和不规则形状，提高模型对于复杂场景的表征能力。

如下图所示，（a）是普通卷积和，（b）（c）（d）是可变形卷积核。

<div align=center><img src="9.png" style="zoom:50%;" /></div>

当偏移后的采样点不是整数时，需要用**双线性插值**来计算对应的特征值。
