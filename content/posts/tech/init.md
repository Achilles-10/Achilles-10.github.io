---
title: "深度学习面试题：初始化方法"
date: 2023-08-10T15:43:02+08:00
lastmod: 2023-08-10T15:43:02+08:00
author: ["Achilles"]
# keywords: 
# - 
categories: # 没有分类界面可以不填写
- 
tags: ["面试","学习笔记"] # 标签
description: "深度学习算法岗初始化方法常见面试题"
weight:
slug: ""
draft: false # 是否为草稿
comments: true # 本页面是否显示评论
# reward: true # 打赏
mermaid: true #是否开启mermaid
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示路径
math: true
cover:
    image: "" #图片路径例如：posts/tech/123/123.png
    zoom: # 图片大小，例如填写 50% 表示原图像的一半大小
    caption: "" #图片底部描述
    alt: ""
    relative: false
---

## 1. 什么是初始化方法

### 1.1 什么是网络参数初始化

在训练神级网络之前，对模型的参数进行初始赋值。合适的初始化方法可以加速收敛，提升性能。

### 1.2 常用的初始化方法

* 随机初始化：通过某个分布（高斯分布、均匀分布）来选择初始值来初始化参数，可以破坏对称性，使不同神经元学习到不同的特征；
* 零初始化：将所有参数初始化为零，容易导致网络对称性和梯度消失的问题；
* Xavier 初始化：适用于具有线性激活函数的网络，它根据输入和输出维度来调整参数的初始化范围，以确保梯度的传播保持合适范围，避免梯度爆炸或梯度消失；
* He 初始化：针对具有 ReLU 激活函数的初始化方法，与 Xavier 类似，根据激活函数的性质进行调整以确保合适的梯度传播范围。

## 2. 为什么需要合理的参数初始化

若初始化不合理，可能导致梯度消失或梯度爆炸。

理想的参数初始化是，经过多层网络后，信号不被过分放大或过分减弱。数学的方法就是使每层网络的输入和输出的方差已知，且尽量保证每层网络参数的分布均值为0，方差不变。

## 3. 详细讲解初始化方法

### 3.1 全零初始化

### 3.2 随机初始化

随机参数服从零均值高斯分布、正态分布或均匀分布。
$$
w = 0.001*randn(N_{in},N_{out})
$$
0.001 是控制因子，避免参数过大，也存在其他合理的小数。

### 3.3 Xavier 初始化

随机初始化没有控制方差，要解决梯度消失，需要控制方差，因此对 $w$ 进行规范化：
$$
w = \frac{0.001*randn(N_{in},N_{out})}{\sqrt{n}}\\\\
$$
其中 $n=N_{in}\ or\ \frac{N_{in}+N_{out}}{2}$，维持了输入输出数据分布方差一致性。Xavier 需要配合 BN 和线性激活函数（sigmoid，tanh）。

### 3.4 He 初始化

由何凯明提出，对应非线性初始化函数（ReLU）。当 ReLU 的输入小于 0 时，其输出为 0，相当于该神经元被关闭了，影响了输出的分布模式。

因此 He 初始化，在 Xavier 的基础上，**假设每层网络有一半的神经元被关闭，于是其分布的方差也会变小**。经过验证发现当对初始化值缩小一半时效果最好，故 He 初始化可以认为是 **Xavier 初始 / 2** 的结果。

### 3.5 预训练

- 初始化权重：通过预训练模型的权重，可以提供更好的初始化，使得模型更容易收敛和学习任务特定的特征。
- 迁移学习：通过预训练模型，在目标任务上微调模型可以更快地学习和适应目标任务的特征。这对于具有较小的目标任务数据集的情况非常有用。
- 数据效率：通过使用大规模的预训练数据，可以提取通用的特征表示，从而更好地利用有限的目标任务数据。

## 4. 零初始化有什么问题

将所有权重置零，神经网络通过梯度更新参数，参数都是零，梯度也就是零，神经网络就停止学习了。

## 5. 随机初始化有什么问题

随机初始化没有控制方差，对于深层网络，可能造成梯度消失或梯度爆炸。

## 6. 怎么缓解梯度消失

* 梯度截断，如 ReLU6，将输出在 6 处截断，控制值域在 [0,6]
* 使用残差连接
* 使用 BN 控制数据分布在激活函数合适的区域
* 使用 ReLU、LReLU、ELU、maxout 等激活函数
* 使用合理的参数初始化方法
* 预训练 + 微调

## 7. 梯度消失的根本原因是什么

在深度神经网络求梯度时，由于链式法则，多个小于 1 的梯度相乘，最浅层的网络梯度趋于 0，无法进行更新；并且 Sigmoid 函数也会因为初始权值过小而趋近于 0，导致梯度趋近于 0，也导致了无法更新。

## 8. 说说归一化方法

数据标准化是对数据进行预处理的一种方式，将数据按特征进行缩放，这有助于使不同特征之间的尺度一致，提高模型的**收敛速度和性能**。

* z-score 标准化

  将数据转化为以均值为 0，标准差为 1 的标准正态分布。
  $$
  z=\frac{x-\mu}{\sigma}
  $$

* min-max 标准化

  将数据线性地缩放到给定的范围内，通常是 `[0, 1]` 或 `[-1, 1]`。
  $$
  z=\frac{x-x_{min}}{x_{max}-x_{min}}
  $$

## 9. 为什么要归一化

- 消除特征间的量纲差异：不同特征往往具有不同的度量单位和尺度，例如身高和体重，它们的取值范围差异较大。如果不对这些特征进行归一化，可能导致模型受到具有较大尺度的特征的影响，而忽略了其他特征的重要性。归一化可以消除特征间的量纲差异，使得模型更公平地对待各个特征，可能提高精度；
- 提高模型收敛速度：某些优化算法（如梯度下降）在训练过程中对输入数据的尺度较为敏感。如果数据未经归一化，可能导致优化过程变慢或不稳定。通过归一化，可以使得优化算法更快地收敛，加快模型的训练速度。

