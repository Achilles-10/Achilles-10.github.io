[{"content":"1. 引言 Transformer 取代了以往递归神经网络为主导的骨干架构，随着ViT的引入，彻底改变了网络架构设计的格局。但ViT的全局注意力机制对输入大小的复杂度过高，难以处理高分辨率的输入。\n层级Transformer采用混合方法来解决这个问题，例如Swin Transformer采用了“滑动窗口”策略，也说明了卷积仍然非常受欢迎。本文目标是为卷积网络弥补前ViT时代和后ViT时代的差距，并测试唇卷积网络可以达到的极限。\n2. Modernizing a ConvNet: a Roadmap（研究路线图） 以ResNet-50作为baseline，考虑以下几种设计决策：\nmarco design（宏观设计） ResNeXt inverted bottlenect（倒置瓶颈） large kernel size（更大的卷积核） various layer-wise micro designs（多样的分层微设计） 2.1 训练技巧 epoch: 90-\u0026gt;300 optimizer: AdamW data augmentation: Mixup, Cutmix, RandAugment, RandomErasing\u0026hellip; regularization: Stochastic Depth, Label Smoothing 2.2 Marco Design（宏观设计） 改变阶段计算比：Swin-T的阶段计算比为1:1:3:1，更大型的Swin的阶段计算比为1:1:9:1。对此，将ResNet-50中的(3,4,6,3)改为 (3,3,9,3)，使模型准确率从78.8%提升至79.4%。 将stem改为\u0026quot;Patchify\u0026quot;（非重叠的卷积）：标准的ResNet中stem为(k=7,p=3,s=2)的卷积后跟一个(k=3,p=1,s=2)的最大池化，这导致输入图像的4倍下采样。将其更换为 (k=4,s=4)的卷积，模型准确率从79.4%提升至79.5%。 2.3 ResNeXt-ify 采用深度可分离卷积，使得每个操作单独混合空间或通道的信息。使用分组卷积(depthwise conv)能够降低网络的FLOPs，但也会降低准确率(78.3%)。将网络宽度从64扩展到96，准确率提升到80.5%。\n2.4 Inverted Bottlenect（倒置瓶颈） Transformer中的MLP的隐藏维度比输入维度大4倍（384:96），这就是倒置瓶颈。对倒置瓶颈的探索如下图(a)(b)，这使得准确率提升(80.5%-\u0026gt;80.6%)的同时降低了FLOPs(下采样残差1x1卷积的FLOPs减少)。\n2.5 Large Kernel Sizes（大卷积核） VGG推广的黄金标准是堆叠3x3的小卷积核，这在现代化GPU上更高效，但Swin中的窗口大小至少为7x7。\n上移分组卷积层：如上图(b)(c)，使复杂低效的模块(MSA)有更少的通道数，降低FLOPS至4.1G，性能暂时下降到79.9%。 增大卷积核：将卷积核大小从3x3增大到7x7，FLOPs大致保持不变，准确率提升至80.6%。当继续增大卷积核时并没有带来更大准确率增益。 2.6 Micro Design（微观设计） 将ReLU更换为GELU：准确率不变 更少的激活函数：如下图所示，复制Swin的样式，将残差块中的激活函数去掉，去掉两个卷积层中的一个激活函数，准确度提升至81.3%。 更少的归一化层：去掉两个归一化层，在1x1卷积前只留下一个BN层，准确率提升到81.4%，超过Swin。 将BN替换为LN：BN能够加速收敛并减少过拟合，但BN错综复杂，可能对模型的性能产生不利影响。在ResNet中直接将BN替换为LN会导致性能不佳，但随着对网络结构和训练技巧的修改，使用LN将准确率提升至81.5%。 可分离的下采样层：ResNet中的下采样是通过每个阶段开始时的残差块实现的。Swin中添加了一个单独的下采样层。本文用单独的(k=2,s=2)卷积实现下采样，后续实验发现在分辨率变化的地方添加归一化层有助于稳定训练，这时准确率达到82.0%。 3. 在ImageNet上的评估 构建了不同的ConvNeXt变体：\nConvNeXt-T: C =(96, 192, 384, 768), B =(3, 3, 9, 3) ConvNeXt-S: C =(96, 192, 384, 768), B =(3, 3, 27, 3) ConvNeXt-B: C =(128, 256, 512, 1024), B =(3, 3, 27, 3) ConvNeXt-L: C =(192, 384, 768, 1536), B =(3, 3, 27, 3) ConvNeXt-XL: C =(256, 512, 1024, 2048), B =(3, 3, 27, 3) 3.1 结果 ImageNet-1K：\nImageNet-22K 预训练，ImageNet-1K 微调：\n3.2 Isotropic ConvNeXt vs. ViT（同质性比较） 同质架构（Isotropic architecture）：同质架构模型没有下采样层，在所有深度都保持相同的特征图分辨率，只需要用特征大小（即patch embedding的维度）和网络深度（即blocks数量）两个参数定义。\nConvNeXt的性能同ViT相当，说明ConvNeXt块设计在用于非层级模型时具有竞争力。\n4. 在下游任务上的评估 4.1 COCO数据集上的目标检测和分割 4.2 ADE20K上的语义分割 4.3 关于模型效率的评论 5. 总结 ConvNeXt模型本身不是全新的，里面的许多设计都被单独测试过，但没有放在一起测试过。ConvNeXt的实验结果是优秀的，在多个计算机视觉基准测试中与最先进的层级Transformer竞争的同时，还保留着标准卷积网络的简单性和效率。\n","permalink":"https://Achilles-10.github.io/posts/paper/convnext/","summary":"1. 引言 Transformer 取代了以往递归神经网络为主导的骨干架构，随着ViT的引入，彻底改变了网络架构设计的格局。但ViT的全局注意力机制对输入大小的复杂度过高，难以处理高分辨率的输入。 层级Transformer采用混合方法来解决这个问题，例如Swin Transformer采用了“滑动窗口”策略，","title":"A ConvNet for the 2020s"},{"content":"图像入门 读取图像 使用cv2.imread(filename,flags)函数读取图像，参数说明如下：\nfilename - 待读取图像的路径\nflags - 读取图像的方式\ncv2.IMREAD_COLOR - 加载彩色图像，图像的透明度会被忽略，默认标志 cv2.IMREAD_GRAYSCALE - 以灰度模式加载图像 cv2.IMREAD_UNCHANGED - 加载图像，不会忽略透明度 可以分别传递整数1，0，-1\nimport numpy as np import cv2 img = cv2.imread(\u0026#39;face.png\u0026#39;,0) 即使图像路径错误，也不会报错，但print(img)会输出None\n写入图像 使用cv2.imwrite(filename,image)函数保存图像，参数说明如下：\nfilename - 保存的文件名 image - 要保存的图像 cv2.imwrite(\u0026#39;save.png\u0026#39;, img) 使用Matplotlib显示图像 from matplotlib import pyplot as plt plt.imshow(img, cmap = \u0026#39;gray\u0026#39;, interpolation = \u0026#39;bicubic\u0026#39;) plt.xticks([]), plt.yticks([]) # 隐藏 x 轴和 y 轴上的刻度值 plt.show() 图像的基本操作 访问和修改像素值 可以通过行列坐标来访问和修改像素值。对于BGR图像，返回一个[BLUE值 GREEN值 RED值]数组；对于灰度图像，只返回对应的灰度。\n\u0026gt;\u0026gt;\u0026gt; px=img[100,100] \u0026gt;\u0026gt;\u0026gt; print(px) [237 217 186] # 访问BLUE值 \u0026gt;\u0026gt;\u0026gt; blue = img[100,100,0] \u0026gt;\u0026gt;\u0026gt; print(blue) 237 \u0026gt;\u0026gt;\u0026gt; img[100,100]=[255,255,255] \u0026gt;\u0026gt;\u0026gt; print(img[100,100]) [255 255 255] 上述方法用于选择数组的区域。对于单像素访问，Numpy数组方法array.item()和array.itemset()返回标量，相对更好。\n\u0026gt;\u0026gt;\u0026gt; img.item(100,100,2) 255 \u0026gt;\u0026gt;\u0026gt; # 修改RED值 \u0026gt;\u0026gt;\u0026gt; img.itemset((10,10,2),99) \u0026gt;\u0026gt;\u0026gt; img.item(10,10,2) 99 访问图像属性 图像属性包括行列通道数，数据类型和像素数等。\n图像的形状由img.shape访问，返回行列通道数的元组，如果图像是灰度的，返回值仅包括行列数。\n像素总数由img.size访问，图像数据类型由img.dtype访问。\n\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;shape:\u0026#39;,img.shape) shape: (512, 512, 3) \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;size:\u0026#39;,img.size) size: 786432 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;dtype:\u0026#39;,img.dtype) dtype: uint8 img.dtype在调试时很重要，因为OpenCV代码中的大量错误是由无效的数据类型引起的。\n图像感兴趣区域(Region of Interest, ROI) 用Numpy获取ROI，例如将图像中的人脸复制到图像的另一个区域\n\u0026gt;\u0026gt;\u0026gt; face = img[10:300,210:430] \u0026gt;\u0026gt;\u0026gt; img[0:290,0:220]=face 拆分和合并图像通道 有时候需要分别处理图像的B, G, R通道，或者将单独的通道加入到BGR图像，在这种情况下需要拆分或合并图像通道。\n\u0026gt;\u0026gt;\u0026gt; b,g,r=cv2.split(img) \u0026gt;\u0026gt;\u0026gt; img=cv2.merge((b,g,r)) 或者采用Numpy索引，例如将所有红色值设置为零：\n\u0026gt;\u0026gt;\u0026gt; img[:,:,2]=0 cv2.split()是一个耗时的操作，非必要时使用Numpy索引。\n为图像设置边框（填充） 可以使用copyMakeBorder(src, top, bottom, left, right, borderType[, dst[, value]]) -\u0026gt; dst在图像周围创建边框，该函数在卷积运算，零填充等方面有更多应用。参数说明如下：\nsrc - 输入图像 top, bottom, left, right - 边界宽度 borderType - 边框类型标志，可以是一下类型： cv2.BORDER_CONSTANT - 添加恒定的彩色边框，该值由下一个参数给出 cv2.BORDER_REFLECT - 边框是边框元素的镜像 cv2.BORDER_REFLECT_101或cv2.BORDER_DEFAULT与上述相同，但略有变化 cv2.BORDER_REPLICATE - 最后一个元素被复制 cv2.BORDER_WRAP value - 边框的颜色，如果边框类型为cv2.BORDER_CONSTANT import cv2 import numpy as np from matplotlib import pyplot as plt BLUE = [255,0,0] img1 = cv2.imread(\u0026#39;face.png\u0026#39;) img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) replicate = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REPLICATE) reflect = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REFLECT) reflect101 = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REFLECT_101) wrap = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_WRAP) constant= cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_CONSTANT,value=BLUE) plt.subplot(231),plt.imshow(img1,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;ORIGINAL\u0026#39;) plt.subplot(232),plt.imshow(replicate,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REPLICATE\u0026#39;) plt.subplot(233),plt.imshow(reflect,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REFLECT\u0026#39;) plt.subplot(234),plt.imshow(reflect101,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REFLECT_101\u0026#39;) plt.subplot(235),plt.imshow(wrap,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;WRAP\u0026#39;) plt.subplot(236),plt.imshow(constant,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;CONSTANT\u0026#39;) plt.show() 图像上的运算 图像加法 可以通过cv2.add()或Numpy操作res=img1+img2完成图像加法操作。相加的图像应该具有相同的深度和类型，或者第二个图像是一个标量值。\nOpenCV加法是饱和运算，Numpy加法是模运算。\n\u0026gt;\u0026gt;\u0026gt; x=np.uint8([250]) \u0026gt;\u0026gt;\u0026gt; y=np.uint8([10]) \u0026gt;\u0026gt;\u0026gt; print(cv2.add(x,y)) [[255]] \u0026gt;\u0026gt;\u0026gt; print(x+y) [4] 当添加两个图像时，尽量使用OpenCV的功能，能提供更好的结果。\n图像融合 这也是图像加法，但是对相加的图像赋予给定的权重，使其具有融合或透明的感觉。\n$$ G(x)=(1-\\alpha)f_0(x)+\\alpha f_1(x) $$\n将$\\alpha$从$0\\rightarrow1$更改，可以实现图像过渡的效果。cv2.addWeighted()在图像上应用以下公式：\n$$ dst=\\alpha\\cdot img_1+\\beta\\cdot img_2+\\gamma $$\n在这里，$\\gamma$ 被视为零。\nimg1 = cv2.imread(\u0026#39;face1.png\u0026#39;) img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2 = cv2.imread(\u0026#39;face2.png\u0026#39;) img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) dst = cv2.addWeighted(img1,0.6,img2,0.4,0) plt.figure(figsize=(15, 9)) plt.subplot(131),plt.imshow(img1),plt.title(\u0026#39;IMG1\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(132),plt.imshow(img2),plt.title(\u0026#39;IMG2\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(133),plt.imshow(dst),plt.title(\u0026#39;DST\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() 按位运算 按位运算包括AND,OR,NOT,XOR操作。这些操作在提取图像的任意部分、定义和处理非矩形ROI方面非常有用。下面的例子是想把OpenCV的标志放到一个图像上。\n如果我们直接使用图像加法，它会改变颜色，无法达到我们想要的效果；如果使用图像融合，会得到一个透明的效果，也不是我们想要的效果。如果是一个矩形区域，我们可以使用ROI，但OpenCV的标志并不是矩形的，故可以用按位操作来实现：\n# 加载两张图片 img1 = cv2.cvtColor(cv2.imread(\u0026#39;shuyi.png\u0026#39;),cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(\u0026#39;logo.jpg\u0026#39;),cv2.COLOR_BGR2RGB) # 我想把logo放在左上角，所以我创建了ROI rows,cols,channels = img2.shape roi = img1[0:rows, 0:cols] # 现在创建logo的掩码，并同时创建其相反掩码 img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY) ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY) mask_inv = cv2.bitwise_not(mask) # 现在将ROI中logo的区域涂黑 img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv) # 仅从logo图像中提取logo区域 img2_fg = cv2.bitwise_and(img2,img2,mask = mask) # 将logo放入ROI并修改主图像 dst = cv2.add(img1_bg,img2_fg) img1[0:rows, 0:cols ] = dst plt.figure(figsize=(9, 6)) plt.subplot(121),plt.imshow(mask,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;MASK\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(122),plt.imshow(img1),plt.title(\u0026#39;RESULT\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() 在计算mask时用到了cv2.bitwise_not(src,dst=None,mask=None)函数，在计算前景和背景区域时用到了cv2.bitwise_and(src1,src2,dst=None,mask=None)，参数说明如下：\nsrc1 - 参与运算的图像 src2 - 参与运算的图像 dst - 可选运算结果输出数组 mask - 可选操作掩码 threshold(src, thresh, maxval, type[, dst])-\u0026gt;ret,dst函数的作用是将一幅灰度图二值化，参数说明如下：\nsrc - 输入的灰度图\nthresh - 阈值\nmaxval - 最大值\ntype - 阈值类型\n阈值类型 灰度值大于阈值(val\u0026gt;threshold) 其他情况 cv2.THRESH_BINARY maxval 0 cv2.THRESH_BINARY_INV 0 maxval cv2.THRESH_TRUNC thresh 当前灰度值 cv2.THRESH_TOZERO 当前灰度值 0 cv2.THRESH_TOZERO_INV 0 当前灰度值 ","permalink":"https://Achilles-10.github.io/posts/tech/opencv1/","summary":"图像入门 读取图像 使用cv2.imread(filename,flags)函数读取图像，参数说明如下： filename - 待读取图像的路径 flags - 读取图像的方式 cv2.IMREAD_COLOR - 加载彩色图像，图像的透明度会被忽略，默认标志 cv2.IMREAD_GRAYSCALE - 以灰度模式加载图像 cv2.IMREAD_UNCHANGED - 加载图像，不会忽略透明度 可以分别传递整数1，0，-1 import numpy as np import cv2 img =","title":"OpenCV-python学习笔记(1)：核心操作"},{"content":"动机与介绍 已有方法对跨域数据集和高压缩高曝光数据的检测能力大幅下降(泛化性差)；\n难以识别的fake样本通常包含更一般伪造痕迹，故要学习更通用和鲁棒的面部伪造表征；\n定义了四种常见的伪影(artifacts)： 主要贡献 提出了source-target generator (STG) and mask generator (MG)来学习更一般鲁棒的人脸伪造表征 通过自换脸而非寻找最接近的landmark换脸，降低了计算成本 在cross-dataset和cross-maniputation测试中都取得了SOTA 方法 学习伪造人脸与背景的不一致分为下列三个模块\nSource-Target Generator(STG):\n对source和target进行数据增强以产生不一致，并且对source进行resize和translate以再现边界混合和landmarks不匹配； 首先对Target和Source之一做图像增强 (color：RGB channels, hue, saturation, value, brightness, and contrast；frequency：downsample or sharpen)； 然后对source进行裁剪：$H_r=u_hH,\\quad W_r=u_wW$,其中$\\ u_h和u_w$是一组均分分布中的随机值，再对裁剪后的图像zero-padded 或者 center-cropped还原回初始大小； 最后对source做变形(translate)：traslate vector$\\ t=[t_h,t_w]$,$\\ t_h=v_hH,t_w=v_wW$，$v_h和v_w$是一组均分分布中的随机值。 Mask Generator: 生成变形的灰度mask图\n计算面部landmarks的convex hull来初始化mask，然后对mask变形(elastic deformation)，在用两个不同参数的高斯滤波器(gaussian filter)对mask进行平滑处理。最后在{0.25, 0.5, 0.75, 1, 1, 1}中选取混合指数(blending ration)； Blending: 用Mask来混合source和target图得到SBI\n$$I_{SB}=I_s\\odot M+I_t\\odot(1-M)$$\nTrain with SBIs: 将target而非原图作为”REAL“，使得模型集中在伪造痕迹上\n实验 实现细节 预处理：Dlib和RetinaFace裁帧，面部区域裁剪：4~20%(训练),12.5%(推理)； Source-Target Augmentation：RGBShift, HueSaturationValue, RandomBrightnessContrast, Downscale, and Sharpen 推理策略：如果在一帧中检测到两个或多个人脸，则将分类器应用于所有人脸，并将最高的虚假置信度用作该帧的预测置信度。 实验设定：各类baseline 跨数据集评估 跨操作评估 定量分析 消融实验 定性分析 局限性 缺乏时序信息、无法解决GAN生成的伪造图像\n","permalink":"https://Achilles-10.github.io/posts/paper/sbi/","summary":"动机与介绍 已有方法对跨域数据集和高压缩高曝光数据的检测能力大幅下降(泛化性差)； 难以识别的fake样本通常包含更一般伪造痕迹，故要学习更通用和鲁棒的面部伪造表征； 定义了四种常见的伪影(artifacts)： 主要贡献 提出了source-target generator (STG) and mask generator (MG)来学习更一般鲁棒","title":"Detecting Deepfakes with Self-Blended Images"},{"content":" 英文名: Achilles Zhang 职业: 学生 爱好: 篮球、健身、Dota2 个性签名: 垒山不止就是幸福 ","permalink":"https://Achilles-10.github.io/about/","summary":"英文名: Achilles Zhang 职业: 学生 爱好: 篮球、健身、Dota2 个性签名: 垒山不止就是幸福","title":"关于"}]