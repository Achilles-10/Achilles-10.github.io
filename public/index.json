[{"content":"1. 什么是正则化 正则化通过在损失函数中引入惩罚项来限制模型的复杂度，以防止模型过度拟合训练数据。惩罚项会在优化过程中对模型的参数进行调整，以平衡模型的拟合能力和泛化能力。\n正则化的作用如下：\n防止过拟合：正则化通过限制模型的复杂度，减少模型对训练数据的过度拟合，提高模型的泛化能力 特征选择：L1 正则化可以使得部分参数变为 0，从而实现特征选择，减少冗余特征的影响 改善模型稳定性：正则化可以降低模型对数据中噪声的敏感性，提高模型的稳定性 常见正则化方法有 L1 正则化、L2 正则化和 Dropout。\n2. L1 正则化 对于待正则的网络参数 $w$，L1 正则化： $$ L_1=\\lambda||w||_1=\\lambda\\sum_i|w_i| $$ 其中，$\\lambda$ 用来控制正则程度的大小。L1 不仅可以约束参数量，还可以是参数更稀疏，即部分参数为零（部分特征被去除），实现了特征选择。\nL1 正则化的优点和应用如下：\n特征选择：L1 正则化倾向于生成稀疏的参数，即将一部分参数变为 0，减少冗余特征，实现特征选择 模型解释性：L1 正则化的特征选择能够确定对预测结果贡献较大的特征，一定程度上解释模型的结果 防止过拟合：限制模型的复杂度，防止模型过度拟合训练数据 稀疏性：L1 正则化可以产生稀疏的参数，使得模型的计算效率更高，减少存储空间 3. L2 正则化 对于待正则的网络参数 $w$，L2 正则化： $$ L_2=\\lambda||w||_2=\\lambda\\sum_{i}{|w_i|^2} $$ L2 正则化会使部分特征趋近于 0，达到正则化的效果。\nL2 正则化的优点和应用包括：\n参数稳定性：L2 正则化可以使模型参数更稳定，减少参数的震荡，使模型对噪声和细节不敏感，提高模型的泛化能力。 防止过拟合：L2 正则化限制了模型参数的大小，使得模型更加平滑。 权重衰减：L2 正则化可以促使模型参数向较小的值靠拢，实现权重衰减，减少复杂模型对训练数据的过度拟合。 L1 正则化和 L2 正则化可以被联合使用，这种方法被称为 Elastic Net 正则化，它综合了两者的优势。可以同时实现特征选择和参数大小的控制。\n4. L1 正则化使得模型参数具有稀疏性的原理是什么 4.1 解空间形状 如下图，在二维情况下，黄色部分为 L2 和 L1 正则约束后的解空间（相当于约束 $w$ 的范数不能超过常数 m），蓝色等高线是凸优化问题中目标函数的等高线。最小化损失函数就是求蓝圈+黄圈的和的最小值，显然 L1 正则的解空间更容易在夹角处与等高线碰撞出稀疏解。\n4.2 函数叠加 如下图，棕线是原始目标函数 $L(w)$ 曲线图，显然最小值在蓝点处，对应的 $w^*$ 值非 0。\n考虑加上 L2 正则化项，目标函数变为黄线 $L(w)+Cw^2$，最小值在黄点处，对应 $w^*$ 绝对值减小了，仍然非 0。\n考虑加上 L1 正则化项，目标函数变为绿线 $L(w)+C|w|$，最小值在红点处，对应 $w^*$ 为 0，产生了稀疏性。\n由于加入 L1 正则化项，对目标函数求导，在原点左侧为 -C，右侧为 C，只要原目标函数导数绝对值小于 C，那么带正则项的目标函数在原点左侧递减，右侧递增，即最小值点在原点处。\n相反 L2 正则化项在原点处的导数为 0，只要原始目标函数导数值在原点不为 0，那么最小值点就不会在原点。所以 L2 正则项只有减小 w 绝对值的作用。\n4.3 贝叶斯先验 L2 正则化相当于引入了高斯先验，L1 正则化相当于对模型参数引入了拉普拉斯先验，使参数为 0 的可能性更大。\n下图为高斯分布曲线图，可见高斯分布在极值处是平滑的，即取不同值的可能性是接近的，因此 L2 正则化只会让 $w$ 更接近 0 点，但不会等于 0。\n下图为拉普拉斯分布曲线图，可见拉普拉斯分布在极值点是一个尖峰，所以 $w$ 的取值为 0 的概率更大。\n5. Dropout 的作用和原理 Dropout 在前向传播时，以一定的概率使部分神经元停止工作，增强模型泛化性。\n前向传播：以概率 p 丢弃 K 维输入向量 $[x_1,x_2,\\dots,x_k]$ 的某些值，有 $$ x\u0026rsquo; = dropout(x) $$ 其过程相当于生成一个随机的 K 维 mask，下式中 @ 表示点乘： $$ r = rand(K) = [r_1,r_2,\\dots,r_k],0\\leq r_i\\leq 1\\\\ mask=\\begin{cases}0,r_i\u0026lt;p\\\\1,r_i\\geq p\\end{cases}\\\\ x\u0026rsquo;=dropoout(x)=x@mask $$ 这样的 dropout，在训练中会使 x\u0026rsquo; 的期望改变： $$ E(X\u0026rsquo;)=p*0+(1-p)*x=(1-p)x $$ 在测试时，dropout 不工作，此时 x\u0026rsquo; 的期望为 x，使得训练和测试时的期望不一致，因此在训练时，可以将 dropout 后的结果除以 1-p，此时 mask 为： $$ mask=\\begin{cases}0,r_i\u0026lt;p\\\\\\frac{1}{1-p},r_i\\geq p\\end{cases} $$ 反向传播： $$ x=[x_1,x_2,\\dots,x_k]\\ x\u0026rsquo;=dropout(x)=x@mask\\ y=forward(x\u0026rsquo;) $$ 则有 $$ \\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial x\u0026rsquo;}\\cdot\\frac{\\partial x\u0026rsquo;}{\\partial x} $$\n其中 $$ \\frac{\\partial x\u0026rsquo;}{\\partial x}= \\begin{cases} 0,r_i\u0026lt;p\\\\ \\frac{1}{1-p},r_i\\geq p \\end{cases} $$\n6. L1 正则化的缺点 非光滑性：L1 正则化的正则化项是参数的绝对值之和，这导致目标函数在参数为零时不可导。在参数为零附近，梯度不连续，可能导致优化过程出现问题。 特征选择过于严格：当特征之间存在高度相关性（多重共线性）时，L1 正则化倾向于选择其中一个特征，而忽略其他相关特征，可能会丢失有用信息。相比之下，L2 正则化对相关特征的惩罚更均衡，可以保留更多相关特征的权重。 不适用于高维问题：虽然 L1 正则化在稀疏数据下表现出色，但对于高维稀疏数据，特征的选择过程可能不够稳定，可能会因微小的数据变动而导致不同的特征被选择。 7. BatchNorm 的原理和作用 BN 对每个批次的输入进行归一化，将输入的均值和方差调整到一个稳定的范围内，用于解决深度网络梯度消失和梯度爆炸的问题，可以起到一定的正则化作用。\n首先计算 mini-batch 的均值 $\\mu$ 和方差 $\\sigma^2$，然后进行规范化得到： $$ x_i^*=\\frac{x_i-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} $$\n最后进行尺度变换和偏移得到：\n$$ y_i=\\gamma\\cdot x^*_i+\\beta $$\nBN 是一个可学习、有参数（$\\gamma,\\beta$）的网络层。\n8. BN 的训练和推理有什么不同 BN 在训练时会更新\nBN 训练时记录每个 batch 的均值和方差，测试时用全部训练数据的均值(期望)和方差(无偏估计)。 $$ E[x]\\leftarrow E_\\beta[\\mu_\\beta]\\\\ Var[x]\\leftarrow\\frac{m}{m-1}E_\\beta[\\sigma_\\beta^2] $$ 推理使用训练阶段累积的统计量（均值和方差的移动平均值）来进行标准化。将训练时的均值和方差的平均值代入如下公式即可： $$ y=\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}\\cdot x+(\\beta-\\frac{\\gamma\\cdot E[x]}{\\sqrt{Var[x]+\\epsilon}}) $$ 有$\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}$ 和 $(\\beta-\\frac{\\gamma\\cdot E[x]}{\\sqrt{Var[x]+\\epsilon}})$ 两项固定值，可以加速计算。\n即使在推理时输入是 mini-batch，也应该使用训练时的移动平均值和方差，这样是为了保证模型在训练和推理时行为的一致性。\n9. BN 的可学习参数作用 BN 有 $\\gamma,\\beta$ 两个可学习的平移参数和缩放参数，具有以下作用：\n保留网络各层在训练过程中的学习成果。若没有这两个参数，BN 退化为普通的标准化，不能有效学习。添加参数后可以保留每个神经元学习的成果； 保证激活单元的非线性表达能力。添加参数后，BN 的数据可以进入到激活函数的非线性区域； 使 BN 模块具有自我关闭功能。若 $\\gamma,\\beta$ 分别取数据的均值和标准差，则可以回复初始的输入值，即关闭 BN 模块。 10. BN 和 LN 的区别 BN 是在每个批次中对每个特征的维度进行归一化。LN 是在每一层中对每个样本的所有特征维度进行归一化。\nBN 适用于批次训练的情况，LN 更适用于变长序列数据的处理。\n如以下两图，直观地展示了 BN、LN、IN 和 GN 的区别。\n11. 将 BN 用于推荐系统时的注意事项 数据稀疏性：推荐系统的输入往往非常稀疏，直接使用 BN 可能导致不稳定的统计量，BN 通常在嵌入层之后的稠密表征或者深层网络中表现得更好； 动态范围的特征：在推荐系统中，某些特征的范围可能动态变化，可能需要频繁地更新 BN 的统计数据或考虑使用其他归一化技巧； 序列化的数据：推荐系统中经常处理序列化的数据，例如用户的行为序列，BN 不适用。 ","permalink":"https://Achilles-10.github.io/posts/tech/regular/","summary":"1. 什么是正则化 正则化通过在损失函数中引入惩罚项来限制模型的复杂度，以防止模型过度拟合训练数据。惩罚项会在优化过程中对模型的参数进行调整，以平衡模型的拟合能力和泛化能力。 正则化的作用如下： 防止过拟合：正则化通过限制模型的复杂度，减少模型对训练数据的过度拟合，提高模型的泛化能力 特征选","title":"深度学习面试题：正则化"},{"content":"图论 51. 岛屿数量（中等） DFS\n当面试时不能修改原数组时需要使用标记。\ndef numIslands(self, grid: List[List[str]]) -\u0026gt; int: directions=[(1,0),(0,1),(-1,0),(0,-1)] m,n,ans=len(grid),len(grid[0]),0 vis = set() def dfs(i,j): vis.add((i,j)) for di,dj in directions: ii,jj = i+di,j+dj if 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n and grid[ii][jj]==\u0026#39;1\u0026#39; and (ii,jj) not in vis: dfs(ii,jj) for i in range(m): for j in range(n): if grid[i][j]==\u0026#39;1\u0026#39; and (i,j) not in vis: dfs(i,j) ans+=1 return ans 52. 腐烂的橘子（中等） BFS\ndef orangesRotting(self, grid: List[List[int]]) -\u0026gt; int: m,n=len(grid),len(grid[0]) direction=[(1,0),(0,1),(-1,0),(0,-1)] ans,remain,stack=0,0,[] for i in range(m): for j in range(n): if grid[i][j]==2: stack.append((i,j)) elif grid[i][j]==1: remain+=1 while stack and remain: for _ in range(len(stack)): i,j=stack.pop(0) for di,dj in direction: ii,jj=i+di,j+dj if 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n and grid[ii][jj]==1: grid[ii][jj]=2 remain-=1 stack.append((ii,jj)) ans+=1 return ans if remain==0 else -1 53. ⭐️ 课程表（中等） DFS\n定义节点三个状态，0：待搜索；1：正在搜索；2：完成搜索。如果在 DFS 过程中遇到了状态为 1 的节点，说明遇到了环，无法完成所有课程的学习。\ndef canFinish(self, numCourses: int, prerequisites: List[List[int]]) -\u0026gt; bool: edges = defaultdict(list) for cour,pre in prerequisites: edges[pre].append(cour) visited = [0]*numCourses valid=True def dfs(u): nonlocal valid visited[u]=1 # 正在搜索 for v in edges[u]: if visited[v]==0: # 当前节点未被搜索 dfs(v) if not valid: return elif visited[v]==1: # 遇到环 valid=False return visited[u]=2 # 完成搜索 for i in range(numCourses): if valid and not visited[i]: dfs(i) return valid BFS\n若一个课程节点的入度为 0，则表示该课程没有先修课程或先修课程已经学完，可以学习当前课程，去掉该节点的所有出边，则表示它的相邻节点少了一门先修课程。维护一个队列，不断地将入度为 0 的课程节点加入，直到答案中包含所有的节点（得到了一种拓扑排序）或者不存在没有入边的节点（图中包含环）。\ndef canFinish(self, numCourses: int, prerequisites: List[List[int]]) -\u0026gt; bool: edges = defaultdict(list) indeg = [0]*numCourses for cour,pre in prerequisites: edges[pre].append(cour) indeg[cour]+=1 # 入度 q = collections.deque([u for u in range(numCourses) if indeg[u]==0]) # 将入度为 0 的节点加入队列 visited=0 # 遍历的节点数 while q: u=q.popleft() visited+=1 for v in edges[u]: indeg[v]-=1 if indeg[v]==0: # 入度为 0，加入队列 q.append(v) return visited==numCourses 54. 实现 Trie (前缀树)（中等） 设计\n将 Trie 视作一个节点\nclass Trie: def __init__(self): self.child = [None]*26 self.isEnd = False def insert(self, word: str) -\u0026gt; None: node = self for c in word: idx = ord(c)-ord(\u0026#39;a\u0026#39;) if not node.child[idx]: node.child[idx]=Trie() node=node.child[idx] node.isEnd=True def searchPrefix(self, word): node=self for c in word: idx = ord(c)-ord(\u0026#39;a\u0026#39;) if not node.child[idx]: return None node = node.child[idx] return node def search(self, word: str) -\u0026gt; bool: node = self.searchPrefix(word) return node is not None and node.isEnd def startsWith(self, prefix: str) -\u0026gt; bool: node = self.searchPrefix(prefix) return node is not None 回溯 55. 全排列（中等） 回溯\n回溯到 idx 处时，依次将后面未排列的数交换到 idx 处。\ndef permute(self, nums: List[int]) -\u0026gt; List[List[int]]: ans=[] n=len(nums) def backtracking(idx): if idx==n: ans.append(nums[:]) for i in range(idx,n): nums[idx],nums[i]=nums[i],nums[idx] backtracking(idx+1) nums[idx],nums[i]=nums[i],nums[idx] backtracking(0) return ans 56. 子集（中等） 回溯\ndef subsets(self, nums: List[int]) -\u0026gt; List[List[int]]: ans = [] def dfs(path,idx): if idx==len(nums): ans.append(path[:]) return dfs(path+[nums[idx]],idx+1) dfs(path,idx+1) dfs([],0) return ans 57. 电话号码的字母组合（中等） 回溯\ndef letterCombinations(self, digits: str) -\u0026gt; List[str]: maps = {\u0026#39;2\u0026#39;:\u0026#39;abc\u0026#39;,\u0026#39;3\u0026#39;:\u0026#39;def\u0026#39;,\u0026#39;4\u0026#39;:\u0026#39;ghi\u0026#39;,\u0026#39;5\u0026#39;:\u0026#39;jkl\u0026#39;,\u0026#39;6\u0026#39;:\u0026#39;mno\u0026#39;,\u0026#39;7\u0026#39;:\u0026#39;pqrs\u0026#39;,\u0026#39;8\u0026#39;:\u0026#39;tuv\u0026#39;,\u0026#39;9\u0026#39;:\u0026#39;wxyz\u0026#39;} if not digits: return [] ans,n=[],len(digits) def dfs(idx,path): if idx==n: ans.append(\u0026#39;\u0026#39;.join(path)) return for c in maps[digits[idx]]: path.append(c) dfs(idx+1,path) path.pop() dfs(0,[]) return ans 58. 组合总和（中等） 回溯+剪枝\ndef combinationSum(self, candidates: List[int], target: int) -\u0026gt; List[List[int]]: candidates.sort(reverse=True) ans,n=[],len(candidates) def backtracking(idx,path,s): if s==target: ans.append(path[:]) return if s\u0026gt;target: return for i in range(idx,n): path.append(candidates[i]) backtracking(i,path,s+candidates[i]) path.pop() backtracking(0,[],0) return ans 59. 括号生成（中等） 回溯\n定义回溯函数 backtracking(path,left,right)，其中 left 和 right 表示还能添加的左括号和右括号的数量。每次添加左括号时，left=left-1, right=right+1；添加右括号时，right=right-1。\ndef generateParenthesis(self, n: int) -\u0026gt; List[str]: ans = [] def backtracking(path,left,right): if len(path)==2*n: ans.append(\u0026#39;\u0026#39;.join(path)) return if left\u0026gt;0: path.append(\u0026#39;(\u0026#39;) backtracking(i+1,path,left-1,right+1) path.pop() if right\u0026gt;0: path.append(\u0026#39;)\u0026#39;) backtracking(i+1,path,left,right-1) path.pop() backtracking([],n,0) return ans 60. ⭐️ 单词搜索（中等） 回溯\n设置标志，注意返回条件\ndef exist(self, board: List[List[str]], word: str) -\u0026gt; bool: directions = [(1,0),(0,1),(-1,0),(0,-1)] m,n = len(board),len(board[0]) length = len(word) seen = set() def dfs(i,j,cnt): if board[i][j]!=word[cnt]: return False if cnt==length-1: return True flag=False seen.add((i,j)) for di,dj in directions: ii,jj=i+di,j+dj if (ii,jj) not in seen and 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n: if dfs(ii,jj,cnt+1): flag = True break seen.remove((i,j)) return flag for i in range(m): for j in range(n): if dfs(i,j,0): return True return False 61. 分割回文串（中等） 回溯\ndef partition(self, s: str) -\u0026gt; List[List[str]]: def check(i,j): while i\u0026lt;j: if s[i]!=s[j]: return False i+=1 j-=1 return True n=len(s) ans=[] def dfs(idx,path): if idx==n: ans.append(path[:]) return for j in range(idx,n): if check(idx,j): path.append(s[idx:j+1]) dfs(j+1,path) path.pop() dfs(0,[]) return ans 62. ⭐️ N 皇后（困难） 回溯\n按行生成合法的棋盘\ndef solveNQueens(self, n: int) -\u0026gt; List[List[str]]: ans=[] grid = [[\u0026#39;.\u0026#39; for _ in range(n)]for _ in range(n)] def check(i,j,grid): for ii in range(n): for jj in range(n): if grid[ii][jj]==\u0026#39;Q\u0026#39; and (ii==i or jj==j or ii+jj==i+j or ii-jj==i-j): return False return True def backtracking(i): if i==n: ans.append([\u0026#39;\u0026#39;.join(tmp) for tmp in grid]) return for j in range(n): if check(i,j,grid): grid[i][j]=\u0026#39;Q\u0026#39; backtracking(i+1) grid[i][j]=\u0026#39;.\u0026#39; backtracking(0) return ans 二分查找 63. 搜索插入位置（简单） 二分查找\ndef searchInsert(self, nums: List[int], target: int) -\u0026gt; int: n=len(nums) i,j=0,n-1 while i\u0026lt;=j: mid=i+(j-i)//2 if target==nums[mid]: return mid elif target\u0026gt;nums[mid]: i=mid+1 else: j=mid-1 return i extra. 二分查找（简单） 二分查找\ndef search(self, nums: List[int], target: int) -\u0026gt; int: n=len(nums) left,right=0,n-1 while left\u0026lt;=right: mid = left+(right-left)//2 if nums[mid]==target: return mid elif nums[mid]\u0026gt;target: right=mid-1 else: left=mid+1 return -1 64. 搜索二维矩阵（中等） 二分查找\n从右上角出发\ndef searchMatrix(self, matrix: List[List[int]], target: int) -\u0026gt; bool: m,n=len(matrix),len(matrix[0]) i,j=0,n-1 while i\u0026lt;m and 0\u0026lt;=j: if target==matrix[i][j]: return True elif target\u0026gt;matrix[i][j]: i+=1 else: j-=1 return False 65. ⭐️ 在排序数组中查找元素的第一个和最后一个位置（中等） 二分查找\n设置一个 lower 标志，控制二分查找是查找第一个大于等于 target 值还是第一个大于 target 值的下标。\ndef searchRange(self, nums: List[int], target: int) -\u0026gt; List[int]: def bisearch(nums,target,lower): left,right,ans=0,len(nums)-1,len(nums) while left\u0026lt;=right: mid = left+(right-left)//2 # 若 lower = True，即使 mid=target，也继续向左搜索。否则，向右搜索第一个大于 target 的下标 if nums[mid]\u0026gt;target or (lower and nums[mid]\u0026gt;=target): right=mid-1 ans=mid else: left=mid+1 return ans n=len(nums) l = bisearch(nums,target,True) if l\u0026gt;=n or nums[l]!=target: return [-1,-1] # 判断 target 在数组中是否存在 r = bisearch(nums,target,False) return [l,r-1] 66. ⭐️ 搜索旋转排序数组（中等） 二分查找\n判断 mid 左右是否有序。 $$ \\begin{cases} left\\leq mid: 左侧有序 \\begin{cases} left\\leq tar\u0026lt;mid: tar\\ 在\\ mid\\ 左侧\\\\ tar\\ 在\\ mid\\ 右侧 \\end{cases}\\\\ 右侧有序 \\begin{cases} mid\u0026lt;tar\\leq right: tar\\ 在\\ mid\\ 右侧\\\\ tar\\ 在\\ mid\\ 左侧 \\end{cases} \\end{cases} $$\ndef search(self, nums: List[int], target: int) -\u0026gt; int: n=len(nums) left,right=0,n-1 while left\u0026lt;=right: mid = (left+right)//2 if target==nums[mid]: return mid if nums[left]\u0026lt;=nums[mid]: if nums[left]\u0026lt;=target\u0026lt;nums[mid]: right=mid-1 else: left=mid+1 else: if nums[mid]\u0026lt;target\u0026lt;=nums[right]: left=mid+1 else: right=mid-1 return -1 # 标准库写法 def search(self, nums: List[int], target: int) -\u0026gt; int: n=len(nums) left,right = 0,n-1 while left\u0026lt;right: mid = left+(right-left)//2 # 防溢出 if nums[mid]\u0026lt;target: left = mid+1 else: right=mid return left if nums[left]==target else -1 # 判断是否合法 67. 寻找旋转排序数组中的最小值 II（困难） 二分查找\n每次将 mid 与 right 比较：\n若相等，则说明 right 有可替代的元素，舍弃，right-=1 若 mid\u0026gt;right，则说明最小值在 mid 右侧，left=left+1 若 mid\u0026lt;right，则说明最小值在 mid 左侧或是 mid def findMin(self, nums: List[int]) -\u0026gt; int: n=len(nums) l,r=0,n-1 while l\u0026lt;r: mid = l+(r-l)//2 if nums[mid]==nums[r]: r-=1 elif nums[mid]\u0026gt;nums[r]: l=mid+1 else: r=mid return nums[l] 68. ⭐️ 寻找两个正序数组的中位数（困难） 归并：时间复杂度 $O(m+n)$\n⭐️ 二分查找\n将寻找中位数转换为在两个数组中寻找第 k 小的元素。\n每次循环比较两个数组的第 k//2 个元素，加入 nums1[k//2]\u0026lt;=nums2[k//2]，则说明 nums1 的前 k//2 个元素都不可能为中位数，将其从数组中移除，并将 k 减去相应的值，反之亦然。当 k=1 时，只需去两个数组中最小的值即可；当其中一个数组为空时，直接取另一个数组中对应的值。\ndef findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -\u0026gt; float: def getKthElement(k): index1, index2 = 0, 0 while True: # 特殊情况 if index1 == m: return nums2[index2 + k - 1] if index2 == n: return nums1[index1 + k - 1] if k == 1: return min(nums1[index1], nums2[index2]) # 正常情况 newIndex1 = min(index1 + k // 2 - 1, m - 1) newIndex2 = min(index2 + k // 2 - 1, n - 1) pivot1, pivot2 = nums1[newIndex1], nums2[newIndex2] if pivot1 \u0026lt;= pivot2: k -= newIndex1 - index1 + 1 index1 = newIndex1 + 1 else: k -= newIndex2 - index2 + 1 index2 = newIndex2 + 1 m, n = len(nums1), len(nums2) totalLength = m + n if totalLength % 2 == 1: return getKthElement((totalLength + 1) // 2) else: return (getKthElement(totalLength//2) + getKthElement(totalLength//2+1)) / 2 划分数组：见题解\n栈 69. 有效的括号（简单） 栈\n若栈顶与遍历到的右括号不匹配，则返回 False；遍历结束检查栈内是否有多余的左括号。\ndef isValid(self, s: str) -\u0026gt; bool: com = [\u0026#39;()\u0026#39;,\u0026#39;{}\u0026#39;,\u0026#39;[]\u0026#39;] n,stack=len(s),[\u0026#39;?\u0026#39;] for c in s: if c in [\u0026#39;(\u0026#39;,\u0026#39;{\u0026#39;,\u0026#39;[\u0026#39;]: stack.append(c) else: tmp=stack.pop() if tmp+c not in com: return False return len(stack)==1 匹配\n循环匹配括号，判断字符串是否完全匹配。\ndef isValid(self, s: str) -\u0026gt; bool: while \u0026#39;()\u0026#39; in s or \u0026#39;{}\u0026#39; in s or \u0026#39;[]\u0026#39; in s: s=s.replace(\u0026#39;()\u0026#39;,\u0026#39;\u0026#39;) s=s.replace(\u0026#39;{}\u0026#39;,\u0026#39;\u0026#39;) s=s.replace(\u0026#39;[]\u0026#39;,\u0026#39;\u0026#39;) if not s: return True return False 70. ⭐️ 最小栈（中等） 辅助栈\n维护一个辅助栈，栈顶始终是当前栈的最小值。当入栈元素小于辅助栈栈顶元素时，将该元素入栈辅助栈，否则辅助栈栈顶元素大小不变。\nclass MinStack: def __init__(self): self.stack=[] self.min_stack=[inf] def push(self, val: int) -\u0026gt; None: self.stack.append(val) if val\u0026lt;self.min_stack[-1]: self.min_stack.append(val) else: self.min_stack.append(self.min_stack[-1]) def pop(self) -\u0026gt; None: self.stack.pop() self.min_stack.pop() def top(self) -\u0026gt; int: return self.stack[-1] def getMin(self) -\u0026gt; int: return self.min_stack[-1] 71. 字符串解码（中等） 栈+模拟\ndef decodeString(self, s: str) -\u0026gt; str: stack=[] i=0 while i\u0026lt;len(s): if 0\u0026lt;=ord(s[i])-ord(\u0026#39;0\u0026#39;)\u0026lt;=9: num=0 while 0\u0026lt;=ord(s[i])-ord(\u0026#39;0\u0026#39;)\u0026lt;=9: num=num*10+int(s[i]) i+=1 stack.append(num) continue elif s[i]==\u0026#39;]\u0026#39;: tmp = [] while stack and stack[-1]!=\u0026#39;[\u0026#39;: c = stack.pop() tmp.append(c) stack.pop() # 弹出左括号 num = stack.pop() tmp = \u0026#39;\u0026#39;.join(tmp[::-1]*num) stack.append(tmp) else: stack.append(s[i]) # 左括号和字符入栈 i+=1 return \u0026#39;\u0026#39;.join(stack) 72. 每日温度（中等） 单调栈\n维护一个单调栈，存储温度下标，保证栈内下标对应的温度呈单调下降。每当遇到比栈顶对应温度高的温度时，将栈顶出栈，对应下标的 answer[idx] = i-idx。\ndef dailyTemperatures(self, temperatures: List[int]) -\u0026gt; List[int]: n=len(temperatures) answer,stack = [0]*n,[] for i,temp in enumerate(temperatures): while stack and temp\u0026gt;temperatures[stack[-1]]: idx = stack.pop() answer[idx]=i-idx stack.append(i) return answer 73. ⭐️ 柱状图中最大的矩形（困难） 暴力解法\n单调栈\n遍历每一柱子，找到左右两侧最近的高度小于 h 的柱子，这两根柱子之间的高度均不小于 h，则以当前柱子高度为高度的矩形面积为 (right-left-1)*h。维护一个单调栈使得栈中元素的高度非递减。\ndef largestRectangleArea(self, heights: List[int]) -\u0026gt; int: n=len(heights) left,right=[-1]*n,[n]*n stack=[(-1,0)] for i,h in enumerate(heights): while stack and h\u0026lt;stack[-1][1]: ii,hh=stack.pop() right[ii]=i left[i]=stack[-1][0] stack.append((i,h)) ans = max((right[i]-left[i]-1)*heights[i] for i in range(n)) return ans extra. ⭐️ 最大矩形（困难） 前缀和+单调栈\n把本题转换为上一题的解法。\ndef maximalRectangle(self, matrix: List[List[str]]) -\u0026gt; int: if not matrix: return 0 m,n=len(matrix),len(matrix[0]) def get_heights(heights,n): # 单调栈求解单层最大矩形 left,right=[-1]*n,[n]*n stack=[(-1,0)] for i,h in enumerate(heights): while stack and h\u0026lt;stack[-1][1]: ii,hh=stack.pop() right[ii]=i left[i]=stack[-1][0] stack.append((i,h)) ans = max((right[i]-left[i]-1)*heights[i] for i in range(n)) return ans for j in range(n): matrix[0][j]=1 if matrix[0][j]==\u0026#39;1\u0026#39; else 0 for i in range(1,m): # 前缀和 for j in range(n): matrix[i][j]=matrix[i-1][j]+1 if matrix[i][j]==\u0026#39;1\u0026#39; else 0 ans = max(get_heights(matrix[i],n) for i in range(m)) return ans 堆 74. 数组中的第K个最大元素（中等） 堆排序：$O(n\\log\\ k)$\n维护一个最小堆，每当遍历到的数大于堆顶元素，则 pushpop 更新堆，保证堆元素为已遍历数组的最大的 K 个元素。\nimport heapq def findKthLargest(self, nums: List[int], k: int) -\u0026gt; int: n=len(nums) heap = [] for i in range(k): heap.append(nums[i]) heapq.heapify(heap) for i in range(k,n): if nums[i]\u0026gt;heap[0]: heapq.heappushpop(heap,nums[i]) return heap[0] 基于快排的划分：$O(n)$\n每次选取一个哨兵进行快排，返回哨兵在排序后的下标，若下标等于 n-k，则说明哨兵刚好是第 K 大的元素。否则，则对第 K 大元素所在的区间继续进行快排。\nimport heapq def findKthLargest(self, nums: List[int], k: int) -\u0026gt; int: def partition(l,r): flag = nums[l] idx = l+1 for p in range(idx,r+1): if nums[p]\u0026lt;flag: nums[idx],nums[p]=nums[p],nums[idx] idx+=1 nums[idx-1],nums[l]=nums[l],nums[idx-1] return idx-1 n=len(nums) i,j=0,n-1 while True: idx = partition(i,j) if idx==n-k: return nums[idx] elif idx\u0026gt;n-k: j=idx-1 else: i=idx+1 冒泡排序：$O(nk)$\n进行 K 次冒泡操作。\n75. 前 K 个高频元素（中等） 堆\ndef topKFrequent(self, nums: List[int], k: int) -\u0026gt; List[int]: cnt = defaultdict(int) for num in nums: cnt[num]+=1 heap = [] for key,val in cnt.items(): heapq.heappush(heap,(val,key)) if len(heap)\u0026gt;k: heapq.heappop(heap) ans = [key for val,key in heap] return ans Counter\ndef topKFrequent(self, nums: List[int], k: int) -\u0026gt; List[int]: cnt = Counter(nums) common = cnt.most_common(k) return [x[0] for x in common] 76. 数据流的中位数（困难） 堆\n将数据流划分为大小两部分，分别用两个堆存储。\n当 left 为空或者 num 小于等于 max(left) 时，将 num 放入 left；若放入 left 后 left 数组长度大于 right 数组长度加一；\n反之亦然。\nimport heapq class MedianFinder: def __init__(self): self.left = [] self.right = [] def addNum(self, num: int) -\u0026gt; None: if not self.left or num\u0026lt;=-self.left[0]: heapq.heappush(self.left,-num) if len(self.right)+1\u0026lt;len(self.left): heapq.heappush(self.right,-heapq.heappop(self.left)) else: heapq.heappush(self.right,num) if len(self.right)\u0026gt;len(self.left): heapq.heappush(self.left,-heapq.heappop(self.right)) def findMedian(self) -\u0026gt; float: if len(self.right)==len(self.left): return (self.right[0]-self.left[0])/2 else: return -self.left[0] extra. ⭐️ 滑动窗口中位数（困难） 双堆+延迟删除\n此题为数据流中的中位数的进阶版，使用两个堆来维护窗口内的元素，中位数只与堆顶元素有关。关键在于延迟删除，使用一个计数器来记录需要删除的元素，当堆顶元素的计数大于 1 时，表示该元素需要删除。\nimport heapq from collections import Counter class Solution: def medianSlidingWindow(self, nums: List[int], k: int) -\u0026gt; List[float]: n,small,big=len(nums),[],[] delay=Counter() def get_mid(): if k%2==1: return -small[0] return (-small[0]+big[0])/2 for i in range(k): heapq.heappush(small,-nums[i]) for i in range(k//2): heapq.heappush(big,-heapq.heappop(small)) ans = [get_mid()] for i in range(k,n): l,balance = nums[i-k],0 delay[l]+=1 if small and l\u0026lt;=-small[0]: balance-=1 else: balance+=1 if small and nums[i]\u0026lt;=-small[0]: # 窗口外元素在小部分 heapq.heappush(small,-nums[i]) balance+=1 else: # 窗口外元素在大部分 heapq.heappush(big,nums[i]) balance-=1 # 平衡大小堆数量 if balance\u0026gt;0: heapq.heappush(big,-heapq.heappop(small)) if balance\u0026lt;0: heapq.heappush(small,-heapq.heappop(big)) # 延迟删除 while small and delay[-small[0]]\u0026gt;0: delay[-small[0]]-=1 if delay[-small[0]]==0: delay.pop(-small[0]) heapq.heappop(small) while big and delay[big[0]]\u0026gt;0: delay[big[0]]-=1 if delay[big[0]]==0: delay.pop(big[0]) heapq.heappop(big) ans.append(get_mid()) return ans 贪心算法 77. 买卖股票的最佳时机（简单） 贪心\ndef maxProfit(self, prices: List[int]) -\u0026gt; int: buy,profit = inf,0 for price in prices: buy=min(buy,price) profit=max(profit,price-buy) return profit extra. 买卖股票的最佳时机 II（中等） 贪心\ndef maxProfit(self, prices: List[int]) -\u0026gt; int: ans=0 for i in range(len(prices)-1): if prices[i+1]\u0026gt;prices[i]: ans+=prices[i+1]-prices[i] # 能赚钱就赚 return ans 78. 跳跃游戏（中等） 贪心\n记录当前能够到达的最远位置，当遍历到超过最远位置时，返回 False。\ndef canJump(self, nums: List[int]) -\u0026gt; bool: far = 0 for i in range(len(nums)): if i\u0026gt;far: return False far = max(far,i+nums[i]) if far\u0026gt;=len(nums)-1: return True 79. ⭐️ 跳跃游戏 II（中等） 贪心\n记录最远位置，每当达到最远位置就 cnt+1\ndef jump(self, nums: List[int]) -\u0026gt; int: ans,n=0,len(nums) maxi,end=0,0 # 分别记录下一步最远位置和当前最远位置 for i in range(n-1): maxi=max(maxi,i+nums[i]) if i==end: # 达到当前最远位置 ans+=1 end=maxi return ans 80. 划分字母区间（中等） 贪心\n记录每个字母最右边出现的位置。当前遍历位置超出当前区间内字母最右出现位置时得到一个划分区间。\ndef partitionLabels(self, s: str) -\u0026gt; List[int]: tab,ans={},[] for i,c in enumerate(s): tab[c]=i start,far=0,tab[s[0]] for i in range(1,len(s)): if i\u0026gt;far: ans.append(far-start+1) start=i far=tab[s[i]] else: far = max(far,tab[s[i]]) ans.append(len(s)-start) return ans 动态规划 81. 爬楼梯（简单） 动态规划\ndef climbStairs(self, n: int) -\u0026gt; int: a,b=1,2 while n\u0026gt;1: a,b=b,a+b n-=1 return a 82. 杨辉三角（简单） 动态规划\ndef generate(self, numRows: int) -\u0026gt; List[List[int]]: if numRows\u0026lt;3: return [[1]] if numRows==1 else [[1],[1,1]] ans=[[1],[1,1]] for i in range(2,numRows): tmp=[1]*(i+1) for j in range(1,i): tmp[j]=ans[i-1][j]+ans[i-1][j-1] ans.append(tmp[:]) return ans 83. 打家劫舍（中等） 动态规划\ndef rob(self, nums: List[int]) -\u0026gt; int: n=len(nums) if n\u0026lt;3: return max(nums) dp=[nums[0],max(nums[:2])] for i in range(2,n): dp.append(max(dp[-1],dp[-2]+nums[i])) return dp[-1] extra. 打家劫舍 II（中等） 动态规划\n分别计算打劫第一间房和不打劫第一间房两种情况\ndef rob(self, nums: List[int]) -\u0026gt; int: def rob_range(nums,l,r): a,b = nums[l],max(nums[l:l+2]) for i in range(l+2,r+1): a,b = b,max(b,a+nums[i]) return b if len(nums)\u0026lt;4: return max(nums) return max(rob_range(nums,0,len(nums)-2),rob_range(nums,1,len(nums)-1)) extra. 打家劫舍 III（中等） DFS\ndef rob(self, root: Optional[TreeNode]) -\u0026gt; int: def dfs(node): if not node: return 0,0 l,notl = dfs(node.left) r,notr = dfs(node.right) return node.val+notl+notr,max(l,notl)+max(r,notr) # 返回偷或不偷该点的最大值 return max(dfs(root)) 84. 完全平方数（中等） 动态规划\ndp[i]=min(dp[i-j]+dp[j])，其中 j 是小于 i 的完全平方数\ndef numSquares(self, n: int) -\u0026gt; int: squres = [i*i for i in range(1,101)] dp=[inf]*(n+1) for i in range(1,n+1): if i in squres: dp[i]=1 else: for j in squres: if j\u0026gt;=i: break dp[i]=min(dp[i],dp[i-j]+dp[j]) return dp[-1] 85. 零钱兑换（中等） 动态规划\ndp[i] 表示 凑成金额 i 所需的最小硬币数。遍历硬币，当遍历到硬币 coin 时，有 dp[j]=min(dp[j],dp[j-coin]+1)\ndef coinChange(self, coins: List[int], amount: int) -\u0026gt; int: n=len(coins) dp = [0]+[inf]*amount for coin in coins: for j in range(coin,amount+1): dp[j]=min(dp[j],dp[j-coin]+1) return dp[-1] if dp[-1]!=inf else -1 86. ⭐️ 单词拆分（中等） 动态规划\ndp[i] 表示以下标 i 结尾的字符串能否成功拆分；dp[i]=dp[j] \u0026amp;\u0026amp; check(s[j:i])，check 表示子串是否出现在单词字典中。\ndef wordBreak(self, s: str, wordDict: List[str]) -\u0026gt; bool: n=len(s) dp = [True]+[False]*n for i in range(n): for j in range(i+1): if s[j:i+1] in wordDict: dp[i+1]|=dp[j] return dp[-1] 记忆化回溯\n定义回溯函数 backtracking(s) 表示 s 能否用单词字典拼接而成。遍历区间 [0,n-1]，若 s[:i+1] 在单词字典中，则 res = backtracking(s[i+1:]) or res。\ndef wordBreak(self, s: str, wordDict: List[str]) -\u0026gt; bool: @functools.cache def backtracking(s): if not s: return True res = False for i in range(len(s)): if s[:i+1] in wordDict: res |= backtracking(s[i+1:]) return res return backtracking(s) 87. ⭐️ 最长递增子序列（中等） 动态规划\n定义 dp[i] 表示以 nums[i] 结尾的最长递增子序列长度，则有 $\\text{dp[i]=max(dp[j])+1}$，其中 $0\\leq j\u0026lt;i,\\ nums[i]\u0026gt;nums[j]$。\ndef lengthOfLIS(self, nums: List[int]) -\u0026gt; int: n=len(nums) dp = [1]*n for i in range(1,n): for j in range(i): if nums[i]\u0026gt;nums[j]: dp[i]=max(dp[i],dp[j]+1) return max(dp) 动态规划+二分查找\n维护一个 tails 数组，tails[i] 表示长度为 i+1 的递增子序列尾部元素的值，可以证明，tails 数组为严格递增的数组。\n设 res 为 tails 当前长度，当遍历到 nums[k] 时，通过二分法找到 nums[k] 在 tails 数组里的大小分界点，会有以下两种情况：\n区间里存在 tails[i] \u0026gt; nums[k]：更新 tails[i] 为 nums[k]； 区间里不存在 tails[i] \u0026gt; nums[k]：意味着 nums[k] 大于 tails 所有元素，nums[k] 可以接在当前最长递增子序列尾部，则 res+1； def lengthOfLIS(self, nums: List[int]) -\u0026gt; int: res, tails = 0, [0]*n for num in nums: i,j=0,res while i\u0026lt;j: m=(i+j)//2 if num\u0026gt;tails[m]: i=m+1 else: j=m tails[i]=num if j==res: res+=1 return res 88. 乘积最大子数组（中等） 动态规划\n记录遍历到 i 处时的最大最小值。\ndef maxProduct(self, nums: List[int]) -\u0026gt; int: ans=maxn=minn=nums[0] for i in range(1,len(nums)): ma,mi= maxn*nums[i],minn*nums[i] maxn = max(ma,mi,nums[i]) minn = min(ma,mi,nums[i]) ans=max(ans,maxn) return ans 89. 分割等和子集（中等） 动态规划\n转换为 0-1 背包问题，判断数组能否有子集的和为 s/2。使用滚动数组优化空间。\ndef canPartition(self, nums: List[int]) -\u0026gt; bool: s,n=sum(nums),len(nums) if s%2: return False target = s//2 dp = [True]+[False]*target for i in range(n): for j in range(target,nums[i]-1,-1): dp[j]|=dp[j-nums[i]] if dp[-1]: return True return False 90. ⭐️ 最长有效括号（困难） 动态规划\ndp[i] 表示以下标 i 结尾的序列最长括号长度。由于左括号结尾的括号长度均为0，只需考虑右括号。\n若 s[i-1]==\u0026rsquo;(\u0026rsquo;，则组成形如 \u0026ldquo;()\u0026rdquo; 的括号，dp[i]=dp[i-2]+2; 若 s[i-1]==\u0026rsquo;)\u0026rsquo; 且 s[i-dp[i-1]-1]==\u0026rsquo;(\u0026rsquo;，则组成形如 \u0026ldquo;(\u0026hellip;)\u0026rdquo; 的括号，dp[i]=dp[i-dp[i-1]-2]+dp[i-1]+2； def longestValidParentheses(self, s: str) -\u0026gt; int: ans,stack=0,[] dp=[0]*(len(s)+1) for i in range(1,len(s)): if s[i]==\u0026#39;)\u0026#39;: if s[i-1]==\u0026#39;(\u0026#39;: dp[i]=dp[i-2]+2 elif s[i-1]==\u0026#39;)\u0026#39;: if i-dp[i-1]-1\u0026gt;=0 and s[i-dp[i-1]-1]==\u0026#39;(\u0026#39;: dp[i]=dp[i-dp[i-1]-2]+dp[i-1]+2 return max(dp) 栈\n维护一个栈，保持栈底元素为最后一个没有被匹配的右括号，初始为 -1。每当遇到左括号时，将下标入栈；每当遇到右括号时，将栈顶元素弹出，若弹出后栈为空，说明当前的右括号为没有被匹配的右括号，将其入栈；否则此时组成一个括号，长度为 i-stack[-1]；\ndef longestValidParentheses(self, s: str) -\u0026gt; int: ans,stack=0,[-1] for i,c in enumerate(s): if c==\u0026#39;(\u0026#39;: stack.append(i) else: stack.pop() if not stack: stack.append(i) else: ans=max(ans,i-stack[-1]) return ans 双遍历+计数\n统计当前左括号和右括号数量，当相等时更新答案；右括号大于左括号时重置答案；左右遍历一次。\n多维动态规划 91. 不同路径（中等） 动态规划\ndef uniquePaths(self, m: int, n: int) -\u0026gt; int: dp = [[1 for _ in range(n)]for _ in range(m)] for i in range(1,m): for j in range(1,n): dp[i][j]=dp[i-1][j]+dp[i][j-1] return dp[-1][-1] 数学\n一共移动 m+n-2 次，其中 m-1 次向下移动，计算 $C_{m+n-2}^{m-1}$ 即可。\ndef uniquePaths(self, m: int, n: int) -\u0026gt; int: return math.comb(m + n - 2, m - 1) 92. 最小路径和（中等） 动态规划\n不用额外空间。\ndef minPathSum(self, grid: List[List[int]]) -\u0026gt; int: for i in range(len(grid)): for j in range(len(grid[0])): if i==j==0: continue if i==0: grid[i][j]+=grid[i][j-1] elif j==0: grid[i][j]+=grid[i-1][j] else: grid[i][j] += min(grid[i-1][j],grid[i][j-1]) return grid[-1][-1] 93. 最长回文子串（中等） 动态规划\n初始化 dp[i][i]=1，若 s[i]=s[j]，则dp[i][j] = dp[i+1][j-1]+2；记录最大长度和起始索引。\n注意，应该倒序遍历行，因为转移方程依赖于 dp[i+1]。\ndef longestPalindrome(self, s: str) -\u0026gt; str: n=len(s) dp = [[0 for _ in range(n)]for _ in range(n)] for i in range(n): dp[i][i]=1 max_l,start = 1,0 for i in range(n-1,-1,-1): for j in range(i+1,n): if s[i]==s[j]: if j-i\u0026lt;3: dp[i][j]=j-i+1 elif dp[i+1][j-1]!=0: dp[i][j]=dp[i+1][j-1]+2 if dp[i][j]\u0026gt;max_l: max_l=dp[i][j] start = i return s[start:start+max_l] 94. 最长公共子序列（中等） 动态规划\n定义 dp[i][j] 为 text1[:i] 与 text[:j] 的最长公共子序列长度：\n当 text[i]=text[j] 时，dp[i][j]=dp[i-1][j-1]+1 否则，dp[i][j]=max(dp[i-1][j],dp[i][j-1]) def longestCommonSubsequence(self, text1: str, text2: str) -\u0026gt; int: m,n=len(text1),len(text2) dp = [[0 for _ in range(n+1)]for _ in range(m+1)] for i in range(1,m+1): for j in range(1,n+1): if text1[i-1]==text2[j-1]: dp[i][j]=dp[i-1][j-1]+1 else: dp[i][j]=max(dp[i-1][j],dp[i][j-1]) return dp[-1][-1] 95. 编辑距离（困难） 动态规划\n定义 dp[i][j] 表示将 word1[:i] 转换成 word2[:j] 的最小操作数：\n当 word1[i]=word2[j] 时，dp[i][j]=dp[i-1][j-1] 否则，dp[i][j] 等于 dp[i-1][j-1]（修改word1）,dp[i-1][j]（添加word1）,dp[i][j-1]（删除word1） 三者中的最小值 +1 def minDistance(self, word1: str, word2: str) -\u0026gt; int: n1,n2=len(word1),len(word2) dp = [[0 for _ in range(n2+1)]for _ in range(n1+1)] # 初始化 for i in range(1,n1+1): dp[i][0]=i for j in range(1,n2+1): dp[0][j]=j for i in range(1,n1+1): for j in range(1,n2+1): if word1[i-1]==word2[j-1]: dp[i][j]=dp[i-1][j-1] else: dp[i][j]=min(dp[i-1][j-1],dp[i-1][j],dp[i][j-1])+1 return dp[-1][-1] 技巧 96. 只出现一次的数字（简单） 异或\n### 调库 from functools import reduce class Solution: def singleNumber(self, nums: List[int]) -\u0026gt; int: return reduce(lambda x,y:x^y,nums) 97. 多数元素（简单） 投票\ndef majorityElement(self, nums: List[int]) -\u0026gt; int: ans,cnt=nums[0],1 for i in range(1,len(nums)): if nums[i]==ans: cnt+=1 else: cnt-=1 if cnt==0: ans,cnt = nums[i],1 return ans 哈希表计数\n排序：排序后下标 $\\lfloor\\frac{n}{2}\\rfloor$ 处的元素为众数\n分治法\n若 a 是 nums 的众数，将 nums 分为两部分，则 a 至少是其中一部分的众数。将数组分成左右两部分，分别求出左半部分的众数 a1 以及右半部分的众数 a2。若 a1=a2，则合并后的众数不变；否则，比较两个众数在整个区间内出现的次数。\ndef majorityElement(self, nums: List[int]) -\u0026gt; int: def majority_element_rec(lo, hi) -\u0026gt; int: if lo == hi: return nums[lo] # 单位长度 mid = (hi - lo) // 2 + lo left = majority_element_rec(lo, mid) right = majority_element_rec(mid + 1, hi) if left == right: return left # 左右半区众数相等 left_count = sum(1 for i in range(lo, hi + 1) if nums[i] == left) right_count = sum(1 for i in range(lo, hi + 1) if nums[i] == right) return left if left_count \u0026gt; right_count else right return majority_element_rec(0, len(nums) - 1) 98. ⭐️ 颜色分类（中等） 单指针+两次遍历\n第一次遍历移动 0，第二次遍历移动 1\n⭐️ 双指针\n设置两个指针 p0 和 p1，分别指向 0 和 1 需要放置的位置。\n当遇到 1 时，将 1 交换至 p1 处，p1 右移一位；\n当遇到 0 时，将 0 交换至 p0 处，但原本 p0 处可能为 1，此时还需将 1 移动至 p1 处。\ndef sortColors(self, nums: List[int]) -\u0026gt; None: n=len(nums) p0=p1=0 for i in range(n): if nums[i]==1: nums[i],nums[p1]=nums[p1],nums[i] p1+=1 elif nums[i]==0: nums[i],nums[p0]=nums[p0],nums[i] if nums[i]==1: nums[i],nums[p1]=nums[p1],nums[i] p0+=1 p1+=1 99. ⭐️ 下一个排列（中等） 双指针\n倒序遍历，找到最后一个满足 nums[i]\u0026lt;nums[i+1] 的升序对，令 left=i；倒序遍历，找到最后一个 nums[j]\u0026gt;nums[i]，令 right=j，交换 nums[left] 和 nums[right]。此时 nums[left+1:] 为非升序排列，只需将其倒序变为非降序排列即可。\ndef nextPermutation(self, nums: List[int]) -\u0026gt; None: i=len(nums)-2 while i\u0026gt;=0 and nums[i]\u0026gt;=nums[i+1]: # 找最后一个升序的位置 i-=1 if i\u0026gt;=0: j=len(nums)-1 while nums[j]\u0026lt;=nums[i]: # 找最后一个大于 nums[i] 的 nums[j] j-=1 nums[i],nums[j]=nums[j],nums[i] l,r=i+1,len(nums)-1 while l\u0026lt;r: # 将 nums[l:] 变为非降序排列 nums[l],nums[r]=nums[r],nums[l] l+=1 r-=1 100. ⭐️ 寻找重复数（中等） 排序\ndef findDuplicate(self, nums: List[int]) -\u0026gt; int: nums.sort() for i in range(1,len(nums)): if nums[i]==nums[i-1]: return nums[i] 双指针\n将该问题转换为找到链表的环入口。考虑以下两种情况：\n数组中没有重复数。如数组 [1,3,4,2]，建立数组下标 n 和 nums[n] 的映射关系，有：0-\u0026gt;1,1-\u0026gt;3,2-\u0026gt;4,3-\u0026gt;2，由此可以产生一个类似链表的序列：0-\u0026gt;1-\u0026gt;3-\u0026gt;2-\u0026gt;4-\u0026gt;null；\n数组中有重复数。如数组 [1,3,4,2,2]，映射关系：0-\u0026gt;1,1-\u0026gt;3,2-\u0026gt;4,3-\u0026gt;2,4-\u0026gt;2，链表序列：0-\u0026gt;1-\u0026gt;3-\u0026gt;2-\u0026gt;4-\u0026gt;2-\u0026gt;4-\u0026gt;2-\u0026gt;...，该序列如下图所示：\n因此，当数组中有重复数时，会产生一个多对一的映射，这样形成的链表就有环。综上：\n数组中有一个重复的整数 \u0026lt;==\u0026gt; 链表中存在环 找到数组中的重复整数 \u0026lt;==\u0026gt; 找到链表的环入口 def findDuplicate(self, nums: List[int]) -\u0026gt; int: slow=fast=0 slow,fast = nums[slow],nums[nums[fast]] while slow!=fast: slow,fast = nums[slow],nums[nums[fast]] p=0 while p!=slow: p,slow=nums[p],nums[slow] return p 二分查找\n取 [left,right] 区间中间的数 mid，统计小于等于 mid 的数的个数 cnt，若 cnt 严格大于 mid，根据抽屉原理，重复的数在区间 [left,mid] 中；否则，在区间 [mid+1,right] 中。\ndef findDuplicate(self, nums: List[int]) -\u0026gt; int: left,right=1,len(nums)-1 while left\u0026lt;right: cnt=0 mid = left+(right-left)//2 for num in nums: # 统计小于 mid 的个数 if num\u0026lt;=mid: cnt+=1 if cnt\u0026gt;mid: right=mid else: left=mid+1 return left 原地交换\n当数组有重复元素时，数组元素的索引和值是多对一的关系。\n遍历数组，第一次遇到数字 x 时，将其交换至索引 x 处；而当第二次遇到数字 x 时，有 nums[x]=x ，即 nums[nums[x]]=nums[x]，此时得到重复数字。\ndef findDuplicate(self, nums: List[int]) -\u0026gt; int: i=0 while i\u0026lt;len(nums): if nums[i]==i: i+=1 continue if nums[i]==nums[nums[i]]: return nums[i] nums[nums[i]],nums[i]=nums[i],nums[nums[i]] ","permalink":"https://Achilles-10.github.io/posts/algo/hot2/","summary":"图论 51. 岛屿数量（中等） DFS 当面试时不能修改原数组时需要使用标记。 def numIslands(self, grid: List[List[str]]) -\u0026gt; int: directions=[(1,0),(0,1),(-1,0),(0,-1)] m,n,ans=len(grid),len(grid[0]),0 vis = set() def dfs(i,j): vis.add((i,j)) for di,dj in directions: ii,jj = i+di,j+dj if 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n and grid[ii][jj]==\u0026#39;1\u0026#39; and (ii,jj) not in vis: dfs(ii,jj) for i in range(m): for j in range(n): if grid[i][j]==\u0026#39;1\u0026#39; and (i,j) not in vis: dfs(i,j) ans+=1 return ans 52. 腐烂的橘子（中等） BFS def orangesRotting(self, grid: List[List[int]]) -\u0026gt; int: m,n=len(grid),len(grid[0]) direction=[(1,0),(0,1),(-1,0),(0,-1)] ans,remain,stack=0,0,[] for i in range(m): for j in range(n): if grid[i][j]==2: stack.append((i,j)) elif grid[i][j]==1: remain+=1 while stack and remain: for _ in range(len(stack)): i,j=stack.pop(0) for di,dj in direction: ii,jj=i+di,j+dj if 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n and grid[ii][jj]==1: grid[ii][jj]=2 remain-=1 stack.append((ii,jj))","title":"LeetCode 热题 100 : 51~100"},{"content":"1. 什么是损失函数 损失函数衡量了模型预测结果与实际目标之间的差异或误差程度，指导模型参数的优化和训练过程。\n损失函数可以分为经验风险损失函数（empirical risk loss function）和结构风险损失函数（structural risk loss function）。经验风险损失函数指预测结果和实际结果的差值，结构风险损失函数是指经验风险损失函数加上正则项。\n2. 说一下了解的损失函数和各自的运用场景 2.1 用于回归的损失函数 MSE：\n较大的差值在 MSE 计算中得到更大的权重，因此MSE对离群值非常敏感，容易受到噪声干扰，可能会导致模型过于拟合噪声。适用于当异常值对结果影响较大时，或者对大误差更关注的情况下。 $$ MSE = \\frac{1}{n}\\sum_{i}^{n}(y_i-\\hat{y}_i)^2 $$\nMAE：\n对异常值不敏感，能够更好地应对数据中的噪声，从而使模型更健壮。但由于导数不连续，可能影响收敛速度和优化的稳定性 $$ MAE = \\frac{1}{n}\\sum_{i}^{n}|y_i-\\hat{y}_i| $$\n2.2 用于分类的损失函数 0-1 损失函数：当预测结果正确时为 0，错误时为 1\n交叉熵损失函数（Cross-Entropy Loss）：\n适用于二分类（sigmoid）和多分类（softmax）问题，对于错误分类的惩罚较大，容易受到类别不平衡的影响。 $$ CE = -\\frac{1}{n}\\sum_{i}^{n}{y_i\\log p_i} $$ 当用于二分类时， $$ BCE = -\\sum_{i}^{n}{[y_i\\log p_i+(1-y_i)\\log(1-p_i)]} $$\n指数损失函数：\n与交叉熵损失函数不同，指数损失函数对于误分类的样本有更大的惩罚，因此对错误分类的样本更加敏感。 $$ L = \\exp(-y\\cdot\\hat{y}) $$\nHinge Loss：\n常用于支持向量机（SVM）等算法中，用于二分类问题。它基于间隔的概念，鼓励模型将正确类别的预测值与错误类别的预测值之间的间隔最大化。 $$ L = max(0,1-y\\cdot\\hat{y}) $$\n2.3 用于分割的损失函数 Jaccard/IoU Loss： $$ L_{IoU}=1-IoU=1-\\frac{|A\\cap B|}{|A\\cup B|}=1-\\frac{|A\\cap B|}{|A|+|B|+|A\\cap B|} $$\nDice 损失：\n衡量两个样本（ A 和 B ）的重叠部分。\nTP: True Positive，GT 和分割掩码 S 之间的交集区域\nFP: False Positive，GT 外的分割掩码 S 区域\nFN: False Negative，分割掩码外的 GT 区域\n$$ L_{Dice}=1-Dice=1-\\frac{2\\cdot|A\\cap B|}{|A|+|B|}=1-\\frac{2\\cdot TP}{2\\cdot TP+FP+FN} $$\n2.4 用于检测的损失函数 Smooth L1 Loss (Huber Loss)：\n增强了 MSE 对离群点的鲁棒性。 $$ L_{smoothL1}(x)= \\begin{cases} \\frac{1}{2}x^2,\\quad |x|\u0026lt;\\beta\\\\ \\beta\\cdot(|x|-\\frac{1}{2}\\beta) \\end{cases} $$\nFocal Loss：\nFocal Loss 主要用于解决类别不平衡问题，能够增强对难分类样本的关注。 $$ FL(p_t)=-\\alpha_t(1-p_t)^\\gamma\\log(p_t) $$\n调制因子 $\\gamma$ 用于减低易分样本的损失贡献，无论是前景类还是背景类，$p_t$ 越大，就说明该样本越容易被区分，调制因子也就越小； $\\alpha_t$ 用于调节正负样本损失之间的比例，前景类别使用 $\\alpha_t$ 时，对应的背景类别使用 $1-\\alpha_t$； 3. 交叉熵损失和最大似然函数的联系 最大似然函数：用于估计模型参数，寻找使观察数据出现概率最大化的参数值。 交叉熵函数：衡量两个概率分布之间的差异，常用于评估模型预测与真实标签之间的差异。 联系：最小化交叉熵函数的本质就是对数似然函数的最大化。 4. 在用 sigmoid 作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数 收敛快\n对于 MSE，有 $$ L_{MSE} = \\frac{1}{2n}\\sum_{x}{(a-y)^2} $$ 其中 y 为标签，a 为实际输出： $$ a=\\sigma(z);\\quad z=wx+b $$ 分别求 w 和 b 的梯度，有： $$ \\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial a}\\cdot\\frac{\\partial a}{\\partial z}\\cdot\\frac{\\partial z}{\\partial w}=(a-y)\\cdot\\sigma\u0026rsquo;(z)x $$ $$ \\frac{\\partial L}{\\partial b}=\\frac{\\partial L}{\\partial a}\\cdot\\frac{\\partial a}{\\partial z}\\cdot\\frac{\\partial z}{\\partial b}=(a-y)\\cdot\\sigma\u0026rsquo;(z) $$\n可见，梯度大小与 $\\sigma\u0026rsquo;(z)$ 有关，若使用 sigmoid 函数，则两端梯度很小，参数更新缓慢。\n对于交叉熵损失函数，有 $$ L_{CE}=-\\frac{1}{n}[y\\log a+(1-y)\\log(1-a)] $$ 若 $a=\\sigma(z)$，那么有 $$ \\begin{align*} \\frac{\\partial L}{\\partial a} \u0026amp;=-\\frac{1}{n}\\sum_{x}{[\\frac{y}{a}-\\frac{(1-y)}{(1-a)}]}\\\\ \u0026amp;=-\\frac{1}{n}\\sum_{x}{[\\frac{y}{a(1-a)}-\\frac{1}{1-a}]} \\end{align*} $$ 因此，有 $$ \\begin{align*} \\frac{\\partial L}{\\partial w} \u0026amp;=\\frac{\\partial L}{\\partial a}\\cdot\\frac{\\partial a}{\\partial z}\\cdot\\frac{\\partial z}{\\partial w}\\\\ \u0026amp;=-\\frac{1}{n}\\sum_{x}{[\\frac{y}{\\sigma(z)(1-\\sigma(z))}-\\frac{1}{1-\\sigma(z)}]}\\sigma\u0026rsquo;(z)x\\\\ \u0026amp;=-\\frac{1}{n}\\sum_{x}{[\\frac{y}{\\sigma(z)(1-\\sigma(z))}-\\frac{1}{1-\\sigma(z)}]}\\sigma(x)(1-\\sigma(z))x\\\\ \u0026amp;=-\\frac{1}{n}\\sum_{x}{(y-a)x} \\end{align*} $$ 因此，梯度更新受 $y-a$ 的影响，当误差大的时候，梯度更新快。\n凸优化性：\n梯度消失问题：Sigmoid 函数在输入较大或较小的情况下，梯度会趋近于零，导致训练过程中的梯度消失问题。而交叉熵损失函数的梯度只与误差大小有关。\n5. 关于交叉熵损失函数（Cross-Entropy）和平方损失（MSE）的区别 定义不一样\n参数更新速度不一样：\n均方差损失函数受 sigmoid 函数影响，导数更新缓慢。\n交叉熵损失函数参数更新只和误差有关，当误差大的时候，权重更新快；当误差小的时候，权重更新慢。\n使用场景不一样\nMSE 适合回归问题，CE 适合分类问题。\n6. BCE 和 CE 的区别 $BCE(y,\\hat{y})=-[y\\cdot\\log(\\hat{y})+(1-y)\\cdot\\log(1-\\hat{y})]$ 适用于二分类任务 通常与 sigmoid 一起使用：$sigmoid(x)=\\frac{1}{1+e^{-z}}$ $CE(y,\\hat{y})=-\\sum{y\\cdot\\log(\\hat{y})}$ 适用于多分类任务 通常与 softmax 一起使用：$softmax(x)=\\frac{e^{-z_i}}{\\sum_{k=1}^{n}{e^{-z_k}}}$ 7. 为什么交叉熵损失函数有 log 项 通过最大似然估计的方式求得交叉熵公式，这个时候引入 log 项。这是因为似然函数（概率）是乘性的，而 loss 函数是加性的，所以需要引入 log 项 “转积为和”，而且也是为了简化运算。\n8. 交叉熵损失函数的设计思想是什么 信息论的概念： 交叉熵是基于信息论中的熵和相对熵（KL 散度）的概念。熵用于度量随机事件的不确定性，而相对熵用于衡量两个概率分布之间的差异。在分类任务中，我们希望通过损失函数来最小化预测结果与真实标签之间的差异，从而减少不确定性并增加对正确类别的置信度。 最大似然估计： 交叉熵损失函数可以解释为最大似然估计的一种形式。在分类任务中，我们可以将模型的输出看作是对样本属于各个类别的概率分布的估计，而真实标签则表示样本的真实分布。通过最大化样本真实分布的似然概率，可以得到与真实标签最匹配的模型参数，从而提高分类准确性。 反映预测概率与真实标签之间的差异： 交叉熵损失函数的计算方式可以将模型的预测概率与真实标签之间的差异量化为一个标量值。通过最小化交叉熵损失函数，模型可以进行梯度下降优化，调整参数以使预测概率更接近真实标签。 9. IoU 的计算与实现 $$ IoU=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|} $$\nIOU（Intersection over Union），交集占并集的大小。其实现如下：\ndef IoU(A: Bbox, B: Bbox) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; 计算两个矩形框的IOU（Intersection over Union） 参数: - Bbox: 矩形框的坐标，格式为 (x0, y0, x1, y1) 返回值: - iou: 两个矩形框的IOU值 \u0026#34;\u0026#34;\u0026#34; H = min(A.x1,B.x1)-max(A.x0,B.x0) # 右侧小的横坐标-左侧大的横坐标 W = min(A.y1,B.y1)-max(A.y0,B.y0) if H\u0026lt;=0 or W\u0026lt;=0: return 0 C = H*W S = (A.x1-A.x0)*(A.y1-A.y0)+(B.x1-B.x0)*(B.y1-B.y0) return C/(S-C) 10. mIoU 的计算与实现 Mean Intersection over Union(MIoU， 均交并比)，为语义分割的标准度量。\n计算时在每个类上计算IoU，然后取平均。\n11. 如何选择 MAE 和 MSE 异常值敏感性：\nMSE 对异常值/离群值敏感，若数据中包含噪声或明显的异常值，MAE 会比 MSE 更加合适\n中位数优化：\nMAE 在优化过程中更倾向于找到中位数，而 MSE 更倾向于找到平均数，对于异常值而言，中位数比平均数更为鲁棒\n梯度更新：\nMAE 存在梯度始终相同的问题，即使对于很小的损失值，梯度也很大，需要调整学习率；而 MSE 使用固定的学习率也能有效收敛\n12. 什么是 Focal Loss Focal Loss 主要用于解决类别不平衡问题，主要思想是对难样本赋予更大的权重，对于易于分类的样本赋予较小的权重。 $$ FL(p_t)=-α_t\\cdot(1 - p_t)^γ\\cdot\\log(p_t) $$ 其中 $p_t$ 为预测概率，$\\alpha_t$ 是平衡因子，$\\gamma$ 是调制参数。\n当 $p_t$ 趋近 1 时，说明他是易分样本，$(1-p_t)^\\gamma$ 趋近于 0，对损失贡献较小；\n$\\alpha_t$ 用于解决类别不平衡问题，用于指定每个类别的权重，对于出现较少的类别，具有较大的权重。例如，在二分类中，前景使用 $\\alpha_t$ 时，背景为 $1-\\alpha_t$。\n13. 什么是 InfoNCE Loss InfoNCE（Normalized Information Neural Estimation）Loss 是一种用于训练自编码器、对比学习和表示学习模型的损失函数。它试图最大化正样本对之间的互信息，并最小化与负样本对之间的互信息。\nInfoNCE loss 公式如下： $$ L=-\\log\\cdot\\frac{\\exp(q\\cdot k_+/\\tau)}{\\sum_{i=0}^{k}(q\\cdot k_i/\\tau)} $$ $q,k_+$ 互为正样本对，在交叉熵里，k 表示类别总数，在 InfoNCE 里，k 表示负样本数量。本质是一个 k+1 分类任务，将 q 分类到 $k_+$ 类。\n温度系数 $\\tau$ 类似知识蒸馏里温度系数的作用，$\\tau$ 越大，logits 分布越平滑，对比损失将所有负样本一视同仁会，太过关注更困难的负样本；$\\tau$ 越小，logits 分布越集中，模型关注与正样本样本相似的难负样本，给予难负样本更大的梯度与正样本分离，但与正样本相似的难样本可能是潜在的正样本，可能导致模型难收敛或泛化性变差。\n即温度系数决定了对比损失对困难负样本的关注程度。\nPytorch 类代码实现如下：\n14. 什么是 KL 散度 KL 散度（Kullback–Leibler divergence，KLD）是一种统计学度量，表示的是一个概率分布相对于另一个概率分布的差异程度，在信息论中又称为相对熵（Relative entropy）。\n对于随机变量 $\\xi$，存在两个概率分布 $P,Q$，当变量为离散或连续时，KL 散度的定义如下： $$ \\mathbb{D}_{KL}(P||Q)=\\sum{P(x)\\log\\frac{P(x)}{Q(x)}} $$\n$$ \\mathbb{D}_{KL}(P||Q)=\\int P(x)\\log\\frac{P(x)}{Q(x)} dx $$\n其中，$Q(x)$ 为理论概率分布，$P(x)$ 为模型预测概率分布。KL 值越小，表明两个分布越接近。\nKL 散度基本性质如下：\n非负性：$\\mathbb{D}_{KL}\\geq0,\\mathbb{D}_{KL}=0$ 当且仅当 $P=Q$。 仿射不变性：$y=ax+b$，$\\mathbb{D}_{KL}(x)=\\mathbb{D}_{KL}(y)$ 非对称性：$\\mathbb{D}_{KL}(P||Q)\\neq\\mathbb{D}_{KL}(Q||P)$ 15. 为什么分类用交叉熵不用 MSE 同问题4，分类任务需要通过 sigmoid 或 softmax 将输出映射到 [0,1]，若使用 MSE，则梯度与激活函数的梯度有关，可能导致梯度消失或梯度爆炸，导致训练困难。而交叉熵损失则不会出现这个问题。\n","permalink":"https://Achilles-10.github.io/posts/tech/loss/","summary":"1. 什么是损失函数 损失函数衡量了模型预测结果与实际目标之间的差异或误差程度，指导模型参数的优化和训练过程。 损失函数可以分为经验风险损失函数（empirical risk loss function）和结构风险损失函数（structural risk loss function）。经验风险损失函数指预测结果和实际结果的","title":"深度学习面试题：损失函数"},{"content":"向量点乘与叉乘的概念及几何意义 向量点乘（内积）： 点乘(Dot Product) 的结果是点积，又称数量积或标量积(Scalar Product)。\n在空间中有两个向量 $\\vec{a}=(x_1,y_1),;\\vec{b}=(x_2,y_2)$，它们之间的夹角为 $\\theta$。\n从代数的角度看，点积是两个向量对应位置上的积的和。\n$$ \\vec{a}\\cdot\\vec{b}=x_1x_2+y_1y_2 $$\n从几何的角度看，点积是两个向量长度与它们夹角余弦的积。\n$$ \\vec{a}\\cdot\\vec{b}=|\\vec{a}||\\vec{b}|\\cos\\theta $$\n几何意义：\n点乘的结果表示 $\\vec{a}$ 在 $\\vec{b}$ 方向上的投影与 $|\\vec{b}|$ 的乘积 ，反映了两个向量在方向上的相似度，结果越大越相似，可用于判断是否正交，具体对应关系为\n$\\vec{a}\\cdot\\vec{b}\u0026gt;0$，表示夹角为锐角($0^\\circ\u0026lt;\\theta\u0026lt;90^\\circ$)\n$\\vec{a}\\cdot\\vec{b}=0$，表示夹角为直角($\\theta=90^\\circ$)\n$\\vec{a}\\cdot\\vec{b}\u0026lt;0$，表示夹角为钝角($\\theta\u0026gt;90^\\circ$)\n向量叉乘（外积） 叉乘(Cross Product) 又称向量积(Vector Product)。\n从代数的角度看：\n$$ \\vec{a}\\times\\vec{b}=x_1y_2-x_2y_1 $$\n从几何的角度看，运算结果是这两个向量所在平面的法向量。可以使用右手法则确定方向。如果 $\\vec{b}$ 在 $\\vec{a}$ 的逆时针方向，则叉乘结果大于零，否则小于零。\n$$ \\vec{a}\\times\\vec{b}=|\\vec{a}||\\vec{b}|\\sin\\theta\\vec{n} $$\n几何意义\n如果以 $\\vec{a}$ 和 $\\vec{b}$ 为边构成一个平行四边形，那么这两个向量外积的的模长与这个平行四边形面积相等。\n判断点是否在矩形内 如上图所示，只需判断该点是否在上下两条边之间和左右两条边之间即可。利用叉乘的方向性，分别判断。\n要满足点 $P$ 在 $P_1P_2$ 和 $P_3P_4$ 之间，有:\n$$ (P_1P_2\\times P_1P)*(P_3P_4\\times P_3P)\\geq0 $$\n同理，要满足 $P$ 在 $P_1P_2$ 和 $P_3P_4$ 之间，要满足:\n$$ (P_2P_3\\times P_2P)*(P_4P_1\\times P_4P)\\geq0 $$\n代码实现如下：\nclass Point: def __init__(self, x, y): self.x = x self.y = y # 计算 p1 x p2 def cross_product(p1, p2): return p1.x * p2.y - p1.y * p2.x # 计算 |p1 p2| x |p1 p| def get_cross(p1,p2,p): return cross_product(Point(p2.x - p1.x, p2.y-p1.y), Point(p.x-p1.x, p.y-p1.y)) def isPointInMatrix(matrix, x,y): \u0026#34;\u0026#34;\u0026#34; p1,p2,p3,p4 分别是矩形 左上角、左下角、右下角、右上角 \u0026#34;\u0026#34;\u0026#34; p1 = Point(matrix[0][0], matrix[0][1]) p2 = Point(matrix[1][0], matrix[1][1]) p3 = Point(matrix[2][0], matrix[2][1]) p4 = Point(matrix[3][0], matrix[3][1]) p5 = Point(x,y) return get_cross(p1,p2,p5) * get_cross(p3,p4,p5) \u0026gt;= 0 and get_cross(p2,p3,p5) * get_cross(p4,p1,p5) \u0026gt;= 0 def test_ispointinmatrix(): matrix = [[0,4],[-1,2],[3,0],[4,2]] print(isPointInMatrix(matrix, 3,3)) test_ispointinmatrix() 其他方法\n点到矩形中心距离法： 计算点到矩形的中心的距离，如果这个距离小于矩形的一半长和一半宽的话，那么点在矩形内； 射线：从判断点向某个统一方向作射线，依交点个数的奇偶判断； 转角：按照多边形顶点逆时针顺序，根据顶点和判断点连线的方向正负（设定角度逆时针为正）求和判断； 夹角和：求判断点与所有边的夹角和，等于360度则在多边形内部； 面积和：求判断点与多边形边组成的三角形面积和，等于多边形面积则点在多边形内部。 ","permalink":"https://Achilles-10.github.io/posts/tech/rectangle/","summary":"向量点乘与叉乘的概念及几何意义 向量点乘（内积）： 点乘(Dot Product) 的结果是点积，又称数量积或标量积(Scalar Product)。 在空间中有两个向量 $\\vec{a}=(x_1,y_1),;\\vec{b}=(x_2,y_2)$，它们之间的夹角为 $\\theta$。 从代数的角度看，点积是两","title":"判断点是否在矩形内？复习向量点乘与叉乘"},{"content":"1. 什么是初始化方法 1.1 什么是网络参数初始化 在训练神级网络之前，对模型的参数进行初始赋值。合适的初始化方法可以加速收敛，提升性能。\n1.2 常用的初始化方法 随机初始化：通过某个分布（高斯分布、均匀分布）来选择初始值来初始化参数，可以破坏对称性，使不同神经元学习到不同的特征； 零初始化：将所有参数初始化为零，容易导致网络对称性和梯度消失的问题； Xavier 初始化：适用于具有线性激活函数的网络，它根据输入和输出维度来调整参数的初始化范围，以确保梯度的传播保持合适范围，避免梯度爆炸或梯度消失； He 初始化：针对具有 ReLU 激活函数的初始化方法，与 Xavier 类似，根据激活函数的性质进行调整以确保合适的梯度传播范围。 2. 为什么需要合理的参数初始化 若初始化不合理，可能导致梯度消失或梯度爆炸。\n理想的参数初始化是，经过多层网络后，信号不被过分放大或过分减弱。数学的方法就是使每层网络的输入和输出的方差已知，且尽量保证每层网络参数的分布均值为0，方差不变。\n3. 详细讲解初始化方法 3.1 全零初始化 3.2 随机初始化 随机参数服从零均值高斯分布、正态分布或均匀分布。 $$ w = 0.001*randn(N_{in},N_{out}) $$ 0.001 是控制因子，避免参数过大，也存在其他合理的小数。\n3.3 Xavier 初始化 随机初始化没有控制方差，要解决梯度消失，需要控制方差，因此对 $w$ 进行规范化： $$ w = \\frac{0.001*randn(N_{in},N_{out})}{\\sqrt{n}}\\\\ $$ 其中 $n=N_{in}\\ or\\ \\frac{N_{in}+N_{out}}{2}$，维持了输入输出数据分布方差一致性。Xavier 需要配合 BN 和线性激活函数（sigmoid，tanh）。\n3.4 He 初始化 由何凯明提出，对应非线性初始化函数（ReLU）。当 ReLU 的输入小于 0 时，其输出为 0，相当于该神经元被关闭了，影响了输出的分布模式。\n因此 He 初始化，在 Xavier 的基础上，假设每层网络有一半的神经元被关闭，于是其分布的方差也会变小。经过验证发现当对初始化值缩小一半时效果最好，故 He 初始化可以认为是 Xavier 初始 / 2 的结果。\n3.5 预训练 初始化权重：通过预训练模型的权重，可以提供更好的初始化，使得模型更容易收敛和学习任务特定的特征。 迁移学习：通过预训练模型，在目标任务上微调模型可以更快地学习和适应目标任务的特征。这对于具有较小的目标任务数据集的情况非常有用。 数据效率：通过使用大规模的预训练数据，可以提取通用的特征表示，从而更好地利用有限的目标任务数据。 4. 零初始化有什么问题 将所有权重置零，神经网络通过梯度更新参数，参数都是零，梯度也就是零，神经网络就停止学习了。\n5. 随机初始化有什么问题 随机初始化没有控制方差，对于深层网络，可能造成梯度消失或梯度爆炸。\n6. 怎么缓解梯度消失 梯度截断，如 ReLU6，将输出在 6 处截断，控制值域在 [0,6] 使用残差连接 使用 BN 控制数据分布在激活函数合适的区域 使用 ReLU、LReLU、ELU、maxout 等激活函数 使用合理的参数初始化方法 预训练 + 微调 7. 梯度消失的根本原因是什么 在深度神经网络求梯度时，由于链式法则，多个小于 1 的梯度相乘，最浅层的网络梯度趋于 0，无法进行更新；并且 Sigmoid 函数也会因为初始权值过小而趋近于 0，导致梯度趋近于 0，也导致了无法更新。\n8. 说说归一化方法 数据标准化是对数据进行预处理的一种方式，将数据按特征进行缩放，这有助于使不同特征之间的尺度一致，提高模型的收敛速度和性能。\nz-score 标准化\n将数据转化为以均值为 0，标准差为 1 的标准正态分布。 $$ z=\\frac{x-\\mu}{\\sigma} $$\nmin-max 标准化\n将数据线性地缩放到给定的范围内，通常是 [0, 1] 或 [-1, 1]。 $$ z=\\frac{x-x_{min}}{x_{max}-x_{min}} $$\n9. 为什么要归一化 消除特征间的量纲差异：不同特征往往具有不同的度量单位和尺度，例如身高和体重，它们的取值范围差异较大。如果不对这些特征进行归一化，可能导致模型受到具有较大尺度的特征的影响，而忽略了其他特征的重要性。归一化可以消除特征间的量纲差异，使得模型更公平地对待各个特征，可能提高精度； 提高模型收敛速度：某些优化算法（如梯度下降）在训练过程中对输入数据的尺度较为敏感。如果数据未经归一化，可能导致优化过程变慢或不稳定。通过归一化，可以使得优化算法更快地收敛，加快模型的训练速度。 ","permalink":"https://Achilles-10.github.io/posts/tech/init/","summary":"1. 什么是初始化方法 1.1 什么是网络参数初始化 在训练神级网络之前，对模型的参数进行初始赋值。合适的初始化方法可以加速收敛，提升性能。 1.2 常用的初始化方法 随机初始化：通过某个分布（高斯分布、均匀分布）来选择初始值来初始化参数，可以破坏对称性，使不同神经元学习到不同的特征； 零初始化：将所有参","title":"深度学习面试题：初始化方法"},{"content":"1. Python 深拷贝与浅拷贝 浅拷贝（copy）：拷贝父对象，不会拷贝对象内部的子对象；类似指针和引用。\n深拷贝（deepcopy）：copy 模块的 deepcopy 方法，完全拷贝父对象及其子对象。\n2. Python 多线程能用多个 cpu 吗？ 不能。Python 解释器使用了 GIL(Global Interpreter Lock)，在任意时刻只允许单个 python 线程运行。\nGIL 为全局解释器锁，其功能是在 CPython 解释器中执行的每一个 Python 线程都会先锁住自己，以阻止别的线程执行。\n利用 multiprocessing 库可以在多个 CPU 上并行运行。在多进程编程中，每个进程都有自己独立的 Python 解释器和内存空间，因此不存在 GIL 的限制。\n2.1 什么是Python的GIL? GIL 是 Python 的全局解释器锁，是 CPython 解释器的实现特性。它是一个互斥锁，阻止多个线程同时执行 Python 程序，这意味着即使在多核处理器上，Python代码也无法同时在多个核心上运行。\n2.2 GIL对Python的性能有何影响? GIL 导致 Python 在多线程环境下，无法充分利用多核处理器。即使在多核处理器上，Python 的多线程代码无法实现真正的并行运行，这对于计算密集型任务来说，可能会成为性能瓶颈。\n2.3 为什么Python要设计GIL? GIL的存在可以简化 Python 对象模型的实现，防止数据竞争和状态冲突，使得对象管理变得更加简单。此外，GIL也使得 CPython 的实现变得更加简单，对于某些依赖于 C 扩展的 Python 程序来说，这是非常重要的。\n2.4 如何避免GIL对性能的影响? 对于 IO 密集型任务，由于大部分时间都在等待 IO，所以 GIL 的影响并不大。对于计算密集型任务，可以使用多进程模块 multiprocessing，因为每个进程都有自己的解释器和内存空间，因此可以避免 GIL 的影响。\n3. Python 垃圾回收机制 [参考]\nPython 垃圾回收机制：以引用计数器为主，标记清除和分代回收为辅。\n引用计数：每个对象内部都维护了一个值，记录此对象被引用的次数，若次数为 0，则 Python 垃圾回收机制会自动清除此对象； 标记-清除（Mark-Sweep）：遍历所有对象，如果有对象引用它，则标记为可达的（reachable）；再次遍历对象，如果某个对象没有被标记为可达，则将其回收。解决循环引用问题 分代回收：对象存在时间越长，越可能不是垃圾，应该越少去收集。根据对象存活时间划分为不同集合（代），Python 将内存分为了 3 代。 当代码中主动执行 gc.collect() 命令时，Python 解释器就会进行垃圾回收。 4. Python 里的生成器是什么 生成器（Generator）是一种特殊的迭代器，生成器函数通过 yield 来生成一个值，并暂停执行保存当前状态。可以有效节省内存空间，执行过程可以被暂停和恢复。\n实例如下：\ndef countdown(n): while n \u0026gt; 0: yield n n -= 1 # 使用生成器函数创建生成器对象 generator = countdown(4) # 通过迭代生成器对象获取值 for value in generator: print(value) # 输出结果为 # 4 # 3 # 2 # 1 5. 迭代器和生成器的区别 [参考]\n迭代器是一种对象，有 __iter__ 和 __next__ 方法，每次调用 __next__ 方法都返回可迭代对象中的下一个元素，当没有更多元素可返回时，会引发 StopIteration 异常（防止无限循环）。Python 中迭代器是一个惰性序列。\n用 iter() 内建方法可以把 list、dict、str 等可迭代对象转换成迭代器。\nlist0 = [0, 1, 2] iter0 = iter(list0) print(type(iter0)) # \u0026lt;class \u0026#39;list_iterator\u0026#39;\u0026gt; 生成器是一种特殊的迭代器。可将生成列表和字典的推到表达式的中括号换成小括号，就得到了生成器表达式。可以借助 next() 方法来获取下一个元素。\nlist_generator = (x * x for x in range(5000)) for i in list_generator: print(i) 区别：\n6. 装饰器 [参考]\n装饰器本质上是一个函数，它接受一个函数作为输入，并返回一个新的函数作为输出，这个新函数通常会在原函数的基础上添加一些额外的功能或行为。\n装饰器的作用是实现代码的重用和功能的动态扩展，它可以在不修改原函数代码的情况下，对函数进行功能增强、日志记录、性能统计等操作。\n以下是一个装饰器示例，用于记录函数的执行时间：\nimport time def timer(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() execution_time = end_time - start_time print(f\u0026#34;函数 {func.__name__} 执行时间：{execution_time} 秒\u0026#34;) return result return wrapper @timer def my_function(): time.sleep(2) print(\u0026#34;执行完成\u0026#34;) my_function() # 执行完成 # 函数 my_function 执行时间：2.0054352283477783 秒 7. Python 有哪些数据类型 数字类型（Numeric Types）：整数 int，浮点数 float，复数 complex 等； 字符串类型（String Type）：由字符组成的序列 str，用于表示文本信息； 列表类型（List Type）：有序可变的集合，可以包含任意类型的元素，使用方括号 [] 表示； 元组类型（Tuple Type）：有序不可变的集合，可以包含任意类型的元素，使用圆括号 () 表示； 集合类型（Set Type）：无序的可变集合，不允许重复元素，使用花括号 {} 表示； 字典类型（Dictionary Type）：无序的键值对集合，用于存储具有唯一键的值，使用花括号 {} 表示； 布尔类型（Boolean Type）：表示真或假的值，包括 True 和 False 两个取值； 空值类型（None Type）：表示空对象或缺失值的特殊类型，只有一个取值 None。 8. Python 中列表 List 的 del，remove 和 pop 的用法和区别 del，是一个关键字，用于删除整个列表或列表中指定位置的元素 删除指定位置的元素：del 关键字加上列表名和索引，例如 del my_list[index]； 删除整个列表：del 关键字加上列表名，例如 del my_list。 remove，是列表的方法，用于删除列表中第一个匹配的指定元素 参数为要删除的元素，例如 my_list.remove(element)，将删除列表中第一个匹配元素 element；若 element 不在列表中则抛出 ValueError。 pop，是列表方法，用于删除并返回指定指定位置的元素 参数为要删除元素的索引，默认为列表最后一个元素，例如 my_list.pop(0)，将删除列表的第一个元素并返回该元素的值。 9. Python yeild 和 return 的区别 return 用于返回最终结果并终止函数执行，yield 用于定义生成器函数，可以通过多次调用来逐步产生结果； return 只能返回一次值，yield 可以多次返回值并保留函数状态，使函数可从上次 yield 语句的位置继续执行。 10. Python set 的底层实现 set 底层是基于哈希表实现的，发生哈希冲突时使用开放地址法或链表解决冲突。有以下特点：\n元素的添加、删除和查找平均时间复杂度为 $O(1)$； set 中元素使无序的，每次遍历的顺序可能不同； set 中的元素不重复，重复添加的元素只会保留一个； set 不支持索引操作，因为元素的存储位置不是固定的。 11. Python 字典和 set() 的区别 存储结构：字典是键值对的集合；set 是元素的集合，没有键值对； 唯一性：字典中键是唯一的；set 中元素是唯一的； 访问方式：字典通过键来访问值，my_dict[key]；set 只能通过遍历或 in 来判断某元素是否存在于集合中； 可变性：字典是可变数据类型，可以增删改；set 也是可变的，可以添加删除元素； 有序性：字典和 set 都是无序的。 12. 怎么对字典的值进行排序 转化为元素 (key,val) 的形式用 sorted() 排序； 设置 sorted 中 key 的参数的值 # 方法一 z = zip(d.values(),d.keys()) #x = zip(d.itervalues(),d.iterkeys()) 迭代类型，省空间 s = sorted(z) # 方法二 a = d.items() s = sorted(a,key= lambda x:x[1]) 13. __init__、__new__ 和 __call__ 的区别 __new__ 方法用于对象的创建，__init__ 方法用于对象的初始化。 __new__ 方法在对象创建前被调用，__init__ 方法在对象创建后被调用。 __call__ 方法用于将对象作为函数进行调用，可以赋予对象函数的行为。\n14. Python 的 lambda 函数 lambda 函数是一种匿名函数，由关键字 lambda 后面跟上参数列表，跟上一个冒号和表达式作为函数的返回值。\nlambda arguments: expression\n# 求两数之和 add = lambda x, y: x + y # 对列表排序 sorted_numbers = sorted(my_list, key=lambda x:x) # 作为 map 函数的参数 squared_numbers = map(lambda x:x**2, my_list) 为什么要使用 lambda 函数？\n匿名函数可以在任何需要的地方使用，但是这个函数只能使用一次，因此 lambda 函数也称为丢弃函数，它可以与其他预定义函数（如filter(),map()等）一起使用。相对于我们定义的可重复使用的函数来说，这个函数更加简单便捷。\n15. Python 内存管理 Python 使用引用计数和垃圾回收两种方式来管理内存。\n引用计数：当一个对象的引用计数变为0时，Python 会立即回收该对象的内存。 垃圾回收：Python 使用垃圾回收机制来处理循环引用的特殊情况。垃圾回收机制会周期性地扫描所有的对象，找出不再被引用的对象，并回收它们的内存空间。 16. Python 在内存上做了哪些优化 引用计数 垃圾回收：Python 使用标记-清除算法（Mark and Sweep）和分代回收算法（Generational Garbage Collection）来回收不再使用的对象，释放内存空间。 内存池：Python通过内存池管理器（Memory Pool Manager）来分配内存。内存池预先在内存中申请一定数量的，大小相等的内存块留作备用，当有新的内存需求时，就先从内存池中分配内存给这个需求，不够之后再申请新的内存。这样做最显著的优势就是能够减少内存碎片，提升效率。 内存共享：对于一些不可变对象（如小整数、字符串等），Python会进行内存共享。多个变量引用相同的不可变对象时，它们可以共享相同的内存空间，从而节约内存。 迭代器和生成器：Python中的迭代器和生成器可以节省大量内存，特别是处理大型数据集时。它们按需生成数据，而不是一次性将所有数据加载到内存中。 字符串驻留机制：对于一些短字符串，Python会将其驻留（intern）在内存中，即多个变量引用相同的字符串对象。这种机制可以减少相同字符串的内存使用。 17. Python 中类方法（class method）和静态方法（static method）的区别 装饰器不同：类方法使用 @classmethod，静态方法使用 @staticmethod； 参数不同：类方法接收类 cls 作为隐式第一个参数；静态方法不接收隐式的第一个参数，类似普通函数； 访问类属性和方法的方式不同：类方法可以访问和修改类属性，也可以调用其他类方法和静态方法；静态方法不能访问类属性，也不能调用其他类方法或静态方法，只能访问静态方法中定义的局部变量； 使用场景不同：类方法通常用于执行与类相关的操作，例如创建类的实例或修改类属性。静态方法通常用于执行与类无关的操作，它们在类的命名空间中定义，但与类的状态无关。 类方法和静态方法均可通过类名或实例对象（类方法不推荐）来调用。 class MyClass(object): # 实例方法 def instance_method(self): # 需要传入 self print(\u0026#39;instance method called\u0026#39;, self) # 类方法 @classmethod def class_method(cls): print(\u0026#39;class method called\u0026#39;, cls) # 静态方法 @staticmethod def static_method(): print(\u0026#39;static method called\u0026#39;) 18. Python 多线程怎么实现 可以用以下模块：\n_thread threading Queue multiprocessing 19. 点积和矩阵相乘的区别 点积：又叫内积，数量积，由两个维度相同的的向量相乘求和，得到一个标量。$a\\cdot b=\\sum_{i}^{n}{a_1b_1+a_2b_2+\\dots+a_nb_n}$\n矩阵乘法：得到仍然是一个矩阵\n20. Python 中错误和异常处理 Python 中会发生两种类型错误：\n语法错误：如果未遵循正确的语言语法，则会引发语法错误，这种错误编译器会指出； 逻辑错误（异常）：在运行时中，通过语法测试后发生错误的情况称为异常或逻辑类型。比如除数除以0，运行时就会抛出一个抛出一个 ZeroDivisionError 异常，叫处理异常。处理方式为：通过 try..except..finally 代码块来处理捕获异常并手动处理。 21. Python 的传参是传值还是传址 在Python中，函数参数传递方式是\u0026quot;传对象引用\u0026quot;，也可以称为\u0026quot;传对象的地址\u0026quot;，即“传址”。根据对象的类型（可变对象和不可变对象），在函数内部对参数对象的操作可能会有不同的效果。\n22. 什么是猴子补丁 猴子补丁（Monkey Patching）是指在运行时动态修改已有的代码，通常是在不修改原始代码的情况下添加、替换或修改现有的功能。\n例如，很多代码用到 import json，后来发现ujson性能更高， 如果觉得把每个文件的import json 改成 import ujson as json成本较高，或者说想测试一下用ujson替换json是否符合预期，只需要在入口加上：\nimport json import ujson def monkey_patch_json(): json.__name__ = \u0026#39;ujson\u0026#39; json.dumps = ujson.dumps json.loads = ujson.loads monkey_patch_json() 23. CPython 退出时是否释放所有内存分配 [参考]\n当 Python 退出时，从全局命名空间或 Python 模块引用的对象并不总是被释放。 如果存在循环引用，则可能发生这种情况，C库分配的某些内存也是不可能释放的（例如像 Purify 这样的工具）。 但是，Python在退出时清理内存并尝试销毁每个对象。\n24. Python 中 is 和 == 有什么区别 is 比较两个对象的 id 值是否相等，即是否指向同一个内存地址；== 比较两个对象的值是否相等，默认调用对象的 __eq__() 方法。\n25. gbk 和 utf8 的区别 GBK编码专门用来解决中文编码的，是双字节的。\nUTF－8 编码是用以解决国际上字符的一种多字节编码，它对英文使用8位（即一个字节），中文使用24位（三个字节）来编码。\n26. 遍历字典可以用什么方法 my_dict = {\u0026#39;name\u0026#39;: \u0026#39;Jack\u0026#39;, \u0026#39;age\u0026#39;: 26, \u0026#39;address\u0026#39;: \u0026#39;Downtown\u0026#39;, \u0026#39;phone\u0026#39;: \u0026#39;1234567890\u0026#39;} for i in my_dict: # 遍历字典中的键 print(i) for key in my_dict.keys(): # 遍历字典中的键 print(key) for value in my_dict.values(): # 遍历字典中的值 print(value) for item in my_dict.items(): # 遍历字典中的元素 print(item) 27. 反转列表的方法 reversed() 迭代器 sorted() 指定 reverse [::-1] 28. Python 元组中等元组转为字典 my_tuple = ((1, \u0026#39;2\u0026#39;), (3, \u0026#39;4\u0026#39;), (5, \u0026#39;6\u0026#39;)) my_dict = dict((y,x) for x,y in my_tuple) 29. range 在 Python2 和 Python3 里的区别 py2 中，range 得到一个列表；\npy3 中，range 得到一个生成器；\n30. __init__.py 文件的作用和意义 [参考]\n这个文件定义了包的属性和方法，可以只是一个空文件，但是必须存在。一个文件夹根目录下存在__init__.py那就会认为该文件夹是Python包，否则那这个文件夹就是一个普通的文件夹。\n31. Python 列表去重 my_list = list(set(my_list)) ","permalink":"https://Achilles-10.github.io/posts/tech/python/","summary":"1. Python 深拷贝与浅拷贝 浅拷贝（copy）：拷贝父对象，不会拷贝对象内部的子对象；类似指针和引用。 深拷贝（deepcopy）：copy 模块的 deepcopy 方法，完全拷贝父对象及其子对象。 2. Python 多线程能用多个 cpu 吗？ 不能。Python 解释器使用了 GIL(Global Interpreter Lock)，在任意时刻只允许单个 python 线程运行。 GIL 为全局解","title":"深度学习面试题：Python"},{"content":"一、卷积基础知识 关键词：卷积操作、卷积核、感受野（Receptive Field）、特征图（feature map）、卷积神经网络\n1. 简述卷积的基本操作，并分析其与全连接层的区别 在卷积操作中，卷积核与输入图像上滑动，进行点乘操作，然后求和得到一个单个值，这个值作为输出特征图中的一个像素，每个卷积核都会产生一个对应的特征图。\n全连接层的输出层的每个节点与输出层的每个节点都有权重连接，而卷积层具有局部连接、权值共享和输入/输出数据结构化的特点，参数量和计算复杂度远小于全连接层，并且与生物视觉传导机制有一定的相似性。\n局部连接：卷积核尺寸小于输入特征图尺寸，输出层上的每个节点都只有输入层的部分节点连接；而全连接层中节点之间的连接是稠密的； 权值共享：由于卷积核的滑动窗口机制，输出层不同位置的节点与输入层的连接权值是一样的（卷积核参数）；全连接层中不同节点之间的权值是不同的； 输入/输出数据的结构化：卷积操作不会改变输入数据的结构信息；而全连接层的的输出数据会被展成（flatten）扁平的一维数组； 平移不变性：即对于输入图像中的某个特征，如果在图像中移动它，卷积层仍然可以检测到相同的特征；全连接层通常不具备这种平移不变性； 输入尺寸： 卷积层对于不同尺寸的输入图像都可以处理；全连接层输入的大小是固定的。 2. 在卷积神经网络中，如何计算各层的感受野大小 感受野的定义是，对于某层输出特征图上的某个点，在原始输入数据上能影响到这个点的取值的区域。\n若第 i 层为卷积层或池化层，不考虑padding，则有： $$ F(i)=[F(i-1)-1]\\times s+K\\\\ K=(k-1)\\times \\text{dilation}+1=(r-1)*(k-1)+k $$ 其中，s 表示 stride，k 表示 kernel_size，$F(0)=1$。\n若第 i 层为激活层、归一化层，则步长为1，感受野大小 $F(i)=F(i-1)$。\n若第 i 层为全连接层，则感受野为整个输入数据全域，即 $F(i)=L_{input}$\n3. 卷积层的输出尺寸、参数量和计算量 输出尺寸： $$ H_{out}=\\lfloor\\frac{H_{in}+2\\times p-K}{s}\\rfloor+1 $$ 其中，p 为 padding，s 为 stride，$K=(k-1)\\times\\text{dilation}+1$，k 为 kernel_size。\n向下取整是放弃了左上侧的部分数据，使得卷积核滑动窗能够恰好到达右下角的点。\npadding=same：在特征图上下/左右共填充 k-1 行/列，此时输出 $H_{out}=\\lfloor\\frac{H_{in}-1}{s}\\rfloor+1$ padding=valid：不对输入特征图进行边界填充，直接放弃右下侧卷积核无法滑动到的区域。 padding 的作用：\n避免边缘信息丢失 保持特征图大小 控制输出大小 参数量：\n每个卷积核的参数量为 $C_{in}K_wK_h+1$ ，1 表示偏置，若不考虑偏置则可以忽略。有 $C_{out}$ 个卷积核，则参数量为： $$ C_{out}(C_{in}K_wK_h+1) $$\n计算量：\n卷积层的计算量主要取决于三个因素：输出特征图的尺寸、卷积核的尺寸和输出通道数。\n卷积操作的计算量大约为 $C_{in}K_wK_h$，卷积核滑动的次数为输出特征图的数据个数，即 $C_{out}L_wL_h$，因此整体的计算量为： $$ C_{in}C_{out}L_w^{(o)}L_h^{(o)}K_wK_h $$\n二、卷积的变种 关键词：分组卷积（Group Convolution）、转置卷积（Transposed Convolution）、空洞卷积（Dilated Convolution）、可变形卷积（Deformable Convolution）\n1. 简述分组卷积及其应用场景 在普通卷积中，由于一个卷积核对应输出特征图的一个通道，而每个卷积核会作用到输入特征图的所有通道上，因此普通卷积在“通道”这个维度上是“全连接”的。\n分组卷积是将输入通道和和输出通道划分同样的组数，仅让处于相同组号的输入输出通道相互进行“全连接”。若记 g 为划分的组数，则分组卷积能降低计算量和参数量为原来的 1/g。\n分组卷积最初在 AlexNet 网络中引入，用于解决单个 GPU 无法处理含有较大计算量和存储需求的卷积层这个问题，用分组卷积将计算和存储分配到多个GPU上并行计算。目前分组卷积更多用于构建小型网络模型，例如深度可分离卷积（depthwise ）。\n分组卷积潜在的问题：\n效率提升不如理论上明显：对内存的访问频繁程度并未降低，且现有 GPU 加速库（cuDNN）对其优化程度有限；\n信息交互弱：因为它独立地处理每个组，可能无法充分利用输入通道之间的相关性。\n深度可分离卷积（Depthwise Separable Convolution）：\n将标准的卷积操作分解为两个步骤：深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）：\n深度卷积（Depthwise Convolution）： 深度卷积独立地对输入的每个通道进行卷积操作，对于输入通道数为C的输入特征图，它使用C个卷积核进行卷积。参数量为 $C_{in}\\times K\\times K$。 逐点卷积（Pointwise Convolution）： 在深度卷积之后，使用1x1的卷积改变深度卷积输出的通道数。参数量为 $C_{in}\\times C_{out}$。 总的参数量为 $C_{in}\\times K\\times K + C_{in}\\times C_{out}$。\n深度可分离卷积能够减少参数量和计算量，加速推理过程，实现轻量级模型和移动端应用。但同样存在信息交互弱的问题，可能对模型的性能产生一定影响。\n2. 1x1 卷积的作用 1x1 卷积，也称为逐点卷积（Pointwise Convolution），卷积核大小为 1x1，不考虑输入数据局部信息之间的关系，而把关注点放在不同通道间。\n作用：\n降维和增加通道数： 1x1 卷积可以降低特征图的通道数，从而减少网络的参数量和计算复杂度。同时，它也可以增加通道数，引入更多的特征表示。 特征融合： 1x1 本质是对每个像素点在不同通道上进行线性组合，可以用于特征的融合，从而提高模型的表征能力。 非线性映射： 尽管1x1卷积只是在通道上进行线性组合，但之后可以通过非线性激活函数进行非线性映射，增加网络的表达能力。 3. 简述转置卷积的主要思想以及应用场景 转置卷积（Transposed Convolution），也称为反卷积（Deconvolution）。主要思想是将卷积操作中的输入和输出交换，从而将低维特征映射扩展到高维特征映射。\n转置卷积是通过卷积核的翻转和补零操作来实现的，具体实现时，以二维卷积为例，一个卷积核尺寸为 $k_w\\times k_h$，滑动步长 $(s_w,s_h)$，边界填充尺寸为 $(p_w,p_h)$ 的普通卷积：\n如果 $s\u0026gt;1$，对输入特征图进行扩张（上采样）：相邻数据点之间填充 $s_{w/h}-1$ 个零； 对输入特征图进行边界填充：对左右/上下两侧分别填充 $\\hat{p}{w/h}=k{w/h}-p_{w/h}-1$ 个零列/行； 变换后再输入特征图上做卷积核大小为 $k_w\\times k_h$，滑动步长为 $(1,1)$ 的普通卷积。 输出特征图大小为 $H_{out} = s\\times(H_{in}-1)+k$\n转置卷积的应用场景：\n图像分割和语义分割： 转置卷积可用于将低分辨率的特征图上采样到原始输入图像大小，以生成像素级的分割结果。 图像生成和超分辨率重建： 在图像生成任务中，例如GAN（生成对抗网络）中的生成器部分，转置卷积用于将输入的随机噪声扩展成高分辨率的图像。同样，转置卷积在超分辨率重建中也能够将低分辨率图像上采样成高分辨率图像。 卷积神经网络中的上采样： 使用转置卷积进行上采样，提高特征图的尺寸，以便更好地捕捉图像中的细节和全局信息。 4. 简述空洞卷积的设计思路 空洞卷积（Dilated Convolution），也称为膨胀卷积或扩张卷积。通过在卷积核中引入扩张率（dilation rate）这个超参数来指定相邻采样点之间的间隔：扩张率为 r 的空洞卷积，卷积核上相邻数据点之间有 r-1 个空洞，如下图所示，这是一个卷积核大小为 3，扩张率为 2 的空洞卷积。\n空洞卷积的感受野为 $F = (r-1)*(k-1)+k$，当卷积核大小为 3，扩张率为 2 时，感受野计算方式如下图所示。经过一层空洞卷积感受野为 $3\\times 3$，两层空洞卷积后为 $5\\times 5$。\n空洞卷积利用空洞结构扩大了卷积尺寸，不经过下采样操作即可增大感受野，同时还保留了输入数据的内部结构，能够捕捉更广阔的上下文信息，从而增强卷积神经网络的感知能力。\n5. 可变形卷积旨在解决哪类问题 可变形卷积（Deformable Convolution）是一种改进的卷积操作，旨在解决传统卷积在处理不规则形状和变形目标时的不足。可变形卷积通过引入可学习的偏移量来动态调整卷积核的采样位置，从而适应不同目标的变形和不规则形状，提高模型对于复杂场景的表征能力。\n如下图所示，（a）是普通卷积和，（b）（c）（d）是可变形卷积核。\n当偏移后的采样点不是整数时，需要用双线性插值来计算对应的特征值。\n三、卷积神经网络的整体结构 关键词：AlexNet、VGGNet、Inception、ResNet\n1. 简述卷积神经网络近年来在结构设计上的主要发展和变迁（从 AlexNet 到 ResNet 系列） AlexNet\n网络结构是堆砌的卷积层和池化层，在网络末端加上全连接层和 Softmax 以处理多分类任务。\n采用 ReLU 替换 Sigmoid 作为激活函数，缓解深层网络梯度消失的问题； 引入局部响应归一化（Local Response Normalization，LRN）； 应用 Dropout 和数据扩充（data augmentation）来提升效果； 使用步长小于池化核的重叠最大池化； 使用分组卷积来突破当时 GPU 的显存瓶颈。 VGGNet\n相较于 AlexNet，VGGNet 做了以下改变：\n用多个 $3\\times3$ 小卷积核替代之前的 $5\\times 5,\\ 7\\times 7$ 大卷积核，这样可以在更少参数量合计算量的情况下获得同样的感受野和更大的网络深度； 用 $2\\times2$ 池化核替代之前的 $3\\times3$ 池化核； 去掉局部响应归一化模块。 GoogLeNet / Inception-v1\n提出 Inception 模块，将大通道卷积层替换为多个小通道卷积层组成的分支结构。\n提出瓶颈（bottleneck）结构，即在计算大卷积层之前先用 $1\\times1$ 卷积对通道压缩以减少计算量； 网络中间层使用多条分支，连接辅助分类器，缓解梯度消失问题； 修改 VGGNet 等网络在末端使用多个全连接层的做法，降低一个全连接层换成平均池化层（Global Average Pooling）； Inception-v2 和 Inception-v3\n提出四点网路结构设计的准则：\n避免表达瓶颈（representational bottleneck），尤其是网络前几层。尽量让网络从前到后各层的信息表征能力逐渐降低，不能突然剧烈下降或中间某些节点出现瓶颈； 特征图通道越多，能表达的解耦信息越多，更容易进行局部处理，加速网络训练过程； 若要在特征图上做空域的聚合操作，可以先压缩特征图通道； 在限定总计算量的情况下，网络结构的深度和宽度需要平衡； 文中提出两种卷积分解思路：\n将 $5\\times5$ 卷积核分解为两个 $3\\times3$ 小卷积核，更一般的，将 $(2k+1)\\times(2k+1)$ 卷积核分解为 k 个 $3\\times3$ 卷积核。如下图 (b)； 将 $k\\times k$ 卷积核分解为 $1\\times k$ 和 $k\\times 1$ 卷积核的串联/并联。如下图 (c) 和 (d)； 为了缓解单纯用池化层下采样带来的表达瓶颈问题，在原始 Inception 模块上修改，将每条支路最后一层的步长改为2，如下图。\nResNet\nResNet 的提出基于这样一种现象：随着网络层数加深，训练误差和测试误差都会上升，这种现象称为网络退化（degeneration）。ResNet 使用跳层连接（shortcut connection）来解决这个问题，跳层连接有以下两点好处：\n抑制梯度消失的现象，使网络在加深时性能不会下降； 由于 “近道” 的存在，若网络在层数加深退化时，可以通过控制 “近道” 和 “非近道” 的组合比例来退回到之前浅层时的状态，即 “近道” 具备自我关闭能力。 Inception-v4 和 Inception-ResNet\nv4 在 v3 的基础上修改了网络初始几层的结构（Stem），同时应用了 Inception-A、Inception-B 和 Inception-C 模块，提出 Reduction-A、Reduction-B 模块。\nResNeXt 缩小了瓶颈比，并将中间的普通卷积改为分组卷积。\n四、卷积神经网络的基础模块 关键词：批归一化（Batch Normalization, BN）、全局平均池化（Global Average Pooling）、瓶颈结构、沙漏结构（hourglass）\n1. 批归一化是为了解决什么问题？它的参数有何意义？它在网络中一般放在什么位置？ 解决梯度消失和梯度爆炸问题：批归一化通过将输入数据在每个批次上进行标准化，可以使得数据的分布更稳定，有助于缓解梯度消失和梯度爆炸问题； 加速收敛：批归一化避免训练过程中的内部协变量偏移现象，使训练过程更稳定，可以使用更高的学习率，加速训练； 增强泛化能力：批归一化作为一种正则化方法，可以减少模型的过拟合，增强泛化能力。 BN公式如下： $$ y=\\gamma\\cdot\\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}+\\beta $$ 有 $\\gamma,\\beta$ 两个可学习的平移参数和缩放参数，具有以下作用：\n保留网络各层在训练过程中的学习成果。若没有这两个参数，BN 退化为普通的标准化，不能有效学习。添加参数后可以保留每个神经元学习的成果； 保证激活单元的非线性表达能力。添加参数后，BN 的数据可以进入到激活函数的非线性区域； 使 BN 模块具有自我关闭功能。若 $\\gamma,\\beta$ 分别取数据的均值和标准差，则可以回复初始的输入值，即关闭 BN 模块。 BN 模块通常放在激活层前，可以避免 BN 破坏非线性特征的分布；此外，BN 可以使数据不落入激活函数的饱和区域，缓解梯度消失问题；\n由于现在常用激活函数为 ReLU，可以将 BN 放在激活层后，避免数据在激活层之前被转化成相似模式而使非线性特征趋于同化。\n2. 用于分类任务的卷积神经网络的最后几层一般是什么层？在最近几年有什么变化？ 网络末端一般是几层全连接层。\n近几年，分类网络在卷积层之后、最后一层之前通常采用全局平均池化，它具备与全连接层相似的效果（提取全局信息），并有如下优点：\n参数量和计算量大大降低。GAP 的参数量为0，计算量为 $cwh$；输出单元数为 k 的全连接层参数量和计算量都是 $cwhk$。 具有较好可解释性。可以知道特征图上那些点对最后的分类贡献大。 3. 卷积神经网络中的瓶颈结构和沙漏结构提出的初衷是什么？可以应用于哪些问题？ 瓶颈结构：\n瓶颈结构初衷是为了降低大卷积层的计算量。在大卷积层之前先用 1x1 卷积来缩减特征图通道数；在完成大卷积后，根据需要可以再使用 1x1 卷积来恢复特征图通道数。\n瓶颈结构能够用更小的计算代价达到与之前相似或更好的效果（增加了网络层数）。\n沙漏结构\n沙漏结构的设计初衷是解决图像分割、关键点检测等任务中对于全局和局部信息的兼顾问题。沙漏结构一般包括以下两个分支：\n自底向上（bottom-up）：利用卷积、池化等操作将特征图的尺寸逐层压缩(通道数可能增加)，类似于自编码器中的编码器(encoder) 自顶向下（top-down）分支：利用反卷积或插值等上采样操作将特征图的尺寸逐层扩大(通道数可能降低)，类似于自编码器中的解码器(decoder)。 五、其他常见问题 1. 为什么卷积核大小一般为奇数？ 方便 padding：padding 大小通常为 $\\frac{k-1}{2}$，当 k 为奇数时，可以整除，实现两侧对称的 padding； 中心点：奇数大小卷积核有中心点，对边沿、对线条更加敏感，可以更有效的提取边沿信息。 ","permalink":"https://Achilles-10.github.io/posts/tech/interview1/","summary":"一、卷积基础知识 关键词：卷积操作、卷积核、感受野（Receptive Field）、特征图（feature map）、卷积神经网络 1. 简述卷积的基本操作，并分析其与全连接层的区别 在卷积操作中，卷积核与输入图像上滑动，进行点乘操作，然后求和得到一个单个值，这个值作为输出特征图中的一个像素","title":"百面深度学习：卷积神经网络"},{"content":"引言 将图像拆分为块（patch），并将这些图像块的线性嵌入序列（patch token）作为 Transformer 输入。\n当在没有强正则化的中型数据集（如 ImageNet）上进行训练时，这些模型产生的准确率比同等大小的 ResNet 低几个百分点。 这种看似令人沮丧的结果可能是意料之中的：Transformers 缺乏 CNN 固有的一些归纳偏置 (inductive biases) ，如平移等效性和局部性 （translation equivariance and locality），因此在数据量不足时，训练不能很好地泛化。\n但若在更大的数据集（14M-300M图像）上训练，情况就会发生变化，我们发现大规模训练胜过归纳偏置。\n归纳偏置：学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合\n例如，深度神经网络偏好性地认为，层次化处理信息有更好效果；卷积神经网络认为信息具有空间局部性，可用滑动卷积共享权重的方式降低参数空间；循环神经网络则将时序信息纳入考虑，强调顺序重要性；图网络则认为中心节点与邻居节点的相似性会更好地引导信息流动。\nCNN 平移不变性：卷积+最大池化约等于平移不变性\n如下面两图，输入图像的左下角有一个人脸，经过卷积，人脸的特征（眼睛，鼻子）也位于特征图的左下角。假如人脸特征在图像的左上角，那么卷积后对应的特征也在特征图的左上角。\n最大池化，它返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。\nCNN 局部性：如果我们想要识别出与物体相对应的图案，如天空中的一架飞机，我们可能需要看看附近的像素是如何排列的，但我们对那些彼此相距很远的组合的像素是如何出现的并不那么感兴趣。\n方法 图像块嵌入（Patch Embeddings） 图像 $X\\in\\mathbb{R}^{H\\times W\\times C}$经过分块得到 2D patch 序列 $x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，其中 $N=\\frac{H\\cdot W}{P^2}$ 为 ViT 有效输入序列长度。\nViT 在所有层中使用相同维度的隐向量大小 D，将 $x_p$经过 FC 层将 $P^2\\cdot C$ 映射到 D，同时 N 保持不变。\n上述即为图像块嵌入，本质即为对每一个图像块向量做一个线性变换，将其映射到 D 维，得到 $x_pE\\in\\mathbb{R}^{N\\times D}$。\nnum_patches = (image_height // patch_height) * (image_width // patch_width) patch_dim = channels * patch_height * patch_width to_patch_embedding = nn.Sequential( Rearrange(\u0026#39;b c (h p1) (w p2) -\u0026gt; b (h w) (p1 p2 c)\u0026#39;, p1 = patch_height, p2 = patch_width), nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim), nn.LayerNorm(dim), ) x = to_patch_embedding(x) 可学习的嵌入（Learnable Embedding） 为图像块嵌入序列手动预设一个可学习的嵌入 cls token，最后取追加的 cls token 作为输出。最终输入 ViT 的嵌入向量总长度为 N+1，cls token 在训练时随机初始化。\ncls_token = nn.Parameter(torch.randn(1, 1, dim)) cls_tokens = repeat(cls_token, \u0026#39;1 1 d -\u0026gt; b 1 d\u0026#39;, b = b) x = torch.cat((cls_tokens, x), dim=1) 位置嵌入（Position Embeddings） 位置嵌入 $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$被加入图像块嵌入以保留输入图像块之间的空间位置关系，这是由于自注意力的扰动不变性（Permutation-invariant），即打乱 Sequence 中 tokens 的顺序并不会改变结果。若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。\n论文中比较了以下几种不同的位置编码方案：\n无位置嵌入 1-D 位置编码嵌入：将 2D 图像块视作 1D 序列 2-D 位置编码嵌入：考虑图像块 2-D 位置 (x,y) 相对位置编码嵌入：考虑图像块的相对位置 其中无位置嵌入效果很差，其他效果接近。Transformer 中采用固定位置编码，ViT 采用标准可学习的 1-D 位置编码嵌入。\npos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) dropout = nn.Dropout(emb_dropout) x += self.pos_embedding[:, :(n + 1)] x = self.dropout(x) 最终输入： $$ x_0=[x_{cls};x_p^1E;x_p^2E;\\dots;x_p^NE]+E_{pos},\\quad x_0\\in\\mathbb{R}^{(N+1)\\times D} $$\nTransformer 编码器 Transformer 编码器由交替的多头自注意力层和多层感知机块构成，每个块前应用 LayerNorm，每个块后残差连接。\n多头自注意力（MSA）机制中的 QKV 矩阵由嵌入向量 X 与三个不同权重的矩阵相乘得到。\nQ 与 K 转置相乘，然后使用 Softmax 计算每一个单词对于其他单词的注意力系数，公式中的 Softmax 是对每一行计算，即每一行的和为 1。\n将 Softmax 矩阵与 V 矩阵相乘得到输出 $Z_i$，最后将每一头的输出 Z 拼接在一起，经过一个线性层得到最终输出 Z。\n公式表达如下： $$ \\begin{align*} \u0026amp;Q,K,V=MatMul(X,(W^Q,W^K,W^V))\\\\ \u0026amp;\\text{head}_i=\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})\\cdot V\\\\ \u0026amp;\\text{MultiHead}(Q,K,V)=Cat(\\text{head}_1,\\text{head}_2,\\dots,\\text{head}_h)\\cdot W^o \\end{align*} $$\nMSA 后面跟一个多层感知机（MLP），将特征维度先增大再恢复，其中使用 GELU 激活函数。\nclass PreNorm(nn.Module): def __init__(self, dim, fn): super().__init__() self.norm = nn.LayerNorm(dim) self.fn = fn def forward(self, x, **kwargs): return self.fn(self.norm(x), **kwargs) class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout = 0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) class Attention(nn.Module): def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.): super().__init__() inner_dim = dim_head * heads project_out = not (heads == 1 and dim_head == dim) self.heads = heads self.scale = dim_head ** -0.5 self.attend = nn.Softmax(dim = -1) self.dropout = nn.Dropout(dropout) self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False) self.to_out = nn.Sequential( nn.Linear(inner_dim, dim), nn.Dropout(dropout) ) if project_out else nn.Identity() def forward(self, x): qkv = self.to_qkv(x).chunk(3, dim = -1) q, k, v = map(lambda t: rearrange(t, \u0026#39;b n (h d) -\u0026gt; b h n d\u0026#39;, h = self.heads), qkv) dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale attn = self.attend(dots) attn = self.dropout(attn) out = torch.matmul(attn, v) out = rearrange(out, \u0026#39;b h n d -\u0026gt; b n (h d)\u0026#39;) return self.to_out(out) class Transformer(nn.Module): def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x 最后提取可学习的嵌入 cls token 对应的特征用于模型输出。\n常见问题 ViT是为了解决什么问题？\n处理大规模图像数据。传统 CNN 在面对大规模数据集时面临着计算和内存方面的挑战。Transformer 高度并行化，适合大规模数据集训练。 跨越空间信息。传统 CNN 使用局部感受野的卷积操作，无法充分捕捉全局语义信息。ViT 能够解决这个问题。 Transformer 为什么用 LayerNorm 不用 BatchNorm？为什么 ViT 定长也用 LayerNorm 不用 BatchNorm？\nBN 是对一个 Batch 内的不同样本的同意特征去做归一化操作，LN 是对单个样本的不同特征做操作，不收样本数的限制。在 NLP 中，每个样本的长度可能不同，无法组合成一个批次进行处理。 LN 是在样本内计算均值方差，这与自注意力机制的计算一致。而 BN 是在一个批次内跨样本计算均值方差，可能会破坏数据原有的数据分布。 为什么 $QK^T$ 除以 $\\sqrt{n}$ 而不是 n?\n点积结果需要除以一个数防止 softmax 结果过大使得梯度趋于 0。假设 Q 和 K 每一维都满足 0 均值 1 方差的分布，那么 Q*K 的方差为 n，除以 $\\sqrt{n}$ 使结果满足方差为 1 的分布，达到归一化的效果。$aX\\sim(0,a^2),X\\sim(0,1)$\n自注意力计算复杂度为 $O(n^2)$，怎么优化？\n[Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection]\n一个元素可能只与部分元素有关，但标准自注意力会给每个元素都分配权重，可通过 top-k 显示稀疏化，只关注部分元素。\n如下图，最左边为标准自注意力计算流程，中间为稀疏的流程，最右边为执行示意图。简而言之，在 Softmax 之前，先通过 top-k 选择少数重要的元素，将其他元素设置为负无穷。通过这种操作可以使注意力更加集中。\n位置编码的作用是什么\n自注意力计算是无向的，若没有位置编码，模型则无法知道 token 各自的位置信息。\n位置编码可以使模型在注意力机制中考虑到输入的绝对和相对位置信息，从而能够更好地捕捉到序列数据中的顺序关系。\ncls token 为什么能用来分类？还有哪些方法来进行最后的分类？\ncls token 随机初始化，并随着网络的训练不断更新，它能够编码整个数据集的统计特性； cls token 可以对全局特征进行聚合，并且不基于图像内容，避免对某一 token 有倾向性； cls token 固定位置为 0，能够避免输出受到位置编码的干扰； 除了将 cls token 用于最后 MLP 分类的输入，还可以对输出特征进行池化或直接将所有特征输出 MLP 分类层。\n为何使用多头注意力机制而非一个头？\n多头注意力机制可以在多个子空间中独立地学习不同的注意力权重，进一步增强网络的容量和表达能力，从而更好地捕捉序列中的信息。\n计算自注意力为何不能使用同一个值进行自身的点乘？\n向量的点乘表示两个向量的相似度，用不同的 Q 和 K 进行点乘才能计算每个 token 对其他 token 的相似度分数。\n为什么 Q 和 K 使用不同的权重矩阵生成\n使用 Q/K/V 不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。\nPreNorm 和 PostNorm 区别\nPreNorm 的残差结构相当于增加了模型的宽度而降低了模型的深度，可以防止模型的梯度爆炸或者梯度消失，更好训练。\nPostNorm 在在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好，但在较深的网络中难以控制梯度。\n","permalink":"https://Achilles-10.github.io/posts/tech/vit/","summary":"引言 将图像拆分为块（patch），并将这些图像块的线性嵌入序列（patch token）作为 Transformer 输入。 当在没有强正则化的中型数据集（如 ImageNet）上进行训练时，这些模型产生的准确率比同等大小的 ResNet 低几个百分点。 这种看似令人沮丧的结果可能是意料之中的：Transformers 缺乏 CNN","title":"速通 Vision Transformer (ViT)"},{"content":"知识蒸馏（Knowledge Distillation） 深度学习的主要挑战在于，受限制于资源容量，深度神经模型很难部署在资源受限制的设备上，如嵌入式设备和移动设备。知识蒸馏作为模型压缩和加速技术的代表，可以有效的从大型的教师模型中学习到小型的学生模型。\n知识蒸馏主要思想是：学生模型模仿教师模型，二者相互竞争，是的学生模型可以与教师模型持平甚至卓越的表现。关键问题是如何将知识从大的教师模型转移到小的学生模型。知识蒸馏系统由知识、蒸馏算法和师生架构三个关键部分组成。如上图所示。\n知识 在知识蒸馏中，知识可以分为基于响应的知识（response-based knowledge），基于特征的知识（ feature-based knowledge）, 基于关系的知识（relation-based knowledge），下图为教师模型中不同知识类别的直观示例。\n基于响应的知识（Response-Based Knowledge） 基于响应的知识的主要思想是让学生网络直接模仿教师网络的最终预测。假设对数向量 $z$ 为全连接层的最后输出，基于响应的蒸馏形式可以描述为： $$ L_{ResD}(z_t,z_s)=L_R(z_t,z_s) $$ 其中 $L_R$ 表示散度损失或者交叉熵损失，典型的基于响应的知识蒸馏结构如下图所示。\n最流行的基于响应的图像分类知识被称为软目标（soft target），软目标是输入的类别的概率，可以通过 softmax 函数估计为： $$ p(z_i,T)=\\frac{\\exp(z_i/T)}{\\sum_j\\exp(z_j/T)} $$ $z_i$ 为第i个类别的 logit，T 是温度因子,因此 soft logits 的知识蒸馏损失函数可以重写为： $$ L_{ResD}(p(z_t,T),p(z_s,T))=L_R(p(z_t,T),p(z_s,T)) $$ 通常，$L_R$ 使用 KL 散度损失。基于响应的知识通常需要依赖最后一层的输出，无法解决来自教师模型的中间层面的监督，而这对于使用非常深的神经网络进行表征学习非常重要。由于 logits 实际上是类别概率分布，因此基于响应的知识蒸馏限制在监督学习。\n下图为基准的知识蒸馏的具体网络架构。可见，知识蒸馏的损失函数为教师与学生网络在温度T下的蒸馏损失和学生网络输出与GT在温度1下的学生损失。\n温度的设置 模型在训练收敛后，softmax的输出不会是完全符合one-hot向量那种极端分布的，而是在各个类别上均有概率，即教师模型中在这些负类别（非正确类别）上输出的概率分布包含了一定的隐藏信息。在使用softmax的时候往往会将一个差别不大的输出变成很极端的分布，用一个三分类模型的输出举例： $$ [10,11,12]\\rightarrow[0.0900,0.2447,0.6652] $$ 原本的分布很接近均匀分布，但经过softmax，不同类别的概率相差很大。这就导致类别间的隐藏的相关性信息不再那么明显。为了解决这个问题，引入温度系数。\n对于随机生成的分布：$z\\in\\mathbb{R}^{10}\\sim N(10,2)$，在不同温度下，数据分布的变化情况如下图所示：\n对于蒸馏温度T，如果T接近于0，则最大值接近1，其他值接近0，近似于 argmax；如果T越大，则输出分布越平缓，相当于平滑的作用，保留相似信息。\n在蒸馏时，令教师与学生模型的损失为$L_1$，学生模型与真实标签之间的损失为$L_2$。$L_1$可以看做是引入的正则项，能够使得学生模型学到教师模型中高度泛化的知识，从而需要更少的训练样本即可收敛。\n$L_1$项始终使用较大的温度系数，$L_2$项使用较小的温度系数。这是由于温度系数较大时，模型需要训练得到一个很陡峭的输出，经过softmax之后才能获得一个相对陡峭的结果；温度系数较小时，模型输出稍微有点起伏，softmax就很敏感地把分布变得尖锐，认为模型学到了知识。\n基于特征的知识（Feature-Based Knowledge） 深度神经网络善于学习到不同层级的表征，因此中间层和输出层的都可以被用作知识来监督训练学生模型，中间层的知识对于输出层的知识是一个很好的补充。其蒸馏损失可以表示为： $$ L_{FeaD}(f_t(x),f_s(x))=L_F(\\Phi_t(f_t(x)),\\Phi_s(f_s(x))) $$ 其中 $f_t(x),f_s(x)$ 分别表示教师和学生网络的中间层特征图。变换函数 $\\Phi(\\cdot)$ 当特征图大小不同时应用，$L_F$ 衡量两个特征图的相似性，常用的有 L1, L2, 交叉熵等。下图为基于特征的知识蒸馏模型的架构。\n基于关系的知识（Relation-Based Knowledge） 基于响应和基于特征的知识都使用了教师模型中特定层的输出，基于关系的知识进一步探索了不同层或数据样本的关系。一般，将基于特征图关系的关系知识蒸馏loss表述如下： $$ L_{RelD}(f_t,f_s)=L_{R}(\\Psi_t(\\hat{f_t},\\check{f_t}),\\Psi_s(\\hat{f_s},\\check{f_s})) $$ 其中，$f_t,f_s$ 表示教师和学生网络的特征图，$(\\hat{f_t},\\check{f_t})$ 和 $(\\hat{f_s},\\check{f_s})$ 表示教师和学生网络的特征图组（pair），$\\Psi(\\cdot)$ 函数表示特征图组的相似性。\n蒸馏策略（Distillation Schemes） 根据教师模型是否与学生模型同时更新，知识蒸馏的学习方案可分为离线蒸馏（offline distillation）、在线蒸馏（online distillation）和自蒸馏（self-distillation）。\n从人类师生学习的角度也可以直观地理解离线、在线和自蒸馏。离线蒸馏是指知识渊博的教师教授学生知识；在线蒸馏是指教师和学生一起学习；自我蒸馏是指学生自己学习知识。\n离线蒸馏（Offline Distillation） 离线蒸馏包括两个阶段：1）大型教师模型蒸馏前在训练集上训练；2）教师模型在蒸馏过程中指导学生模型训练。\n离线蒸馏方法有训练时间长、复杂等缺点，而在教师模型的指导下，离线蒸馏中的学生模型的训练通常是有效的。此外，教师与学生之间的能力差距始终存在，而且学生往往对教师有极大依赖。\n在线蒸馏（Online Distillation） 为了克服离线蒸馏的局限性，提出了在线蒸馏来进一步提高学生模型的性能。在线蒸馏时，教师模型和学生模型同步更新，而整个知识蒸馏框架都是端到端可训练的。\n自蒸馏（Self-Distillation） 在自蒸馏中，教师和学生模型使用相同的网络，这可以看作是在线蒸馏的一个特例，例如将网络深层的知识蒸馏到浅层部分。\n教师学生结构（Teacher-Student Architecture） 学生网络的结构通常有以下选择：\n教师网络的简化版本，层数更少，每一层的通道数更少； 保留教师网络的结构，学生网络为其量化版本； 具有高效基本运算的小型网络； 具有优化全局网络结构的小网络； 与教师网络的结构相同. 教师和学生网络的关系如下图。\n蒸馏算法（Distillation Algorithms） 对抗性蒸馏（Adversarial Distillation）\n多教师蒸馏（Multi-teacher Distillation）\n跨模态蒸馏（Cross-Modal Distillation）\n基于图的蒸馏（Graph-Based Distillation）\n基于注意力的蒸馏（Attention-Based Distillation）\n无数据的蒸馏（Data-Free Distillation）\n量化蒸馏（Quantized Distillation）\n终身蒸馏（Lifelong Distillation）\n基于神经架构搜索的蒸馏（NAS-Based Distillation）\n","permalink":"https://Achilles-10.github.io/posts/tech/kd/","summary":"知识蒸馏（Knowledge Distillation） 深度学习的主要挑战在于，受限制于资源容量，深度神经模型很难部署在资源受限制的设备上，如嵌入式设备和移动设备。知识蒸馏作为模型压缩和加速技术的代表，可以有效的从大型的教师模型中学习到小型的学生模型。 知识蒸馏主要思想是：学生模型模","title":"知识蒸馏速览"},{"content":"哈希 1. 两数之和（简单） 哈希\ndef twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: tab = {} for i in range(len(nums)): if target-nums[i] in tab: return [i,tab[target-nums[i]]] tab[nums[i]]=i 2. 字母异位词分组（中等） 排序+哈希\ndef groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: tab = defaultdict(list) for s in strs: key = \u0026#39;\u0026#39;.join(sorted(s)) # list需要转换为字符串才能进行哈希 tab[key].append(s) return list(tab.values()) 计数+哈希\ndef groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: mp = defaultdict(list) for st in strs: counts = [0] * 26 for ch in st: counts[ord(ch) - ord(\u0026#34;a\u0026#34;)] += 1 # 需要将 list 转换成 tuple 才能进行哈希 mp[tuple(counts)].append(st) return list(mp.values()) 3. 最长连续序列（中等） 哈希\n当 num-1 在集合中时，跳过。\n当 num-1 不在集合中，说明 num 为一个序列的起点，依次判断 cur+1 是否在集合中。\ndef longestConsecutive(self, nums: List[int]) -\u0026gt; int: ans=0 nums_set = set(nums) for num in nums: if num-1 not in nums_set: cur,long = num,1 while cur+1 in nums_set: cur = cur+1 long += 1 ans = max(ans,long) return ans 哈希+动规\n记录序列端点的最大长度，每次遍历到 num，得到 num-1 和 num+1 序列的长度 l 和 r，则连续长度为 l+r+1，更新哈希表。\ndef longestConsecutive(self, nums: List[int]) -\u0026gt; int: tab = dict() ans = 0 for num in nums: if num not in tab: l = tab.get(num-1,0) r = tab.get(num+1,0) cur = l+r+1 ans = max(ans,cur) tab[num]=cur tab[num-l]=cur tab[num+r]=cur return ans 双指针 4. 移动零（简单） 双指针\n将把 0 移动到末尾转换为把非零数字移动到数组开头，这样可以保证非零数字的相对顺序。\n设置左右指针 l 和 r，l 指向非零数字需要放置的地方，遍历 r，每当遇到 0 就和 l 处交换。\ndef moveZeroes(self, nums: List[int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; l,r,n=0,0,len(nums) while r\u0026lt;n: if nums[r]!=0: nums[l],nums[r]=nums[r],nums[l] l+=1 r+=1 5. 盛最多水的容器（中等） 双指针\n每次移动较小的指针，才有可能使存储的水量变多。\ndef maxArea(self, height: List[int]) -\u0026gt; int: n=len(height) ans,i,j=0,0,n-1 while i\u0026lt;j: water = min(height[i],height[j])*(j-i) ans = max(water,ans) if height[i]\u0026lt;=height[j]: i+=1 else: j-=1 return ans 6. 三数之和（中等） 双指针+排序\n排序，遍历到相同的数时跳过以达到去重的目的。\ndef threeSum(self, nums: List[int]) -\u0026gt; List[List[int]]: n=len(nums) ans=[] nums.sort() for i in range(n-2): if i!=0 and nums[i]==nums[i-1]: continue if nums[i]\u0026gt;0: break l,r=i+1,n-1 while l\u0026lt;r: if nums[l]+nums[r]==-nums[i]: ans.append([nums[i],nums[l],nums[r]]) while l\u0026lt;r and nums[l+1]==nums[l]: l+=1 while l\u0026lt;r and nums[r-1]==nums[r]: r-=1 l+=1 r-=1 elif nums[l]+nums[r]\u0026gt;-nums[i]: r-=1 else: l+=1 return ans extra. 合并两个有序数组（简单） 逆向双指针\ndef merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -\u0026gt; None: p1,p2=m-1,n-1 idx = m+n-1 while p2\u0026gt;=0: if p1\u0026lt;0 or nums1[p1]\u0026lt;nums2[p2]: nums1[idx]=nums2[p2] p2-=1 else: nums1[idx]=nums1[p1] p1-=1 idx-=1 7. ⭐️ 接雨水（困难） 动态规划\n维护两个数组 left 和 right，分别表示 i 处左右最高柱子的高度。i 处接的雨水为 min(left[i],right[i])-height[i]。\ndef trap(self, height: List[int]) -\u0026gt; int: n=len(height) left,right=[height[0]]+[0]*(n-1),[0]*(n-1)+[height[-1]] for i in range(1,n): left[i]=max(left[i-1],height[i]) right[n-i-1]=max(right[n-i],height[n-i-1]) return sum(min(left[i],right[i])-height[i] for i in range(n)) 双指针\n使用双指针优化动态规划方法的空间复杂度。维护双指针 left 和 right 以及 两个变量 leftmax 和 rightmax 标志左右两侧柱子最高的高度。\ndef trap(self, height: List[int]) -\u0026gt; int: n,ans=len(height),0 left,right=0,n-1 leftmax,rightmax=0,0 while left\u0026lt;right: leftmax=max(leftmax,height[left]) rightmax=max(rightmax,height[right]) if height[left]\u0026lt;height[right]: ans+=leftmax-height[left] left+=1 else: ans+=rightmax-height[right] right-=1 return ans 单调栈\n维护一个单调栈，存储下标，保证下标对应的柱子高度递减。\n当遍历到下标 i 时，若下标 i 的柱子的高度大于栈顶元素 top 的高度，由于 height[top-1]\u0026gt;height[top]，此时得到一个可以接雨水的区域。重复这个操作直至栈空或 height[top]\u0026gt;=height[i]。将 i 入栈。\ndef trap(self, height: List[int]) -\u0026gt; int: n,ans,stack=len(height),0,[] for i, hi in enumerate(height): while stack and hi\u0026gt;height[stack[-1]]: top = stack.pop() if not stack: break w = i-stack[-1]-1 h = min(hi,height[stack[-1]])-height[top] ans += w*h stack.append(i) return ans 滑动窗口 8. 无重复字符的最长子串（中等） 滑动窗口+哈希\n哈希表存储窗口内的子串，遇到重复则更新左指针。\ndef lengthOfLongestSubstring(self, s: str) -\u0026gt; int: seen=set() n=len(s) rp,ans=-1,0 for i in range(n): if i!=0: seen.remove(s[i-1]) while rp+1\u0026lt;n and s[rp+1] not in seen: seen.add(s[rp+1]) rp+=1 ans = max(ans,rp-i+1) return ans #################################################### def lengthOfLongestSubstring(self, s: str) -\u0026gt; int: n,ans=len(s),0 i,j=0,0 tab = dict() while j\u0026lt;n: if s[j] in tab: i = max(i,tab[s[j]]+1) # 更新左指针 ans = max(j-i+1,ans) tab[s[j]]=j j+=1 return ans 9. 找到字符串中所有字母异位词（中等） 滑动窗口+哈希表\ndef findAnagrams(self, s: str, p: str) -\u0026gt; List[int]: m,n,ans=len(s),len(p),[] if m\u0026lt;n: return [] cntp = Counter(p) cnts = Counter(s[:n]) if cntp==cnts: ans.append(0) for i in range(1,m-n+1): cnts[s[i-1]]-=1 cnts[s[i+n-1]]+=1 if cntp==cnts: ans.append(i) return ans 滑动窗口+数组\n使用长度为 26 的数组代替哈希表\ndef findAnagrams(self, s: str, p: str) -\u0026gt; List[int]: m,n,ans=len(s),len(p),[] if m\u0026lt;n: return [] cntp,cnts = [0]*26,[0]*26 for i in range(n): cnts[ord(s[i])-ord(\u0026#39;a\u0026#39;)]+=1 cntp[ord(p[i])-ord(\u0026#39;a\u0026#39;)]+=1 if cnts==cntp: ans.append(0) for i in range(1,m-n+1): cnts[ord(s[i-1])-ord(\u0026#39;a\u0026#39;)]-=1 cnts[ord(s[i+n-1])-ord(\u0026#39;a\u0026#39;)]+=1 if cnts==cntp: ans.append(i) return ans 子串 10. ⭐️ 和为 K 的子数组（中等） ⭐️ 哈希表+前缀和\n用哈希表记录前缀和出现的次数，若当前 s-k 在哈表中出现过，则说明可以构成 dict[s-k] 个和为 k 的子数组。\ndef subarraySum(self, nums: List[int], k: int) -\u0026gt; int: s=ans=0 my_dict = defaultdict(int) my_dict[0]=1 for num in nums: s += num ans += my_dict[s-k] my_dict[s]+=1 return ans 11. ⭐️ 滑动窗口最大值（困难） ⭐️ 堆\n维护一个大根堆，存储元素为 (-nums[i], i)。当窗口右端点遍历到 j 处时，将窗口右边的数加入到堆中，若堆顶元素 i\u0026lt;j-k+1，说明堆顶元素已不在窗口内，将其从堆中删除。\n存储下标很重要，用于判断堆顶元素是否在窗口内。\nimport heapq class Solution: def maxSlidingWindow(self, nums: List[int], k: int) -\u0026gt; List[int]: heap,j=[],0 ans,n=[],len(nums) for i in range(k): heapq.heappush(heap,(-nums[i],i)) ans.append(-heap[0][0]) for j in range(k,n): heapq.heappush(heap,(-nums[j],j)) while heap[0][1]\u0026lt;=j-k: heapq.heappop(heap) ans.append(-heap[0][0]) return ans 单调双端队列\n维护一个单调双端队列，队列里的元素单调递减。队列里存储下标，每次移动窗口时，先判断队尾元素与新元素的大小关系，保持队列单调递减，将新元素下标加入到队列；判断队首元素下标是否在窗口外，如果在则出队。\n存储下标很重要，用于判断队首元素是否在窗口内。\n用 list 模拟双端队列效率较低，尽量用 collections.deque\nclass Solution: def maxSlidingWindow(self, nums: List[int], k: int) -\u0026gt; List[int]: n = len(nums) q = collections.deque() for i in range(k): while q and nums[i] \u0026gt;= nums[q[-1]]: q.pop() q.append(i) ans = [nums[q[0]]] for i in range(k, n): while q and nums[i] \u0026gt;= nums[q[-1]]: q.pop() q.append(i) while q[0] \u0026lt;= i - k: q.popleft() ans.append(nums[q[0]]) return ans 12. 最小覆盖子串（困难） 滑动窗口+哈希表\n维护一个哈希表，存储所需字符的数量。\n不断右移窗口右端点，直至窗口内包含了 t 的所有元素；不断右移窗口左端点，将不必要的元素排除，记录最佳答案；不断重复，直至右端点超出了 s 的范围。\n额外使用一个 cnt 变量来记录所需元素的总数量，当 cnt=0 时即可开始收缩窗口。\ndef minWindow(self, s: str, t: str) -\u0026gt; str: m,n=len(s),len(t) if n\u0026gt;m: return \u0026#34;\u0026#34; tab = Counter(t) cnt,i = n,0 ans,start=float(\u0026#39;inf\u0026#39;),-1 for j,c in enumerate(s): if tab[c]\u0026gt;0: cnt-=1 tab[c]-=1 if cnt==0: # 满足所需字符 while s[i] not in tab or tab[s[i]]\u0026lt;0: # 排除不必要元素 if tab[s[i]]\u0026lt;0: tab[s[i]]+=1 i+=1 if j-i+1\u0026lt;ans: ans,start=j-i+1,start # 最佳答案 tab[s[i]]+=1 cnt+=1 i+=1 return s[start:start+ans] if start!=-1 else \u0026#34;\u0026#34; 普通数组 13. 最大子数组和（中等） 动态规划\n定义 dp[i] 表示取 nums[i] 时的最大子数组和，则当 dp[i-1] 大于零时，加上 nums[i]；否则，取 nums[i]。\n注意初始值不能为零，考虑负值。\ndef maxSubArray(self, nums: List[int]) -\u0026gt; int: n=len(nums) s,ans=-inf,-inf for i in range(n): if s\u0026gt;=0: s+=nums[i] else: s=nums[i] ans=max(ans,s) return ans 14. 合并区间（中等） 排序\ndef merge(self, intervals: List[List[int]]) -\u0026gt; List[List[int]]: intervals.sort() ans=[] for interval in intervals: if not ans or interval[0]\u0026gt;ans[-1][1]: # 不重叠区间 ans.append(interval) else: # 重叠区间，更新右端点 ans[-1][1]=max(ans[-1][1],interval[1]) return ans 15. ⭐️ 轮转数组（中等） 使用额外数组\ndef rotate(self, nums: List[int], k: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; n,k=len(nums),k%n if k==0: return nums[:]=nums[n-k:]+nums[:n-k] ⭐️ 数组翻转\n当我们将数组的元素向右移动 $k$ 次后，尾部 $k\\bmod n$ 个元素会移动至数组头部，其余元素向后移动 $k\\bmod n$ 个位置。因此可依次翻转全部数组、翻转 $[0,k\\bmod n-1]$ 区间，翻转 $[k\\bmod n,n-1]$ 区间。\ndef rotate(self, nums: List[int], k: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; def reverse(i,j): while i\u0026lt;j: nums[i],nums[j]=nums[j],nums[i] i,j=i+1,j-1 n=len(nums) reverse(0,n-1) reverse(0,k%n-1) reverse(k%n,n-1) ⭐️ 循环交换\n位置为 i 的元素将会出现在位置 (i+k)%n，每次处理时保存新位置的元素并将其交换至下一个位置，直到遍历完全部元素，遍历循环次数为 gcd(n,k)。\ndef rotate(self, nums: List[int], k: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; def gcd(a,b): return gcd(b%a,a) if a else b n=len(nums) k=k%n cnt = gcd(k,n) for start in range(cnt): cur = (start + k) % n prev = nums[start] while start!=cur: nums[cur],prev=prev,nums[cur] cur = (cur + k)%n nums[cur],prev=prev,nums[cur] 16. 除自身以外数组的乘积（中等） 额外数组\n使用两个额外的数组 left 和 right 分别存储当前元素左侧和右侧元素的乘积。\ndef productExceptSelf(self, nums: List[int]) -\u0026gt; List[int]: n,ans=len(nums),[] left,right=[1]*n,[1]*n for i in range(n-1): left[i+1]=left[i]*nums[i] right[n-2-i]=right[n-1-i]*nums[n-1-i] for i in range(n): ans.append(left[i]*right[i]) return ans 常数空间\n使用除法操作；同时记录下零的数量和索引。\ndef productExceptSelf(self, nums: List[int]) -\u0026gt; List[int]: n=len(nums) ans,mul,zeros,zero_idx=[0]*n,1,0,0 for i in range(n): if nums[i]==0: zeros,zero_idx=zeros+1,i if zeros==2: return [0]*n else: mul*=nums[i] if zeros==1: ans[zero_idx]=mul else: for i in range(n): ans[i]=mul//nums[i] return ans 17. ⭐️ 缺失的第一个正数（困难） 标记数组\n首先将所有负数转换为 n+1；遍历数组，将属于 $[1,N]$ 的元素对应位置标记为负数；寻找第一个不为负数的位置，即为缺失的第一个正数。\ndef firstMissingPositive(self, nums: List[int]) -\u0026gt; int: n = len(nums) for i in range(n): if nums[i]\u0026lt;=0: nums[i]=n+1 for i in range(n): num=abs(nums[i]) # 若已被标记为负数，取其绝对值进行判断 if num\u0026lt;n+1: nums[num-1]=-abs(nums[num-1]) for i in range(n): if nums[i]\u0026gt;0: return i+1 return n+1 置换\n通过置换使 $nums[i]=i+1$。当 $1\\leq nums[i]\\leq n\\ 且\\ nums[i]-1\\neq i$ 时，进行交换。为防止死循环，还需要保证进行置换的两个值不相等，即 $nums[i]\\neq nums[nums[i]-1]$。\ndef firstMissingPositive(self, nums: List[int]) -\u0026gt; int: n = len(nums) for i in range(n): while 1\u0026lt;=nums[i]\u0026lt;=n and nums[i]!=nums[nums[i]-1]: # 为防止死循环，判断条件不能为 i!=nums[i]-1 nums[nums[i]-1],nums[i]=nums[i],nums[nums[i]-1] for i in range(n): if nums[i]-1!=i: return i+1 return n+1 矩阵 18. ⭐️ 矩阵置零（中等） 使用标记数组\n记录下置零的行和列，O(m+n) 空间复杂度。\ndef setZeroes(self, matrix: List[List[int]]) -\u0026gt; None: m,n=len(matrix),len(matrix[0]) rows,cols=[0]*m,[0]*n for i in range(m): for j in range(n): if matrix[i][j]==0: rows[i]=cols[j]=1 for i in range(m): for j in range(n): if rows[i] or cols[j]: matrix[i][j]=0 ⭐️ 标记常量\n使用矩阵第一行和第一列代替标记数组，但这会导致第一行和第一列被修改。使用两个标记变量记录第一行和第一列是否包含零。常数空间复杂度。\ndef setZeroes(self, matrix: List[List[int]]) -\u0026gt; None: m,n=len(matrix),len(matrix[0]) flag_r0 = any(matrix[0][j]==0 for j in range(n)) flag_c0 = any(matrix[i][0]==0 for i in range(m)) for i in range(1,m): for j in range(1,n): if matrix[i][j]==0: matrix[i][0]=matrix[0][j]=0 for i in range(1,m): for j in range(1,n): if matrix[i][0]==0 or matrix[0][j]==0: matrix[i][j]=0 if flag_r0: for j in range(n): matrix[0][j]=0 if flag_c0: for i in range(m): matrix[i][0]=0 19. 螺旋矩阵（中等） 模拟\n设置上下左右四个边界，模拟螺旋过程依次输出，遇到越界则停止。\ndef spiralOrder(self, matrix: List[List[int]]) -\u0026gt; List[int]: m,n=len(matrix),len(matrix[0]) top,bottom,left,right=0,m-1,0,n-1 ans=[] while True: for j in range(left,right+1): ans.append(matrix[top][j]) # 从左到右 top+=1 if top\u0026gt;bottom: break for i in range(top,bottom+1): ans.append(matrix[i][right]) # 从上到下 right-=1 if left\u0026gt;right: break for j in range(right,left-1,-1): ans.append(matrix[bottom][j]) # 从右到左 bottom-=1 if top\u0026gt;bottom: break for i in range(bottom, top-1,-1): ans.append(matrix[i][left]) # 从下到上 left+=1 if left\u0026gt;right: break return ans 20. 旋转图像（中等） 原地交换\n找到对应四个点的坐标：$(i,j)\\rightarrow(j,n-1-i)\\rightarrow(n-1-i,n-1-j)\\rightarrow(n-1-j,i)\\rightarrow(i,j)$\ndef rotate(self, matrix: List[List[int]]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify matrix in-place instead. \u0026#34;\u0026#34;\u0026#34; n=len(matrix) cnt = (n+1)//2 for i in range(cnt): for j in range(i,n-1-i): matrix[i][j],matrix[j][n-1-i],matrix[n-1-i][n-1-j],matrix[n-1-j][i]=matrix[n-1-j][i],matrix[i][j],matrix[j][n-1-i],matrix[n-1-i][n-1-j] 21. 搜索二维矩阵 II（中等） 二分查找\ndef searchMatrix(self, matrix: List[List[int]], target: int) -\u0026gt; bool: m,n=len(matrix),len(matrix[0]) i,j=0,n-1 while i\u0026lt;m and j\u0026gt;=0: if target==matrix[i][j]: return True elif target\u0026gt;matrix[i][j]: i+=1 else: j-=1 return False 链表 22. 相交链表（简单） 双指针\n双指针遍历，当遇到表尾时，跳转至另一表头，直至双指针相遇。\ndef getIntersectionNode(self, headA: ListNode, headB: ListNode) -\u0026gt; Optional[ListNode]: pa,pb=headA,headB while pa!=pb: pa = pa.next if pa else headB pb = pb.next if pb else headA return pa 哈希表\n先遍历 headA，将每一个节点加入到哈希表中；遍历 headB，判断节点是否在哈希表中。\n23. 反转链表（简单） 指针\n一行赋值时注意先更新 p.next，若先更新 p，则 p.next 会改变。\ndef reverseList(self, head: Optional[ListNode]) -\u0026gt; Optional[ListNode]: if not head or not head.next: return head pre,p = None,head while p: pre,p.next,p = p,pre,p.next # 注意先更新 p.next return pre extra. ⭐️ 反转链表 II（中等） 穿针引线\n先定位 left 和 right，然后设置前驱和后继指针 pre 和 tail，对 left 和 right 区间反转链表。这个方法的弊端是当 left 和 right 的跨度特别大时，需要遍历链表两次。\ndef reverseBetween(self, head: Optional[ListNode], left: int, right: int) -\u0026gt; Optional[ListNode]: def reverse(head,tail): pre,p=None,head while pre!=tail: pre,p.next,p=p,pre,p.next return tail,head pre=dummy=ListNode(next=head) tail=head for i in range(left-1): head=head.next pre=pre.next for j in range(right-1): tail=tail.next nxt=tail.next head,tail=reverse(head,tail) pre.next,tail.next=head,nxt return dummy.next ⭐️ 头插法（一次穿针引线）\n在需要反转的区间里，每遍历到一个节点，让这个新节点来到反转部分的起始位置。下面的图展示了整个流程。具体来说，需要三个指针：pre，curr 和 next。\npre：指向 left 的前一个节点 curr：指向反转区域的第一个节点 left next：指向curr的下一个节点 具体步骤如下：\n设置 curr 的下一个节点为 next curr.next 指向 next.next next.next 指向 pre.next pre.next 指向 next def reverseBetween(self, head: Optional[ListNode], left: int, right: int) -\u0026gt; Optional[ListNode]: pre=dummy=ListNode(next=head) for _ in range(left-1): pre=pre.next curr = pre.next for _ in range(right-left): nxt = curr.next curr.next = nxt.next nxt.next = pre.next pre.next = nxt return dummy.next 24. 回文链表（简单） 反转链表\n找中点+反转链表+遍历比较\ndef isPalindrome(self, head: Optional[ListNode]) -\u0026gt; bool: slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next pre,p = None,slow while p: pre,p.next,p = p,pre,p.next p = head while p and pre: if p.val != pre.val: return False p,pre=p.next,pre.next return True 25. 环形链表（简单） 快慢指针\ndef hasCycle(self, head: Optional[ListNode]) -\u0026gt; bool: if not head or not head.next: return False dummy = ListNode() dummy.next=head slow=fast=dummy while fast and fast.next: fast=fast.next.next slow=slow.next if slow==fast: return True return False 26. 环形链表 II（中等） 快慢指针\n设置快慢指针，假设链表存在环，则快慢指针会在环内相遇，如下图：\n此时有 $2(a+b)=a+(n+1)(b+c)+b\\Rightarrow a=n(b+c)+c$，即头结点到入环点的距离为相遇点到入环点的距离加上 n 圈的环长。因此，再设置一个指针从头结点出发，则一定能和慢指针在入环点相遇。\ndef detectCycle(self, head: ListNode) -\u0026gt; ListNode: slow,fast=head,head while fast and fast.next: slow,fast=slow.next,fast.next.next if slow==fast: break # 这个判断条件不能为 if slow!=fast，需考虑只有一个节点且没有环的情况 if not fast or not fast.next: return None p=head while p!=slow: p,slow=p.next,slow.next return slow 27. 合并两个有序链表（简单） 递归\ndef mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: if not list1: return list2 if not list2: return list1 if list1.val \u0026lt; list2.val: list1.next = self.mergeTwoLists(list1.next,list2) return list1 else: list2.next = self.mergeTwoLists(list1,list2.next) return list2 28. 两数相加（中等） 模拟\ndef addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: dummy = ListNode() p1,p2,p=l1,l2,dummy carry = 0 while p1 or p2 or carry: int1 = p1.val if p1 else 0 int2 = p2.val if p2 else 0 s = int1+int2+carry val,carry = s%10,s//10 p.next = ListNode(val=val) p=p.next p1=p1.next if p1 else p1 p2=p2.next if p2 else p2 return dummy.next extra. ⭐️ 两数相加 II（中等） 栈\n若不反转链表，则用栈存储链表的值。返回时注意链表头。\ndef addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: s1,s2=[],[] while l1: s1.append(l1.val) l1=l1.next while l2: s2.append(l2.val) l2=l2.next dummy=ListNode() carry = 0 while s1 or s2 or carry: int1 = s1.pop() if s1 else 0 int2 = s2.pop() if s2 else 0 s = int1+int2+carry carry,val = s//10,s%10 # divmod(s,10) dummy.next = ListNode(val,dummy.next) return dummy.next 29. 删除链表的倒数第 N 个结点（中等） 快慢指针\n设置快指针先遍历 n 次，然后用慢指针定位待删除节点的前节点。\ndef removeNthFromEnd(self, head: Optional[ListNode], n: int) -\u0026gt; Optional[ListNode]: slow = dummy = ListNode(next=head) # 使用 dummy 定位待删除节点的前节点 fast=head for _ in range(n): fast=fast.next while fast: slow,fast=slow.next,fast.next slow.next=slow.next.next return dummy.next 30. 两两交换链表中的节点（中等） 迭代\ndef swapPairs(self, head: Optional[ListNode]) -\u0026gt; Optional[ListNode]: pre = dummy = ListNode(next=head) while pre.next and pre.next.next: s,f = pre.next,pre.next.next pre.next,s.next,f.next=f,f.next,s pre = s return dummy.next 递归\ndef swapPairs(self, head: Optional[ListNode]) -\u0026gt; Optional[ListNode]: if not head or not head.next: return head pre,cur = head,head.next pre.next = self.swapPairs(cur.next) cur.next = pre return cur 31. K 个一组翻转链表（困难） 模拟\n循环，每次设置四个指针 pre, head, tail, nxt，分别指向待翻转区间前的节点，待翻转区间头结点，待翻转区间尾节点，带翻转区间后的节点。\n翻转待翻转区间，设置 pre.next=head,tail.next=nxt。\ndef reverseKGroup(self, head: Optional[ListNode], k: int) -\u0026gt; Optional[ListNode]: def reverse(head,tail): pre, p, tmp = None, head, head while pre != tail: pre,p.next,p = p,pre,p.next return pre,tmp pre = tail = dummy = ListNode(next=head) while True: for i in range(k): if not tail.next: return dummy.next tail = tail.next nxt = tail.next head,tail = reverse(head,tail) pre.next = head tail.next = nxt pre,head = tail,tmp 32. 复制带随机指针的链表（中等） 节点拆分\n三次遍历。第一次遍历，在每个节点的后面复制一个新节点；第二次遍历设置新节点的随机指针为前指针的随机指针的新节点；第三次遍历连接复制的新节点。\ndef copyRandomList(self, head: \u0026#39;Optional[Node]\u0026#39;) -\u0026gt; \u0026#39;Optional[Node]\u0026#39;: dummy = Node(x=-1,next=head) p=head while p: new_node = Node(x=p.val,next=p.next) # 新建节点 p.next,p=new_node,p.next p=head while p: p.next.random = p.random.next if p.random else None # 设置新节点的随机指针 p=p.next.next p=dummy while p and p.next: p.next,p=p.next.next,p.next.next # 连接新节点 return dummy.next 哈希表\n使用哈希表来存储节点对应的复制节点\ndef copyRandomList(self, head: \u0026#39;Optional[Node]\u0026#39;) -\u0026gt; \u0026#39;Optional[Node]\u0026#39;: if not head: return None p,tab = head,{} while p: tab[p]=Node(x=p.val) p=p.next p=head while p: tab[p].next = tab.get(p.next,None) tab[p].random = tab.get(p.random,None) p=p.next return tab[head] 33. ⭐️ 排序链表（中等） 归并排序（递归）\n通过快慢指针将链表一分为二，进行归并排序\nclass Solution: def merge(self,list1,list2): if not list1 or not list2: return list1 or list2 if list1.val\u0026lt;=list2.val: list1.next = self.merge(list1.next,list2) return list1 else: list2.next = self.merge(list1,list2.next) return list2 def sortList(self, head: Optional[ListNode]) -\u0026gt; Optional[ListNode]: if not head or not head.next: return head slow=fast=head while fast.next and fast.next.next: fast=fast.next.next slow=slow.next mid,slow.next = slow.next,None # 划分链表 return self.merge(self.sortList(head),self.sortList(p)) ⭐️ 迭代\n若面试中要求常数级空间复杂度，需要迭代。\n令子链表长度依次为 1, 2, 4, \u0026hellip; ，循环归并。\nclass Solution: def merge(self,list1,list2): p = dummy = ListNode(0) while list1 and list2: if list1.val\u0026lt;=list2.val: p.next = list1 list1=list1.next else: p.next = list2 list2=list2.next p=p.next if not list1: p.next = list2 if not list2: p.next = list1 return dummy.next def sortList(self, head: Optional[ListNode]) -\u0026gt; Optional[ListNode]: if not head or not head.next: return head n,tmp=0,head while tmp: tmp=tmp.next n+=1 dummy = ListNode(next=head) k = 1 # 当前合并子链表长度 while k\u0026lt;=n: pre,cur = dummy,dummy.next while cur: left = cur # 左子链表头 for i in range(1,k): if cur.next: cur = cur.next else: break if not cur.next: # 左子链表长度不足 k，不用再归并 pre.next = left break right = cur.next # 右子链表头 cur.next = None # 断开左子链表 cur = right for i in range(1,k): if cur.next: cur = cur.next else: break nxt = None if cur.next: nxt = cur.next # 记录后续待合并链表表头 cur.next = None # 断开右子链表 cur = nxt pre.next = self.merge(left,right) # 将合并好的链表接在已合并好链表的表尾 while pre.next: pre = pre.next k*=2 return dummy.next 34. ⭐️ 合并 K 个升序链表（困难） 分治合并\n两两合并链表。\ndef merge(self, list1, list2): # 迭代合并 p1,p2=list1,list2 tmp=dummy=ListNode() while p1 and p2: if p1.val\u0026lt;p2.val: tmp.next=p1 p1=p1.next else: tmp.next=p2 p2=p2.next tmp=tmp.next if not p1: tmp.next=p2 if not p2: tmp.next=p1 return dummy.next def merge_(self, list1, list2): # 递归合并 if not list1 or not list2: return list1 or list2 if list1.val\u0026lt;list2.val: list1.next = self.merge_(list1.next,list2) return list1 else: list2.next = self.merge_(list1,list2.next) return list2 def mergeKLists(self, lists: List[Optional[ListNode]]) -\u0026gt; Optional[ListNode]: if not lists: return None n=len(lists) if n==1: return lists[0] mid = n//2 return self.merge(self.mergeKLists(lists[:mid]),self.mergeKLists(lists[mid:])) ⭐️ 堆合并（K个指针）\n维护当前每个链表没有被合并的元素的最前面一个，每次在这些元素里面选取值最小的元素合并到答案中。使用小顶堆来维护这些元素。\ndef mergeKLists(self, lists: List[Optional[ListNode]]) -\u0026gt; Optional[ListNode]: if not lists: return None p=dummy=ListNode() heap=[] for i in range(len(lists)): if lists[i]: heapq.heappush(heap,(lists[i].val,i)) lists[i]=lists[i].next while heap: val,idx = heapq.heappop(heap) p.next = ListNode(val) if lists[idx]: heapq.heappush(heap,(lists[idx].val,idx)) lists[idx]=lists[idx].next p=p.next return dummy.next 35. ⭐️ LRU 缓存（中等） 双向链表+哈希\n双向链表，表头为最久未使用节点，表尾为最新节点。\n每次 get，将访问节点移至表尾；每次 put，若 key 存在，则取出对应值并将该节点移至表尾，否则，则新建一个节点添加至表尾，此时若链表长度超过容量，则删除表头节点，并在哈希表中删除对应元素。\n在表头表尾各用一个空节点表示 head，tail。\nclass ListNode: def __init__(self,key=0,val=0,pre=None,net=None): self.key=key self.val=val self.pre=pre self.next=net class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.num = 0 self.head = ListNode() self.tail = ListNode() self.head.next = self.tail self.tail.pre = self.head self.tab = dict() def delnode(self,node): node.pre.next = node.next node.next.pre = node.pre def delhead(self): node = self.head.next self.delnode(node) return node def addtotail(self,node): self.tail.pre.next = node node.pre = self.tail.pre node.next = self.tail self.tail.pre = node def movetotail(self,node): self.delnode(node) self.addtotail(node) def get(self, key: int) -\u0026gt; int: if key not in self.tab: return -1 node = self.tab[key] self.movetotail(node) return node.val def put(self, key: int, value: int) -\u0026gt; None: if key in self.tab: node = self.tab[key] node.val = value self.movetotail(node) else: node = ListNode(key,value) self.addtotail(node) self.tab[key]=node self.num+=1 if self.num\u0026gt;self.capacity: node = self.delhead() self.tab.pop(node.key) self.num-=1 OrderedDict\n直接使用封装好的 OrderedDict 定义 LRU。\nclass LRUCache(collections.OrderedDict): #继承 OrderedDict def __init__(self, capacity: int): super().__init__() self.capacity = capacity def get(self, key: int) -\u0026gt; int: if key not in self: return -1 self.move_to_end(key) return self[key] def put(self, key: int, value: int) -\u0026gt; None: if key in self: self.move_to_end(key) self[key] = value if len(self) \u0026gt; self.capacity: self.popitem(last=False) # last=False 删除第一个(最早添加)节点 二叉树 36. ⭐️ 二叉树的中序遍历（简单） 递归\ndef inorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: def dfs(node,ans): if not node: return dfs(node.left,ans) ans.append(node.val) dfs(node.right,ans) ans=[] dfs(root,ans) return ans ⭐️ 迭代\n结合指针和栈，每当节点不为空时，则将节点入栈，遍历左子树；当节点为空时，则说明左子树遍历完毕，此时栈顶为当前左子树的根节点，将根节点值加入到 ans 中，遍历右子树。\ndef inorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if not root: return [] ans,stack=[],[] cur=root while cur or stack: if cur: stack.append(cur) cur=cur.left else: cur=stack.pop() ans.append(cur.val) cur=cur.right return ans ⭐️ 前序遍历和后序遍历的迭代解法\n# 前序遍历 def preorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if not root: return [] stack,ans=[],[] cur=root while cur or stack: if cur: ans.append(cur.val) # 先处理根节点 stack.append(cur) cur = cur.left else: cur = stack.pop() cur = cur.right return ans 后序遍历有两种方法，一种是按照后序遍历的顺序访问节点，一种是将 左-右-中 的后序遍历转换为 中-右-左 的前序遍历，在输出时将答案翻转。\n# 后序遍历顺序访问节点 def postorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if not root: return [] stack, ans =[], [] cur,pre = root,None # 设置一个 pre 指针，指向上一个加入 ans 数组的节点 while cur or stack: while cur: stack.append(cur) cur = cur.left cur = stack.pop() if not cur.right or cur.right==pre: # 若右节点为 pre，说明右子树已经遍历完毕 ans.append(cur.val) pre = cur cur = None else: stack.append(cur) cur = cur.right return ans # 转换为 中-右-左 的前序遍历 def postorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if not root: return [] stack, ans =[], [] cur = root while cur or stack: if cur: ans.append(cur.val) stack.append(cur) cur = cur.right else: cur = stack.pop() cur = cur.left return ans[::-1] 37. 二叉树的最大深度（简单） 递归\ndef maxDepth(self, root: Optional[TreeNode]) -\u0026gt; int: if not root: return 0 l=self.maxDepth(root.left) r=self.maxDepth(root.right) return max(l,r)+1 38. 翻转二叉树（简单） 递归\ndef invertTree(self, root: Optional[TreeNode]) -\u0026gt; Optional[TreeNode]: if not root: return root root.left,root.right = self.invertTree(root.right),self.invertTree(root.left) # 同时赋值 return root 39. 对称二叉树（简单） 递归\ndef isSymmetric(self, root: Optional[TreeNode]) -\u0026gt; bool: def sym(a,b): if not a and not b: return True if not a or not b: return False return a.val == b.val and sym(a.left,b.right) and sym(a.right,b.left) return sym(root.left,root.right) 迭代\ndef isSymmetric(self, root: Optional[TreeNode]) -\u0026gt; bool: queue = [root.left,root.right] while queue: l=queue.pop(0) r=queue.pop(0) if not l and not r: continue if not l or not r: return False if l.val!=r.val: return False queue.append(l.left) queue.append(r.right) queue.append(l.right) queue.append(r.left) return True extra. 平衡二叉树（简单） 自底向上\ndef isBalanced(self, root: Optional[TreeNode]) -\u0026gt; bool: def helper(node): if not node: return 0 l = helper(node.left) r = helper(node.right) if l==-1 or r==-1 or abs(l-r)\u0026gt;1: # -1 表示不平衡，提前终止 return -1 return max(l,r)+1 return helper(root)\u0026gt;=0 自顶向下\ndef isBalanced(self, root: TreeNode) -\u0026gt; bool: def height(root: TreeNode) -\u0026gt; int: if not root: return 0 return max(height(root.left), height(root.right)) + 1 if not root: return True return abs(height(root.left) - height(root.right)) \u0026lt;= 1 and self.isBalanced(root.left) and self.isBalanced(root.right) 40. 二叉树的直径（简单） DFS\ndef diameterOfBinaryTree(self, root: Optional[TreeNode]) -\u0026gt; int: def dfs(node): if not node: return 0 nonlocal ans l=dfs(node.left) r=dfs(node.right) ans=max(ans,l+r) return max(l,r)+1 ans=0 dfs(root) return ans 41. 二叉树的层序遍历（中等） BFS\ndef levelOrder(self, root: Optional[TreeNode]) -\u0026gt; List[List[int]]: if not root: return [] queue, ans = [root], [] while queue: tmp, n = [], len(queue) for _ in range(n): node = queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) ans.append(tmp) return ans extra. 二叉树的锯齿形层序遍历（中等） BFS\n增加一个标志变量，判断正序还是倒序添加每一层的遍历结果\ndef zigzagLevelOrder(self, root: Optional[TreeNode]) -\u0026gt; List[List[int]]: if not root: return [] queue, ans = [root], [] flag=True while queue: tmp, n = [], len(queue) for _ in range(n): node = queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) if flag: ans.append(tmp) else: ans.append(tmp[::-1]) flag=not flag return ans 42. 将有序数组转换为二叉搜索树（简单） 递归\ndef sortedArrayToBST(self, nums: List[int]) -\u0026gt; Optional[TreeNode]: if not nums: return None mid=len(nums)//2 root = TreeNode(nums[mid]) root.left = self.sortedArrayToBST(nums[:mid]) root.right = self.sortedArrayToBST(nums[mid+1:]) return root 43. 验证二叉搜索树（中等） DFS\n设置一个上下界，每当节点元素越界时，则不是二叉搜索树，递归到下一层时更新上下界为当前节点元素值。\ndef isValidBST(self, root: Optional[TreeNode]) -\u0026gt; bool: def dfs(node,low,high): if not node: return True if node.val \u0026lt;=low or node.val \u0026gt;=high: return False return dfs(node.left,low,node.val) and dfs(node.right,node.val,high) return dfs(root,-inf,inf) 44. 二叉搜索树中第K小的元素（中等） 中序遍历（递归）\ndef kthSmallest(self, root: Optional[TreeNode], k: int) -\u0026gt; int: self.cnt,self.ans=0,-1 def dfs(node): if not node or self.ans!=-1: return dfs(node.left) self.cnt+=1 if self.cnt==k: self.ans=node.val dfs(node.right) dfs(root) return self.ans 中序遍历（遍历）\ndef kthSmallest(self, root: Optional[TreeNode], k: int) -\u0026gt; int: stack = [] while stack or root: while root: stack.append(root) root=root.left root=stack.pop() k-=1 if k==0: return root.val root=root.right 记录子树节点数\n如果你需要频繁地查找第 k 小的值，你将如何优化算法？\n记录下以每个结点为根结点的子树的结点数，使用类似二分查找的方法搜索\nclass Bst: def __init__(self,root): self.root=root self._node_num = {} self._count_node_num(root) def _count_node_num(self,node): if not node: return 0 self._node_num[node] = 1+self._count_node_num(node.left)+self._count_node_num(node.right) return self._node_num[node] def _get_node_num(self,node): return self._node_num[node] if node else 0 def kth_smallest(self,k): node = self.root while node: left = self._get_node_num(node.left) if left==k-1: return node.val elif left\u0026lt;k-1: node = node.right k-=left+1 else: node = node.left class Solution: def kthSmallest(self, root: Optional[TreeNode], k: int) -\u0026gt; int: bst = Bst(root) return bst.kth_smallest(k) 45. 二叉树的右视图（中等） 层序遍历\n将每层最后一个节点的值加入到 ans\ndef rightSideView(self, root: Optional[TreeNode]) -\u0026gt; List[int]: if not root: return [] queue,ans = [root],[] while queue: for _ in range(len(queue)): node = queue.pop(0) if node.left: queue.append(node.left) if node.right: queue.append(node.right) ans.append(node.val) return ans 46. ⭐️ 二叉树展开为链表（中等） 前序遍历+原地展开\n类似迭代前序遍历，先将右节点压栈，遍历左节点。\ndef flatten(self, root: Optional[TreeNode]) -\u0026gt; None: if not root: return stack = [] pre,cur=None,root while stack or cur: if cur: if cur.right: stack.append(cur.right) cur.right,cur.left = cur.left, None pre,cur = cur,cur.right else: cur = stack.pop() pre.right = cur ⭐️ 寻找前驱节点\n空间复杂度为 O(1)\n展开的关键操作是将左子树的最后一个访问节点的右节点设置为当前节点的右节点，于是寻找每个节点左子树的最后一个节点。\ndef flatten(self, root: Optional[TreeNode]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify root in-place instead. \u0026#34;\u0026#34;\u0026#34; if not root: return cur = root while cur: if cur.left: pre=cur.left while pre.right: pre = pre.right pre.right = cur.right cur.right,cur.left = cur.left,None cur = cur.right 47. 从前序与中序遍历序列构造二叉树（中等） 递归\npreorder[0] 是根节点的值，找到根节点在 inorder 的下标，划分左右子树递归处理。\ndef buildTree(self, preorder: List[int], inorder: List[int]) -\u0026gt; Optional[TreeNode]: if not preorder: return None root_val = preorder[0] root = TreeNode(root_val) idx = inorder.index(root_val) root.left = self.buildTree(preorder[1:idx+1],inorder[:idx]) root.right = self.buildTree(preorder[idx+1:],inorder[idx+1:]) return root 迭代\n48. ⭐️ 路径总和 III（中等） 前缀和+哈希\n使用一个字典存储前缀和，若当前 s-targetSum 在前缀和字典中存在，则说明到当前节点有路径存在。\ndef pathSum(self, root: Optional[TreeNode], targetSum: int) -\u0026gt; int: def dfs(node,s): if not node: return 0 res=0 s+=node.val res+=presum[s-targetSum] #到当前节点满足要求的路径数量 presum[s]+=1 # 更新前缀和 res+=dfs(node.left,s) res+=dfs(node.right,s) presum[s]-=1 return res presum = defaultdict(int) presum[0]=1 return dfs(root,0) extra. 路径总和（简单 ） 递归\ndef hasPathSum(self, root: Optional[TreeNode], targetSum: int) -\u0026gt; bool: def dfs(node,target): if not node: return False if not node.left and not node.right: return node.val==target l=dfs(node.left,target-node.val) r=dfs(node.right,target-node.val) return l or r return dfs(root,targetSum) extra. 路径总和 II（中等） 递归\ndef pathSum(self, root: TreeNode, target: int) -\u0026gt; List[List[int]]: ans,path=[],[] def dfs(node,s): if not node: return s+=node.val path.append(node.val) if s==target and not node.left and not node.right: ans.append(path[:]) dfs(node.left,s) dfs(node.right,s) path.pop() dfs(root,0) return ans 49. ⭐️ 二叉树的最近公共祖先（中等） DFS\n若 root 是 p,q 的最近公共祖先，则为以下几种情况：\np,q 在 root 的子树中且分列 root 的异侧； p=root 且 q 为 root 的子树； q=root 且 p 为 root 的子树 考虑通过递归对二叉树进行先序遍历，每当遇到 p 或 q 时返回。假设左右子树递归的返回值分别为 left 和 right，返回值有以下几种情况：\nleft 和 right 同时为空。表示 root 的左右子树都不包含 p 和 q，返回 None； left 和 right 均不为空。表示 p,q 在 root 的子树中且分列 root 的异侧，root 为最近公共祖先，返回 root； left 为空，right 不为空。表示 p,q 都不在 root 的左子树中，返回right。具体分为两种情况： p,q 都在 root 的右子树，此时 right 指向最近公共祖先； p,q 其一在 root 的右子树，此时 right 指向在右子树中的节点 同 3. def lowestCommonAncestor(self, root: \u0026#39;TreeNode\u0026#39;, p: \u0026#39;TreeNode\u0026#39;, q: \u0026#39;TreeNode\u0026#39;) -\u0026gt; \u0026#39;TreeNode\u0026#39;: if not root or root.val==p.val or root.val==q.val: return root left = self.lowestCommonAncestor(root.left,p,q) right = self.lowestCommonAncestor(root.right,p,q) if not left: return right if not right: return left return root 50. 二叉树中的最大路径和（困难） DFS\n如下列二叉树，最大路径和可能为以下三种情况：\np+a+b p+a+c a+b+c p / a / \\ b c 因此当递归到节点 a 时，求出以 a 为根节点时的最大路径和，更新 ans 为 上述第三种情况和当前 ans 的最大值，返回时返回 a+max(b,c)。\ndef maxPathSum(self, root: Optional[TreeNode]) -\u0026gt; int: self.ans=-float(\u0026#39;inf\u0026#39;) def dfs(node): if not node: return 0 l=max(dfs(node.left),0) r=max(dfs(node.right),0) self.ans=max(self.ans,node.val+l+r) return node.val+max(l,r) dfs(root) return self.ans ","permalink":"https://Achilles-10.github.io/posts/algo/hot1/","summary":"哈希 1. 两数之和（简单） 哈希 def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: tab = {} for i in range(len(nums)): if target-nums[i] in tab: return [i,tab[target-nums[i]]] tab[nums[i]]=i 2. 字母异位词分组（中等） 排序+哈希 def groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: tab = defaultdict(list) for s in strs: key = \u0026#39;\u0026#39;.join(sorted(s)) # list需要转换为字符串才能进行哈希 tab[key].append(s) return list(tab.values()) 计数+哈希 def groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: mp = defaultdict(list) for st in strs: counts = [0] * 26 for ch in st: counts[ord(ch) - ord(\u0026#34;a\u0026#34;)] += 1 # 需要将 list 转换成 tuple 才能进行哈希 mp[tuple(counts)].append(st) return list(mp.values())","title":"LeetCode 热题 100 : 0~50"},{"content":"比赛应用场景 MTCNN 的速度更快，优先使用 MTCNN 进行检测，对于采帧数量不够的视频再使用准确率和鲁棒性更强的 RetinaFace 检测，若仍然不够，则对成功识别的帧进行过采样。\nRetinaFace由于 FPN 结构的存在，对于小尺寸和大尺寸的人脸检测准确率更高。\nMTCNN( Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks) [paper] [code]\n引言 多任务卷积神经网络（Multi-task convolutional neural network，MTCNN），将人脸检测和人脸关键点检测集成在一个模型中实现。\n如上图所示，MTCNN 总体可分为 P-Net、R-Net 和 O-Net 三层网络。该模型采用候选框+分类器的思想，同时兼顾速度与精度，实现高效的人脸检测。\n滑动窗口 MTCNN中使用了滑动窗口的思想，利用卷积核的滑动方式，对图像进行多次提取。在MTCNN中，滑动窗口是一个Kernel_size=12，Stride=1的卷积核。但在面对被检测物体大小远大于或远小于卷积核大小时，需要用到图像金字塔，在指定缩放比例（论文中为0.7）以及最小图片尺寸后，将原图设置为图像金字塔底层，然后逐层缩放，在图像金字塔的每一层都用滑动窗口扫描，这样就能对各个位置、各个尺寸的物体进行识别。\nMTCNN网络结构 MTCNN 的三层网络结构分别是快速生成候选窗口的 P-Net、进行高精度候选窗口过滤选择的 R-Net 和生成最终边界框与人脸关键点的 O-Net。利用低复杂度的模型快速生成候选框，再利用高复杂度的模型对候选框进行筛选，以此实现速度与精度的共同提升。\n候选网络（Proposal Network, P-Net） 如上图（\u0026ldquo;MP 表示 max pooling\u0026rdquo;，图中应为 1$2\\times2$），P-Net 输入为$(12\\times 12\\times 3)$的特征图，经过卷积网络得到$(1\\times1\\times32)$的特征图，然后通过3个$(1\\times1)$卷积得到人脸分类、回归候选框（bounding box 的四个坐标偏移量）和人脸特征点坐标（5 个特征点的 x,y 坐标）。\n原图经过图像金字塔处理后，逐层输入到 P-Net 中，获取候选框后还原至原图大小，进行边框回归（bounding box regression），并利用NMS进行窗口过滤，得到 P-Net 的最终输出。\n精炼网络（Refinement Network, R-Net） R-Net 对候选框进行精筛。R-Net 的输入尺寸为$(24\\times24\\times3)$，经过卷积及最大池化后将特征图形状变为 $(3\\times3\\times64)$，经过flatten和全连接层后得到长度为 128 的向量，再用三个全连接层分别输出人脸分类、边框和特征点信息。\n输出网络（Output Network, O-Net） O-Net 输入为$(48\\times48\\times3)$，数据处理流程和 R-Net 一样， R-Net 和 O-Net 都起到一个对 P-Net 的输出结构进行精筛的作用。\n损失函数 MTCNN 的损失函数为： $$ \\mathcal{L}_{MTCNN}=\\alpha\\mathcal{L}^{det}+\\beta\\mathcal{L}^{box}+\\gamma\\mathcal{L}^{landmark} $$\n其中 $\\mathcal{L}^{det}$ 为人脸分类损失，使用交叉熵损失函数：\n$$ \\mathcal{L}_i^{det}=-\\sum_{i}^{N}{(y_i\\cdot\\log(\\hat{y}_i)+(1-y_i)\\cdot\\log(1-\\hat{y}_i))} $$\n$\\mathcal{L}^{box}$ 计算欧氏距离作为边框回归损失函数： $$ \\mathcal{L}_i^{box}=||\\hat{y}_i^{box}-y_i^{box}||_2^2 $$\n$\\mathcal{L}^{landmark}$ 为人脸特征点坐标损失，使用欧氏距离 $$ \\mathcal{L}_i^{landmark}=||\\hat{y}_i^{landmark}-y_i^{landmark}||^2_2 $$ 在 P、R、O 网络中，三个损失函数的超参数取值有所不同：\nP-Net: $\\alpha:\\beta:\\gamma=1:0.5:0.5$ R-Net: $\\alpha:\\beta:\\gamma=1:0.5:0.5$ O-Net: $\\alpha:\\beta:\\gamma=1:0.5:1$ Hard Sample mining 在训练时，对分类损失进行 Hard Sample mining。在一个batch里，取分类损失最大的前 70% 进行反向传播。不对回归做难样本挖掘是因为回归问题即使是微小的修正都是有用的，但二分类则不然。\n样本选择与训练 在 MTCNN中 的 P、R、O 网络都要同时预测是否为人脸和预测人脸框，在训练时使用以下4类数据样本进行训练：\nNegatives: 与 GT $\\text{IoU} \u0026lt; 0.3$; Positives: $\\text{IoU}\u0026gt;0.65$; Part Faces: $0.4\u0026lt;\\text{IoU}\u0026lt;0.65$; Landmark Faces: 带有5个特征点标签的图片 这4类数据的数据量比例大概为3:1:1:2，不同类型的数据将用于训练不同的分类或回归网络：\n人脸分类：Negatives + Positives; 正样本和负样本易区分，容易使模型收敛； 人脸边框回归：Positive + Part Faces; 负样本中几乎没有人脸，无法训练人脸框； 人脸特征点：Landmark Faces. 在生成训练数据的时候，先从原始数据集的真实人脸框周围随机生成切片（crop patches），根据其与 GT 的 IoU，切图并划分正样本（Positives）、负样本（Negatives）、部分样本（Part Faces）三类数据集，并结合原始数据集中的 GT 和人脸特征点（Landmark Faces）进行切图，获取 Landmark Faces 类数据。而后将所有数据的尺寸修改到 P、R、O 三个网络的输入尺寸。\n在训练过程中应该优先训练 P-Net，再训练 R-Net 和 O-Net 时，可直接使用 P-Net 的预测结果与 GT 人脸框做 IoU，来生成 Positives、 Negatives、 Part Faces 三类数据，这样更符合模型的真实预测情景，有助于提高模型在实际应用场景中的精度。\nPytorch 实现 P-Net import torch import torch.nn as nn import torch.nn.functional as F class PNet(nn.Module): def __init__(self): super(PNet, self).__init__() self.conv_layer = nn.Sequential( nn.Conv2d(3, 10, kernel_size=3, stride=1), # padding=\u0026#39;valid\u0026#39; 不填充 nn.PReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # padding=\u0026#39;same\u0026#39; 填充 nn.Conv2d(10, 16, kernel_size=3, stride=1), # padding=\u0026#39;valid\u0026#39; nn.PReLU(), nn.Conv2d(16, 32, kernel_size=3, stride=1), # padding=\u0026#39;valid\u0026#39; nn.PReLU() ) self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1) self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1) self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1) def forward(self, x): x = self.conv_layer(x) cond = F.sigmoid(self.conv4_1(x)) box_offset = self.conv4_2(x) land_offset = self.conv4_3(x) return cond, box_offset, land_offset R-Net import torch import torch.nn as nn import torch.nn.functional as F class RNet(nn.Module): def __init__(self): super(RNet, self).__init__() self.conv_layer = nn.Sequential( nn.Conv2d(3, 28, kernel_size=3, stride=1), nn.PReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), nn.Conv2d(28, 48, kernel_size=3, stride=1), nn.PReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(48, 64, kernel_size=2, stride=1), nn.PReLU() ) self.line1 = nn.Sequential( nn.Linear(64 * 3 * 3, 128), nn.PReLU() ) self.line2_1 = nn.Linear(128, 1) self.line2_2 = nn.Linear(128, 4) self.line2_3 = nn.Linear(128, 10) def forward(self, x): x = self.conv_layer(x) x = x.view(x.size(0), -1) x = self.line1(x) label = F.sigmoid(self.line2_1(x)) box_offset = self.line2_2(x) land_offset = self.line2_3(x) return label, box_offset, land_offset O-Net import torch import torch.nn as nn import torch.nn.functional as F class ONet(nn.Module): def __init__(self): super(ONet, self).__init__() self.conv_layer = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, stride=1), nn.PReLU(), nn.MaxPool2d(kernel_size=2, stride=2, padding=1), nn.Conv2d(32, 64, kernel_size=3, stride=1), nn.PReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.PReLU(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(64, 128, kernel_size=2, stride=1), nn.PReLU() ) self.line1 = nn.Sequential( nn.Linear(128 * 3 * 3, 256), nn.PReLU() ) self.line2_1 = nn.Linear(256, 1) self.line2_2 = nn.Linear(256, 4) self.line2_3 = nn.Linear(256, 10) def forward(self, x): x = self.pre_layer(x) x = x.view(x.size(0), -1) x = self.conv5(x) x = self.prelu5(x) label = F.sigmoid(self.line2_1(x)) box_offset = self.line2_2(x) land_offset = self.line2_3(x) return label, box_offset, land_offset RetinaFace: Single-shot Multi-level Face Localisation in the Wild [paper] [code]\n主要贡献 在 WIDER FACE 上添加 landmark 标注，在 hard face 上检测显著改善； 在已有的监督分支上添加 mesh decoder 分支，预测像素级 3D 人脸信息； 在 WIDER FACE 的 hard 级别的测试集中，RetinaFace 超出 SOTA 1.1%； 在 IJB-C 测试集中，RetinaFace 使 Arcface 人脸识别中的结果得到提升 采用轻量级的 backbone，RetinaFace 能在单CPU上实时运行 VGA 分辨率（320*240）的图像 Multi-task Loss $$ \\begin{align*} \\mathcal{L}\u0026amp;=\\mathcal{L}_{cls}(p_i,p_i^*)+\\lambda_1p_i^*\\mathcal{L}_{box}(t_i,t_i^*)\\\\ \u0026amp;+\\lambda_2p_i^*\\mathcal{L}_{pts}(l_i,l_i^*)+\\lambda_3p_i^*\\mathcal{L}_{mesh}(v_i,v_i^*) \\end{align*} $$\n$\\mathcal{L}_{cls}$ 为分类损失，$\\mathcal{L}_{box}$ 表示 bbox 回归损失，$\\mathcal{L}_{pts}$ 表示面部标志点回归损失，$\\mathcal{L}_{mesh}$ 为三维重建损失。\n特征金字塔（Feature Pyramid Networks，FPN） 为了能检测出图像中不同尺寸大小的目标，有以下几种图像算法：\n特征图像金字塔（Featurized image pyramid）：\n生成不同尺寸的图片，每张图片生成各自尺寸的特征图，分别进行预测，最后统计所有尺寸的预测结果。\n单一特征图（Single feature map）\n使用深度神级网络的某一层 feature map 进行预测。靠近网络输入层的特征图包含粗略位置信息，使得目标 bbox 位置不准确；靠近最后一层的特征图会忽略微小物体信息。\n金字塔特征层（Pyramidal feature hierarchy）\n使用不同层次的金字塔层feature map进行预测。\n特征金字塔网络（Feature Pyramid Network）\n对深层特征进行上采样，再与对应层特征进行融合，得到高分辨率、强语义的特征，加强了对特征的提取。\n核心思想：把深层语义传递回浅层，补充浅层的语义信息，从而特到高分辨率和强语义的特征，有利于小目标的检测。\n特征融合 特征融合的流程是，深层特征图经过上采样，浅层特征图经过$1\\times1$卷积对齐通道，两者相加。\n","permalink":"https://Achilles-10.github.io/posts/tech/face/","summary":"比赛应用场景 MTCNN 的速度更快，优先使用 MTCNN 进行检测，对于采帧数量不够的视频再使用准确率和鲁棒性更强的 RetinaFace 检测，若仍然不够，则对成功识别的帧进行过采样。 RetinaFace由于 FPN 结构的存在，对于小尺寸和大尺寸的人脸检测准确率更高。 MTCNN( Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks) [paper] [code] 引言 多任务卷积神经网络（Multi","title":"人脸检测器：MTCNN 与 RetinaFace"},{"content":"[paper] [code]\n1. 引言 现有基于视频的HR估计方法分为两大类：1. 基于rPPG的方法；2. 基于心冲击信号（BCG）的方法。rPPG信号容易受到头部运动和环境条件（光照）的影响。\n构建了一个包含107名受试者的2378人脸视频和752个近红外人脸视频的数据集VIPL-HR，由三种不同设备拍摄（RGB-D相机，智能手机和网络摄像头）9种不同情况下的视频，涵盖面部姿势和光照变化等。\n主要贡献：\n回顾远程HR估计工作，分析该问题仍面临的挑战；\n考虑相邻HR测量之间的相关性，利用GRU进行建模，实现鲁棒的HR估计；\n对多个数据集进行评估，实验结果超过SOTA.\n2. VIPL-HR DATABASE 2.1 设备设置与数据收集 考虑不同光照条件，受试者姿态，不同摄像头的距离，不同拍摄设备\n2.2 人脸视频压缩 MJPG压缩至2/3分辨率。\n2.3 数据集统计信息 3. 方法 首先执行检测和面部标志检测来定位ROI，从中计算时空图来作为HR信号的low-level表示。最后设计了一个CNN-RNN模型来学习。\n3.1 人脸检测、landmark检测和分割 SeetaFace检测人脸和定位81个标志点，以获得准确一致的ROI面部区域。\n3.2 用时空图表示HR信号 通过landmark跨帧对齐人脸然后将其转换为YUV颜色空间，将人脸划分为n个ROI区域，计算每个区域每个颜色通道的均值$[Y_1,U_1,V_1,Y_2,U_2,V_2,\\dots,Y_n,U_n,V_n]$，这样得到了$n*c$序列的空间图，然后得到$T\\times n\\times c$的时空图。\n对每个时间信号应用最小-最大归一化，并将时间序列的值缩放为[0,255]。\n颜色空间的选择对HR信号非常重要，经过测试选择YUV色彩空间。\n考虑由于运动导致的部分帧数缺失的情况，沿时间维度随机屏蔽一小部分时空图来模拟确实数据的情况，并视作一种数据增强方式来增强RhythmNet的鲁棒性。\n3.3 对HR信号的时序建模 对于每个人脸视频，使用包含w帧的固定滑动窗口(0.5s步长移动)将其分为单独视频片段$[v_1,v_2,\\dots,v_t]$，使用ResNet-18作为backbone，L1损失衡量预测HR值与心率值的差异。\n我们进一步考虑相邻视频段的关系，利用GRU进行时序关系建模。将CNN输出的特征输入到GRU，将GRU的输出输入到全连接层回归各个视频段的HR值，将每个视频段预测的HR值的均值作为最终的HR结果。\n此外，由于同一视频的连续视频块心率变化不大，引入Smooth函数来约束单一视频块的预测值：\n$$ \\mathcal{L}_{\\text{smooth}}=\\frac{1}{T}\\sum_{i}^{T}{|hr_t-hr_{mean}|} $$\n总损失函数($\\lambda=100$)为：\n$$ \\mathcal{L}=\\mathcal{L_{l1}}+\\lambda\\mathcal{L}_{\\text{smooth}} $$\n4. 实验 4.1 数据集与实验设定 选取300帧来计算时空图，5*5的ROI区域。用6个连续的hr值计算$L_\\text{smooth}$。\n4.2 数据集内测试 RGB视频上的测试：\n近红外视频上的测试\n4.3 跨数据集测试 在VIPL-HR数据集上训练，在MAHNOB-HCI和MMSE-HR数据集上训练。\n4.4 关键模块分析 时空图\n颜色空间选择\n时序建模\n见RGB视频测试部分。\n4.5 鲁棒性分析 视频压缩\n光照变化\n昏暗环境下误差变大，且暗光条件下，在NIR视频上训练的模型误差更小。\n头部运动\n","permalink":"https://Achilles-10.github.io/posts/paper/rhythm/","summary":"[paper] [code] 1. 引言 现有基于视频的HR估计方法分为两大类：1. 基于rPPG的方法；2. 基于心冲击信号（BCG）的方法。rPPG信号容易受到头部运动和环境条件（光照）的影响。 构建了一个包含107名受试者的2378人脸视频和752个近红外人脸视频的数据集VIPL-HR，由三种不同设备拍摄（RG","title":"RhythmNet: End-to-end Heart Rate Estimation from Face via Spatial-temporal Representation"},{"content":"1. PPG测量心率 PPG(photoplethysmographic,光电容积脉搏波描记法) 以LED光源和探测器为基础,测量经过人体血管和组织反射、吸收后的衰减光,记录血管的搏动状态并测量脉搏波。\n1.1 原理 当LED光射向皮肤，透过皮肤组织反射回的光被光敏传感器接受并转换成电信号再经过AD转换成数字信号。像肌肉、骨骼、静脉等等对光的吸收是基本不变的（前提是测量部位没有大幅度的运动），但是血液不同，由于动脉里有血液的流动，那么对光的吸收自然也有所变化。我们把光转换成电信号时，正是由于动脉对光的吸收有变化，其他组织对光的吸收基本不变，得到的信号就可以分为直流交流AC信号和DC信号。提取其中的AC信号，就能反应出血液流动的特点。\n传感器选择绿光原因：\n皮肤的黑色素会吸收大量波长较短的波 皮肤上的水份也会吸收大量的UV和IR部分的光 进入皮肤组织的绿光(500nm)\u0026ndash; 黄光(600nm)大部分会被红细胞吸收 红光和接近IR的光相比其他波长的光更容易穿过皮肤组织 血液要比其他组织吸收更多的光 相比红光，绿（绿-黄）光能被氧合血红蛋白和脱氧血红蛋白吸收 1.2 对PPG信号的处理方法 时域分析\n对原始PPG信号进行滤波处理，得到一定时间内的波峰个数，即可算出心率值。\n","permalink":"https://Achilles-10.github.io/posts/tech/ppgsignal/","summary":"1. PPG测量心率 PPG(photoplethysmographic,光电容积脉搏波描记法) 以LED光源和探测器为基础,测量经过人体血管和组织反射、吸收后的衰减光,记录血管的搏动状态并测量脉搏波。 1.1 原理 当LED光射向皮肤，透过皮肤组织反射回的光被光敏传感器接受并转换成电信号再经过A","title":"PPG信号测心率原理与常见信号评价指标"},{"content":"[paper] [code]\n摘要 显式建模生成由人脸运动产生的运动失真；使用逆渲染从视频帧中获取人脸和环境光照的3D形状和反照率然后渲染每一帧人脸。利用生成的运动扰动对运动诱导的测量值进行滤波。信号质量提高2dB；激烈运动场景心率估计RMSE提高30%。\n1. Introduction 来自相机的rPPG信号具有极低的信号强度，并且受到传感器噪声和运动伪影的影响。目前方法大多无法处理剧烈运动产生的运动伪影，仅限于实验室环境使用。\n我们使用双向LSTM生成运动伪影来过滤运动诱导的rPPG信号。\n如下图所示：(a)输入参考视频帧; (b)基于估计的3D形状和光照渲染人脸; (c)对参考帧注册连续帧; (d)从(a)中测量人脸上一个跟踪点的像素强度变化; (e)运动信号，使用生成的运动信号抵消运动扰动，从而在(f)中产生干净的信号，红线表示实际心率。\n为了生成运动扰动，需要知道人脸几何形状和光照；人脸几何形状可以通过深度估计相机来获得；光照信息需要在相同环境下进行预校准。在实际场景中这几乎是不可行的。\n通过使用3D FaceMesh模型来获得人脸的三维几何结构，该模型给出了人脸在每个时刻的近似几何结构。其次，基于一个近似的三维人脸几何图形，使用序列帧来估计场景光照。\n主要贡献如下：\n开发了一个框架，使用通过反向渲染来估计的3D人脸模型和场景光照，显式地建模基于相机的rPPG信号中的运动扰动。我们使用生成的运动信号，通过双向LSTM过滤rPPG信号中的运动扰动，得到干净的信号。\n实验表明该方法在提取的rPPG信号质量和估计的心率精度方面优于SOTA。RobustPPG在复杂运动场景下的信号质量提高了2dB以上，在剧烈运动场景下的心率估计比次优方法提高了33%。\n使用一个扩展的光度立体装置来验证pipeline。FaceMesh生成的表面法线与光度立体法生成的表面法线GT平均偏离13°。表明即使采用近似的人脸几何估计，使用FaceMesh估计的运动信号与GT运动信号的归一化均方根误差也小于10%。在rPPG信号提取方面，FaceMesh生成的3D人脸几何图形获得了近乎最优的性能。\n2. 背景和挑战 主要目标是开发一种鲁棒的算法来从视频中皮肤像素强度波动中恢复rPPG信号，然后从rPPG信号中估计心率。\n以下原因导致获得运动扰动信息是困难的：\n运动扰动依赖于面部局部方向：如上图a)，即使分别跟踪人脸的不同区域，运动扰动也不同\n运动扰动依赖于光照环境：如上图b)，对于人脸上的同一点的相同运动，不同光照方向下的强度变化也不同\n3. 方法 在这项工作中，利用逆渲染 (Inverse Rendering) 来显式地从视频中生成运动扰动。\n如图，首先用3D人脸跟踪器FaceMesh获取每帧人脸的3D形状，然后估计光照方向，并在每个三角形局部区域生成精确的运动扰动。最后在双向LSTM中同时利用愚弄当心好和损耗的原生像素强度波动来获得干净的rPPG信号。\n3.1 运动信号模型 根据二色反射模型 (Dichromatic Reflection Model, DRM)，人脸任意3D位置r在t时刻的RGB像素强度可以描述为漫反射和镜面反射分量之和：\n$$ \\mathrm{i}(\\mathrm{r},t)=\\mathrm{i}_{\\text{diffuse}}+\\mathrm{i}_{\\text{specular}} $$\n$\\mathrm{i}_{\\text{diffuse}}$ 和 $\\mathrm{i}_{\\text{specular}}$均$\\in\\mathbb{R}^{3\\times1}$。\n如下图，我们做出以下假设：\n光源为远离人脸的点光源，且与相机的位置保持不变。因此在所有位置上均为平行光，且光源强度保持恒定\n人脸具有Lambertian反射，在Lambertian假设下，所有点源可以建模为一个点源\n在这个假设下有：\n$$ \\mathbf{i}(\\mathbf{r},t)=\\mathbf{c}*\\mathbf{n}(\\mathbf{r},t)\\cdot \\mathbf{l}+\\mathbf{e}*p(t)\\odot(\\mathbf{c}*\\mathbf{n}(\\mathbf{r},t)\\cdot \\mathbf{l})\\tag{1} $$\n3.2 生成运动信号 要从上述公式中提取搏动的血容量信号$p(t)$，需要生成运动扰动。需要三个参数：表面法线$\\mathbf{n(r},t)$；有效光源方向$\\mathbf{I}$；随时间保持不变的平均肤色$\\mathbf{c}$\n3.2.1 3D 人脸建模 使用FaceMesh在视频的每一帧进行人脸跟踪和拟合。\n首先在每一帧中检测并跟踪人脸，然后检测每一帧的人脸特征点；\n然后利用3DMM (3D Morphable Models) 进行人脸拟合，生成3D人脸几何形状和纹理，生成稠密的三角形网络，如下图，计算每个三角形像素强度的平均值；\n因此，对于每一个视频，有表面法向量$\\mathbf{N}\\in\\mathbb{R}^{K\\times T\\times 3}$，K是每一帧中的三角形数，T是帧数，3表示xyz三个空间分量。强度$\\mathbf{I}\\in\\mathbb{R}^{K\\times T\\times 3}$，3表示RGB通道像素强度。\n3.2.2 光照估计 对于一序列帧，剔除高光、嘴唇和头发区域，使用面上所有三角网络的测量值估计光源方向。上面的公式(1)中忽略rPPG部分，有：\n$$ \\mathbf{I}=\\mathbf{N}*\\mathbf{l}*\\mathbf{c}^\\intercal $$\n其中$\\mathbf{I}\\in\\mathbb{R}^{K\\times T_w\\times 3}$是像素强度，$\\mathbf{N}\\in\\mathbb{R}^{K\\times T_w\\times 3}$为表面法线方向，$T_w$为帧数。需要估计有效光源方向$\\mathbf{l}\\in\\mathbb{R}^{3\\times1}$和评价肤色（假设不同位置肤色相同）$\\mathbf{c}\\in\\mathbb{R}^{3\\times 1}$\n3.2.3 生成信号矩阵 如下图所示，在估计有效光照方向$\\mathbf{\\widehat{I}}$和平均肤色$\\mathbf{\\widehat{c}}$后，生成每个三角区域$\\mathbf{r}$的运动信号$\\mathbf{m(r},t)$：\n$$ \\mathbf{m(r},t)=\\mathbf{\\widehat{c}}*\\mathbf{n(r},t)\\cdot\\mathbf{\\widehat{I}}=(\\mathbf{n(r},t)^\\intercal*\\mathbf{\\widehat{I}}*\\mathbf{\\widehat{c}}^\\intercal)^\\intercal $$\n因此，对于每个三角区域，能够得到六个信号：RGB像素强度$(i_{r}(t),i_{g}(t),i_{b}(t))$和RGB运动信号$(m_r(t),m_g(t),m_b(t))$，重写公式(1)得到时序运动扰动$m$的函数：\n$$ \\mathbf{i}(\\mathbf{r},t)=\\mathbf{m}(\\mathbf{r},t)+e*p(t)\\odot\\mathbf{m}(\\mathbf{r},t)\\tag{2} $$\n其中$\\mathbf{i}(\\mathbf{r},t)$是有干扰的rPPG信号，$\\mathbf{m}(\\mathbf{r},t)$是运动扰动信号$p(t)$是干净的rPPG信号。使用生成的运动信号,构造信号特征矩阵\n$$ S_r=[i_{r}(t),i_{g}(t),i_{b}(t),m_r(t),m_g(t),m_b(t)]^\\intercal\\in\\mathbb{R}^{6\\times t} $$\n3.3 rPPG信号的运动抵消 将$S_r$输入Bi-LSTM，接触的脉冲器波形作为标签进行训练。\n对于该架构，使用包含30隐藏单元的3层的Bi-LSTM网络。将信号划分为4秒的窗口，重叠部分为2秒，作为输入，损失函数为MSE。\n4 实验 4.1 FaceMesh 验证 FaceMesh 面部跟踪器决定了光照估计的准确率和运动信号生成的质量。\n人脸几何形状的准确性\n光照方向的误差\n运动信号的质量\n不准确的运动信号生成对rPPG信号的影响\n4.1.1 光度立体设置 使用光度立体来获取运动中的真实3D人脸几何形状。\n4.2 3D 面部几何形状估计 如上图。在鼻子区域的角度误差最大；平均角误差（除去眼睛鼻子和嘴巴区域）在人脸模型和真人上分别为$13.8^\\circ, 18.37^\\circ$\n在上图的下半部分，展示了旋转过程中人脸模型前额上一个三角形和说话场景下真人的角误差。\n4.3 光照估计准确率 用FaceMesh得到的三维几何结构来获得光照矩阵的估计值$\\mathbf{\\widehat{U}}$。在四个人体模型上平均误差为$4.56^\\circ$。\n4.4 运动信号生成 下图展示了一个由光度立体和FaceMesh生成的0运动信号和来自单个三角形的实际像素强度的例子。第三行滤波后的残余信号不包含强信号，说明成功去除了运动信号。\n然后用带通滤波器$([0.5-5]Hz)$对运动信号滤波（人体心率属于这一频率范围）。计算两个指标：1）归一化方根误差(NRMSE)；2）人体模型视频中估计的运动信号和实际像素强度之间的归一化互相关(normalized cross-correlation, NCC)。如下表：\n4.5 rPPG信号估计 使用光度立体生成的运动信号在估计rPPG信号的平均信噪比方面，与FaceMesh相比，没有显著提高（0.15dB,p\u0026gt;0.005）。因此，使用FaceMesh生成的运动信号在rPPG信号估计方面达到了接近最优的性能。\n5 PPG信号估计 5.1 数据集 PURE dataset\nPURE数据集包含10名被试在6种运动条件下同步生理数据的人脸视频，包括头部转动和说话。视频时长约为1 min，真值PPG波形由接触式脉搏血氧监测仪提供。\n为了验证在剧烈运动场景下的性能，创建了一个单独的数据集RICE-motion，包括12名受试者的72段视频(9男3女)，包含快速的头部旋转动作和自然表情说话场景。\n5.2 训练和验证运动消除网络 将信号特征矩阵$S_r$作为Bi-LSTM的输入，模型学习像素强度变化和运动扰动信号与PPG信号关联的函数。由于真实数据集较小，通过在rPPG信号中生成各种运动扰动来生成一个合成数据集用于训练。\n合成数据集：使用公式(2)来合成信号矩阵。使用参数化模型来生产干净的PPG信号$p(t)$，心率从30bpm到240bpm均匀分布中随机选取，生成一个30s的干净PPG信号。使用随机布朗噪声生成器（random Brownian noise generator）来生产运动信号$\\mathbf{m}(t)$。最后，在像素强度中添加随即白噪声来模拟建模误差和相机传感器误差。参数$\\mathbf{e}_{ppg}=[0.18,0.78,0.60]$保持恒定。生成了400个运动信号$\\mathbf{m}(t)$，与合成的RGB信号强度$\\mathbf{i}(t)$一起合成信号矩阵$S_r$。\n对信号进行标准化（减均值除标准差），然后用带通滤波器$([0.5~5])Hz$进行滤波。\n将模型预测的rPPG信号从面部所有三角形位置进行空间平均，得到整体rPPG信号。\n5.3 性能比较 计算不同方法提取的rPPG信号的信噪比来评估PPG信号的质量。\n基于提取的rPPG信号计算心率，使用5秒和1秒的短重叠窗口来计算瞬时心率，然后计算心率与真值的RMSE。\n5.4 结果和讨论 上图为两个数据集中原始的rPPG信号和生成的运动信号。\n下表报告了PURE数据集里所有受试者六种运动的平均SNR值。对于静态或平稳简单的运动，所有方法SNR相对一致；在说话等复杂的面部动作中，所有方法的SNR都会下降。其次，RobustPPG在所有运动场景下的表现优于其他方法且在会话场景下的优势最大。\n展示下图方法估计的血容量信号相较于PURE和RICE-motion数据集的真值频谱图。可以观察到，RobustPPG方法估计的心率信号比其他方法更干净，且上表表明RobustPPG能够提供给更可靠的心率变异性（heart rate variability, HRV）和平均心率测量。\n下表为在更具挑战性的RICE-motion数据集上的SNR值和心率的RMSE值。\n此外，还通过测试在室内和室外手机视频上评估RobustPPG方法，如下图所示。RobustPPG估计的频谱图在预期心率频带内有较强的信号成分和更少的扰动，且有着更高的SNR，展现了在不同光照条件下更强的鲁棒性。\n此外，下表展示了有无胡须和不同肤色的测试结果。\nSNR(dB) RMSE(bpm) 有胡须 4.75 2.31 无胡须 5.16 1.92 白色皮肤 5.89 2.33 橄榄色皮肤 4.96 2.94 在训练时使用手指的PPG信号作为标签，但相对于面部PPG信号，手指PPG信号1）具有更多的特征和更高的谐波；2）并且由于脉冲传输产生相位延迟。通过对指脉波形进行低通滤波解决第一个问题；相位延迟问题较难解决。\nRobustPPG有4个部分：1）人脸跟踪；2）表面法线、像素强度波动提取和光照估计；3）运动信号生成；使用Bi-LSTM网络提取每个三角网格处的rPPG信号。计算瓶颈在从每一帧图像中提取每个三角网格处的像素强度。\n6 总结和展望 我们提出了一种新的算法RobustPPG用于基于摄像头的rPPG信号提取和心率估计。我们证明了像FaceMesh这样的3D人脸跟踪器可以在像素强度变化的情况下产生精确的运动失真。此外，使用Bi-LSTM网络进行信号滤波，我们在rPPG信号提取中表现出比现有方法更好的准确性。我们希望这项工作将大大推动运动鲁棒性的极限，以实现可靠的心率估计，并能将其应用到现实生活中。\n在本工作中，我们仅对Lambertian建模引起的运动畸变进行建模。可以考虑镜面成分，使建模更加准确。其次，我们在工作中只考虑了远距离照明的假设。近光场景要求建模的复杂性可以被探索以更好地估计运动信号。第三，我们还考虑了相机固定的情况。摄像头的移动会造成rPPG信号中额外的信号失真，这可能会影响手持电话场景下心率估计的准确性。这些都是值得探索的有趣途径，或许可以作为未来工作的令人兴奋的方向。\n","permalink":"https://Achilles-10.github.io/posts/paper/robustppg/","summary":"[paper] [code] 摘要 显式建模生成由人脸运动产生的运动失真；使用逆渲染从视频帧中获取人脸和环境光照的3D形状和反照率然后渲染每一帧人脸。利用生成的运动扰动对运动诱导的测量值进行滤波。信号质量提高2dB；激烈运动场景心率估计RMSE提高30%。 1. Introduction 来自相机的rPPG信号具有极低的信号强度，并且受","title":"RobustPPG: camera-based robust heart rate estimation using motion cancellation"},{"content":"形态学转换 形态变换是基于图像形状的简单操作，两种基本形态学算子是侵蚀和膨胀，也包括变体形式开运算和闭运算等。\n侵蚀(Erosion) 侵蚀前景物体的边界，内核在2D卷积时，只有当内核下所有像素都为1时才为1，否则被侵蚀变成0。根据内核大小，边界附近的像素都会被丢弃，因此能减小前景对象（白色区域）的大小，有助于去除小的白色噪声。如下例。\nimg = cv2.imread(\u0026#39;j.png\u0026#39;,0) kernel = np.ones((5,5),np.uint8) erosion = cv2.erode(img,kernel,iterations = 1) plt.imshow(np.hstack((img,erosion)),\u0026#39;gray\u0026#39;) 扩张、膨胀(Dilation) 与侵蚀相反，当内核下至少一个像素为1时，则为1，因此能增加图像中前景对象（白色区域）的大小。\ndilation = cv2.dilate(img,kernel,iterations = 1) plt.imshow(np.hstack((img,dilation)),\u0026#39;gray\u0026#39;) 开运算(Opening) 开运算是先侵蚀再扩张，有助于消除盐噪声，如下例。\nopening = cv2.morphologyEx(salt, cv2.MORPH_OPEN, kernel) plt.imshow(np.hstack((salt,opening)),\u0026#39;gray\u0026#39;) 闭运算(Closing) 闭运算是先扩张再侵蚀，有助于去除前景对象内部的小黑点（椒噪声），如下例。\nclosing = cv2.morphologyEx(pepper, cv2.MORPH_CLOSE, kernel) plt.imshow(np.hstack((pepper,closing)),\u0026#39;gray\u0026#39;) 形态学梯度 图像扩张和侵蚀的差，结果类似于图像的轮廓线。\ngradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel) # dilation-erosion plt.imshow(np.hstack((img,gradient)),\u0026#39;gray\u0026#39;) Top Hat 输入图像与开运算的差。\nkernel_tophat = np.ones((9,9),np.uint8) tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel_tophat) # img-opening plt.imshow(np.hstack((img,tophat)),\u0026#39;gray\u0026#39;) Black Hat 输入图像与闭运算的差。\nkernel_blackhat = np.ones((9,9),np.uint8) blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel_blackhat) # img-closing plt.imshow(np.hstack((img,blackhat)),\u0026#39;gray\u0026#39;) 结构元素 可以通过cv.getStructuringElement()函数来创建椭圆形、圆形的内核，只需传递内核形状和大小即可。\n\u0026gt;\u0026gt;\u0026gt; cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)) #矩形 array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], dtype=uint8) \u0026gt;\u0026gt;\u0026gt; cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5)) #椭圆 array([[0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0]], dtype=uint8) \u0026gt;\u0026gt;\u0026gt; cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5)) #十字形 array([[0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]], dtype=uint8) 图像梯度 OpenCV提供三种类型的梯度滤波器或高通滤波器用于查找图像梯度和边缘，即Sobel，Scharr和Laplacian。\nSobel和Scharr算子 Sobel算子是高斯平滑加微分运算的联合运算，它对噪声更具鲁棒性。可以通过xorder和yorder来指定导数方向，通过ksize指定内核大小，当ksize=-1时，使用Scharr滤波器，效果比Sobel滤波器更好。\nLaplacian算子 计算由关系$\\Delta src=\\frac{\\partial src}{\\partial^2 x^2}+\\frac{\\partial^2 src}{\\partial y^2}$给出的图像的拉普拉斯图。每一阶导数是由Sobel算子计算。如果ksize=1，则使用以下内核进行滤波： $$ kernel=\\begin{bmatrix}0\u0026amp;1\u0026amp;0\\\\1\u0026amp;-4\u0026amp;1\\\\0\u0026amp;1\u0026amp;0\\end{bmatrix} $$\n代码 以下实例在一个图标中展示了所有算子，内核都是5x5大小，深度设置为1来得到np.int8类型的结果图像。\nimport numpy as np import cv2 from matplotlib import pyplot as plt img = cv2.imread(\u0026#39;sudoku.png\u0026#39;,0) laplacian = cv2.Laplacian(img,cv2.CV_64F) sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5) sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5) plt.figure(figsize=(8,8)) plt.subplot(2,2,1),plt.imshow(img,cmap = \u0026#39;gray\u0026#39;) plt.title(\u0026#39;Original\u0026#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,2),plt.imshow(laplacian,cmap = \u0026#39;gray\u0026#39;) plt.title(\u0026#39;Laplacian\u0026#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,3),plt.imshow(sobelx,cmap = \u0026#39;gray\u0026#39;) plt.title(\u0026#39;Sobel X\u0026#39;), plt.xticks([]), plt.yticks([]) plt.subplot(2,2,4),plt.imshow(sobely,cmap = \u0026#39;gray\u0026#39;) plt.title(\u0026#39;Sobel Y\u0026#39;), plt.xticks([]), plt.yticks([]) plt.tight_layout() plt.show() ","permalink":"https://Achilles-10.github.io/posts/tech/opencv5/","summary":"形态学转换 形态变换是基于图像形状的简单操作，两种基本形态学算子是侵蚀和膨胀，也包括变体形式开运算和闭运算等。 侵蚀(Erosion) 侵蚀前景物体的边界，内核在2D卷积时，只有当内核下所有像素都为1时才为1，否则被侵蚀变成0。根据内核大小，边界附近的像素都会被丢弃，因此能减小前景对象","title":"OpenCV-Python学习笔记(5)：形态学变换与图像梯度"},{"content":"[paper]\n1. Introduction 如下图所示，真实图像的边缘图保留了丰富的纹理信息，在伪造图像的边缘途中丢失了一些细节，此外，噪声图展示噪声在真实图像的面部特征和边界上均匀分布，在伪造图像中集中于边界。\n本文主要贡献如下：\n提出prior-attention机制，利用先验信息构建注意力图，引入边缘和噪声信息，有助于突出鉴别性特征； 基于prior-attention机制，设计了可嵌入的特征提取模块(Feature Abstraction Block, FAB)来促进层级间交互，以检测微小的伪造痕迹。 实验结果表面本文方法有很好的模块化和可解释性，以及出色的准确性。 2. Method 本文的框架如下图所示。基于Transformer的FAB模块被嵌入到骨干网络中，先验注意力模块替换Transformer中的多头注意力机制来增强纹理信息。\n2.1 先验注意力模块(Prior-Attention) 使用SRM滤波器提取噪声图，Canny边缘检测器提取边缘图，将这些先验信息来构建注意力图。记第l层CNN的特征图为$f_l\\in\\mathbb{R}^{H\\times W\\times C}$。将特征图分成N个patch，得到patch-level特征序列$S=[X_{cls},X_1,\\dots,X_N]\\in\\mathbb{R}^{(N+1)\\times D}$。模块细节见下图。由特征序列$S$得到$Q,K,V\\in\\mathbb{R}^{(N+1)\\times(D/h)}$矩阵，h为头数(number of heads)。\n应用先验信息时，仅将其添加到ClsToken上，对每个注意力头以相同的方式执行该操作。 $$ AttMap_{cls}=Att_{template}+Att_{cls} $$\n2.2 特征提取模块(Feature Abstraction Block, FAB) 随着网络的深入，特征通道增多，可能造成信息冗余，于是通过$1\\times1$卷积和Instance Normalization来实现层内特征压缩提取。然后，将前一层的特征转换成1D的ClsToken，实现层间特征提取。 $$ \\text{ClsToken}=Linear(Flatten(MaxPool(f_{l-1}))) $$\n$$ \\text{PatchTokens}=Reshape(IN(Conv_{1\\times1(f_l)})) $$\n$$ \\text{S}=Concat(\\text{ClsToken,PatchTokens}) $$\n2.3 特征融合与损失函数 考虑到感受野的多样性，各特征层的patch大小分别设置为4,2,1。为了使不同层的特征交互学习并且节省开销，FAB模块共享权重。最后对齐各层的ClsToken并相加，通过全连接层得到最终的预测概率。损失函数为交叉熵损失函数。\n3. 实验 3.1 实现 人脸分辨率为$320\\times 320$，不使用数据增强。使用FcaNet34和ResNet34作为骨干网络，将FAB模块插入到主干网络的后三个块中。\n3.2 性能比较 FF++上的测试结果：\nCeleb-DF上的泛化性结果\n3.3 消融实验 先验注意力模块\nFAB模块\n3.4 可视化 先验注意力图相较于MHSA提供了更具判别性的线索。\n","permalink":"https://Achilles-10.github.io/posts/paper/prior/","summary":"[paper] 1. Introduction 如下图所示，真实图像的边缘图保留了丰富的纹理信息，在伪造图像的边缘途中丢失了一些细节，此外，噪声图展示噪声在真实图像的面部特征和边界上均匀分布，在伪造图像中集中于边界。 本文主要贡献如下： 提出prior-attention机制，利用先验信息构建注意力图，引入边缘和噪声信息，有","title":"Focus by Prior: Deepfake Detection Based on Prior-Attention"},{"content":"[paper]\n1. Introduction 利用基于残差的特征能够突出细微的伪造痕迹并且抑制无关的图像内容。下图展示了从图像中提取伪造痕迹的理想流程，在原始图像中减去图像内容得到篡改痕迹。本文将通过这种方法提取的残差称为“引导残差”。\n残差特征能够提高对高质量图像的检测性能，但对低质量图像的提升帮助甚微，因为低质量图像存在的压缩和resize等操作会对伪造痕迹产生干扰，故还需要RGB的空域信息来提供更多的信息。本文主要贡献如下：\n提出了细粒度的伪造痕迹提取器MTE(Manipulation Trace Extractor)来提取引导残差，克服了基于预测残差可能存在的偏差问题； 设计了有效的注意力融合机制AFM(Attention Fusion Mechanism)来进行特征融合，自适应地分配双流网络的空间特征和特征权重。同时，还利用通道注意力模块来建立伪造痕迹之间的依赖关系。 提出了双流模型AdapGRnet(Adaptive Fusion Based Guided Residuals Network)，通过将MTE和AFM与Backbone结合，在现实场景下进行伪造检测。AdapGRnet能够学习空域和残差域特征，同时检测高质量和低质量的伪造图像。在四个数据集上达到了SOTA的准确率和鲁棒性。 2. Method 2.1 Overview 补充性分析：\n通常情况下，空域信息既包含有适用于伪造检测任务的伪造痕迹，也包含适用于分类和识别任务的图像内容信息。残差特征可以抑制图像内容信息，但同时也可能造成伪造痕迹的部分丢失，特别是当图像质量低时。\n因此，利用空域信息和残差特征的互补性来进行伪造检测。\n双流网络架构：\n通过双流相互增强的方式学习空域和残差域特征，下图为双流模型的框架图。首先用MTE提取RGB图像的引导残差，然后将RGB图像和引导残差输入到主干网络(ResNet-18移除全连接层)中进行特征学习。最后通过AFM融合双流学习到的空域和残差特征。\n2.2 Manipulation Trace Extractor 引导滤波器是一个边缘保留平滑算子，它保留图像内容并过滤到平坦区域的篡改痕迹。篡改痕迹可通过$R_{gr}=|p-q|$得到，其中$p$为输入图像，$q$为滤波器输出，如下图所示。\n下图展示了高质量人脸和低质量人脸通过高通滤波器和MTE得到的残差结果。第五行是用噪声分析对引导残差进一步放大。可以观察到，在高质量图像中，残差具有丰富的细节，并且不同篡改图像的残差之间存在明显差异。对于低质量图像，从第五行噪声分析中可以看到，在残差图像中存在白色块状纹理，难以区分。\n2.3 Attention Fusion Mechanism 利用注意力机制融合双流的特征，空域流特征适用于从低质量的图像中学习特征，残差流适用于从高质量的图像中学习特征，设计的AFM模块如下图所示。\n令得到的空域特征$f_{rgb}\\in\\mathbb{R}^{C\\times H\\times W}$，将其reshape得到$f_{rgb}\\in\\mathbb{R}^{C\\times N_p}$，其中$N_p=H\\times W$。经过矩阵乘法$f_{rgb}\\cdot f^T_{rgb}=M_{rgb}\\in\\mathbb{R}^{C\\times C}$，经过softmax得到注意力图$M_{rgb}$。同理得到残差域注意力图$M_{gr}$。再分别与reshape后的特征图相乘，得到新的特征图${f_{rgb},f_{gr}}$。\n根据交叉熵损失$L_1$和$L_2$的softmax输出来分配双流的权重$\\alpha$，$F_{attention}=\\alpha_1f_{rgb}+\\alpha_2f_{gr}$.\n3. Experiments 使用的数据集为Hybrid Fake Face (HFF)，后处理包括JPEG压缩(压缩率为60，JP60)和模糊操作(5x5的均值滤波，ME5)。\n3.1 消融实验 MTE与AFM模块消融实验\n不同残差提取方法消融实验\n特征融合模块的比较\n","permalink":"https://Achilles-10.github.io/posts/paper/resduals/","summary":"[paper] 1. Introduction 利用基于残差的特征能够突出细微的伪造痕迹并且抑制无关的图像内容。下图展示了从图像中提取伪造痕迹的理想流程，在原始图像中减去图像内容得到篡改痕迹。本文将通过这种方法提取的残差称为“引导残差”。 残差特征能够提高对高质量图像的检测性能，但对低质量图像的提升帮助甚微，因为低质量图像","title":"Exposing Deepfake Face Forgeries with Guided Residuals"},{"content":"einsum Einsum是爱因斯坦求和约定(Einstein summation convention)，提供一种间接的方式来计算多维线性代数数组运算，可以在计算表达式中省去求和符号。\n函数原型 numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe', optimize=False)\n参数：\nsubscripts: str: 将求和的下标指定为逗号分隔的下标标签列表 operands: list of array_like: 操作的数组 out: ndarray, optional: 输出数组，可选 dtype: {data-type, None}, optional: 使用指定的数据类型计算，默认为None order: {'C','F','A','K'}, optional: 输出的内存布局，默认为\u0026rsquo;K' casting: {'no','equiv','safe','same_kind','unsafe'}, optional: 控制可能发生的数据转换类型，默认为\u0026rsquo;safe' optimize: {False,True,'greedy','optimal'}, optional: 控制是否进行中间过程优化，默认为False 返回值：\noutput: ndarray: 计算结果 详解einsum表达式 写出数学表达式 对于以下einsum表达式：\nA = np.arange(2*3*4).reshape(2,3,4) C = einsum(\u0026#39;ijk-\u0026gt;jik\u0026#39;, A) 该表达式的数学表达式为： $$ C_{jk}=A_{ijk} $$\n补充求和符号$\\sum$ 求和符号的下标为数学表达式右边的下标剪左边的下标，在这个例子中，求和下标为i。 $$ C_{jk}=\\sum_i{A_{ijk}} $$\n用for循环复现 求和的部分用+=即可\ni,j,k=A.shape[0],A.shape[1],A.shape[2] C_ = np.zeros((j,k)) for i_ in range(i): for j_ in range(j): for k_ in range(k): C_[j_,k_] += A[i_,j_,k_] C,C_ (array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]), array([[12., 14., 16., 18.], [20., 22., 24., 26.], [28., 30., 32., 34.]])) 特殊写法补充 若等号右边是一个标量，则-\u0026gt;右边可以什么都不写： $$ b=\\sum_{ijk}{A_{ijk}} $$\n\u0026gt;\u0026gt;\u0026gt; A = np.arange(1*2*3).reshape(1,2,3) \u0026gt;\u0026gt;\u0026gt; b = einsum(\u0026#39;ijk-\u0026gt;\u0026#39;, A) \u0026gt;\u0026gt;\u0026gt; b 15 举例 例如矩阵$A=\\begin{bmatrix}0\u0026amp; 1\u0026amp; 2\\\\3\u0026amp; 4\u0026amp; 5\\\\6\u0026amp; 7\u0026amp; 8\\end{bmatrix}$,$B=\\begin{bmatrix}0\u0026amp; 1\u0026amp; 2\\\\3\u0026amp; 4\u0026amp; 5\\end{bmatrix}$，\n求矩阵的迹(trace)：与np.trace等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ii\u0026#39;, a) # einsum(a, [0,0]) 12 einsum(a, [0,0])中的[0,0]与'ii'对应\n提取矩阵对角线元素：与np.diag等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ii-\u0026gt;i\u0026#39;, a) # einsum(a, [0,0], [0]) array([0, 4, 8]) \u0026lsquo;ii-\u0026gt;i\u0026rsquo;表示$b_i=\\sum_{i}{A_{ii}}$\n在某一轴上求和：与np.sum(a, axis)等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ij-\u0026gt;i\u0026#39;, a) # einsum(a, [0,1], [0]), np.sum(a, axis=1) array([ 3, 12, 21]) \u0026lsquo;ij-\u0026gt;i\u0026rsquo;表示$b_i=\\sum_{j}{A_{ij}}$，即行求和\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ij-\u0026gt;j\u0026#39;, a) # einsum(a, [0,1], [1]), np.sum(a, axis=0) array([ 9, 12, 15]) \u0026lsquo;ij-\u0026gt;j\u0026rsquo;表示$b_j=\\sum_{i}{A_{ij}}$，即列求和\n高维数组对单一轴求和可以结合省略号(ellipsis, \u0026hellip;)完成\n\u0026gt;\u0026gt;\u0026gt; np.einsum(\u0026#39;...j-\u0026gt;...\u0026#39;,a) # einsum(a, [Ellipsis,1], [Ellipsis]) array([ 3, 12, 21]) \u0026lsquo;\u0026hellip;j-\u0026gt;\u0026hellip;\u0026lsquo;与\u0026rsquo;ij-\u0026gt;i\u0026rsquo;等价\n矩阵转置或调整矩阵轴顺序：与np.transpose等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ji\u0026#39;, c) # einsum(\u0026#39;ij-\u0026gt;ji\u0026#39;, c), einsum(c, [1,0]) array([[0, 3], [1, 4], [2, 5]]) $B_{ij}=A_{ji}$\n向量内积(结果为标量)：与np.inner等价\n\u0026gt;\u0026gt;\u0026gt; np.einsum(\u0026#39;i,i\u0026#39;,b,b) # einsum(b,[0],b,[0]) 5 向量外积：与np.outer等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;i,j\u0026#39;, b, b) # einsum(b,[0],b,[1]) array([[0, 0, 0], [0, 1, 2], [0, 2, 4]]) 矩阵向量相乘：与np.dot等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ij,j\u0026#39;,a,b) # einsum(a, [0,1], b, [1]),einsum(\u0026#39;...j,j\u0026#39;, a, b),np.dot(a,b) array([ 5, 14, 23]) \u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ij,i\u0026#39;,a,b) # einsum(a, [0,1], b, [0]),einsum(\u0026#39;i...,i\u0026#39;, a, b),np.dot(b,a) array([15, 18, 21]) \u0026lsquo;ij,j\u0026rsquo;表示$c_i=\\sum_{j}{A_{ij}\\cdot b_j}$\n标量乘法：与np.multiply等价\n\u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;..., ...\u0026#39;, 3, c) # einsum(\u0026#39;,ij\u0026#39;, 3, c) array([[ 0, 3 , 6 ], [ 9, 12, 15]]) 张量收缩：与np.tensordot等价\n\u0026gt;\u0026gt;\u0026gt; a = np.arange(60.).reshape(3,4,5) \u0026gt;\u0026gt;\u0026gt; b = np.arange(24.).reshape(4,3,2) \u0026gt;\u0026gt;\u0026gt; einsum(\u0026#39;ijk,jil-\u0026gt;kl\u0026#39;, a, b) # einsum(a, [0,1,2], b, [1,0,3], [2,3]),np.tensordot(a,b, axes=([1,0],[0,1])) array([[4400., 4730.], [4532., 4874.], [4664., 5018.], [4796., 5162.], [4928., 5306.]]) einops rearrange rearrange( tensor: Tensor@rearrange | List[Tensor@rearrange], pattern: str, **axes_lengths: Any ) -\u0026gt; Tensor@rearrange\n基础操作：reordering, composition and decomposition of axes\nfrom einops import rearrange, reduce, repeat import numpy as np ims = np.load(\u0026#39;./test_images.npy\u0026#39;,allow_pickle=False) print(ims.shape,ims.dtype) # (6, 96, 96, 3) float64 ims[0] 转置：\nrearrange(ims[0],\u0026#39;h w c -\u0026gt; w h c\u0026#39;) 拼接轴\nrearrange(ims,\u0026#39;b h w c-\u0026gt; (b h) w c\u0026#39;) # [6, 96, 96, 3] -\u0026gt; [6*96, 96, 3] 分解轴\nrearrange(ims, \u0026#39;(b1 b2) h w c -\u0026gt; (b1 h) (b2 w) c\u0026#39;, b1=2) # 6分解为2*3 将部分高度尺寸移动到宽度：\nrearrange(ims, \u0026#39;b (h1 h2) w c -\u0026gt;h1 (b w h2) c\u0026#39;, h2=2) 轴的顺序：\n当分解轴的时候若顺序不同，则结果也不同，如下例\nrearrange(ims,\u0026#39;b h w c-\u0026gt; h (b w) c\u0026#39;) rearrange(ims, \u0026#39;b h w c -\u0026gt; h (w b) c\u0026#39;) 还可以在合成宽度时重新排序，得到不一样的字母顺序\nrearrange(ims, \u0026#39;(b1 b2) h w c -\u0026gt; h (b1 b2 w) c \u0026#39;, b1=2) # \u0026#39;einops\u0026#39; rearrange(ims, \u0026#39;(b1 b2) h w c -\u0026gt; h (b2 b1 w) c \u0026#39;, b1=2) # \u0026#39;eoipns\u0026#39; einops.reduce reduce( tensor: Tensor@reduce, pattern: str, reduction: Reduction('min', 'max', 'sum', 'mean', 'prod'), **axes_lengths: int ) -\u0026gt; Tensor@reduce\n均值\n单一轴\nreduce(ims,\u0026#39;b h w c -\u0026gt; h w c\u0026#39;, \u0026#39;mean\u0026#39;) # ims.mean(axis=0) 多轴\nreduce(ims,\u0026#39;b h w c -\u0026gt; h w\u0026#39;, \u0026#39;min\u0026#39;) # ims.min(axis=(0,3)) 池化\n以2x2最大池化为例\nreduce(ims, \u0026#39;b (h h2) (w w2) c -\u0026gt; h (b w) c\u0026#39;, \u0026#39;max\u0026#39;, h2=2, w2=2) 不按比例resize\nreduce(ims,\u0026#39;b (h 2) (w 3) c -\u0026gt; h (b w) c\u0026#39;,\u0026#39;mean\u0026#39;) Stack and concatenate rearrange可以处理list数据，list的长度是模式字符串里的第一项(\u0026lsquo;b\u0026rsquo;)\n\u0026gt;\u0026gt;\u0026gt; x = list(ims) \u0026gt;\u0026gt;\u0026gt; print(type(x), \u0026#39;with\u0026#39;, len(x), \u0026#39;tensors of shape\u0026#39;, x[0].shape) \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; with 6 tensors of shape (96, 96, 3) \u0026gt;\u0026gt;\u0026gt; rearrange(x, \u0026#39;b h w c -\u0026gt; h w c b\u0026#39;).shape # np.stack(x, axis=3) (96, 96, 3, 6) \u0026gt;\u0026gt;\u0026gt; rearrange(x, \u0026#39;b h w c -\u0026gt; h (b w) c\u0026#39;).shape # np.concatenate(x, axis=1) (96, 576, 3) 轴的添加或删除 用1来创建一个新的维度，用相似的方法也可以去除维度\n\u0026gt;\u0026gt;\u0026gt; x = rearrange(ims, \u0026#39;b h w c -\u0026gt; b 1 h w 1 c\u0026#39;) # np.expand_dims(ims, axis=(1, 4)) \u0026gt;\u0026gt;\u0026gt; print(x.shape) (6, 1, 96, 96, 1, 3) \u0026gt;\u0026gt;\u0026gt; print(rearrange(x, \u0026#39;b 1 h w 1 c -\u0026gt; b h w c\u0026#39;).shape) # np.squeeze(xx, axis=(1, 4)) (6, 96, 96, 3) 可以使用()来当做占位符，它具有单位长度\nx = reduce(ims, \u0026#39;b h w c -\u0026gt; b () () c\u0026#39;, \u0026#39;max\u0026#39;) - ims # np.expand_dims(ims.max(axis=(1, 2)),axis=(1,2))-ims rearrange(x, \u0026#39;b h w c -\u0026gt; h (b w) c\u0026#39;) einops.repeat repeat( tensor: Tensor@repeat, pattern: str, **axes_lengths: Any ) -\u0026gt; Tensor@repeat\n使用Repeat实现元素重复操作。\n\u0026gt;\u0026gt;\u0026gt; repeat(ims[0], \u0026#39;h w c -\u0026gt; h new_axis w c\u0026#39;, new_axis=5).shape # shortcut: repeat(ims[0], \u0026#39;h w c -\u0026gt; h 5 w c\u0026#39;) (96, 5, 96, 3) 沿已有的维度重复\nrepeat(ims[0], \u0026#39;h w c -\u0026gt; h (repeat w) c\u0026#39;, repeat=3) # np.tile(ims[0], (1, 3, 1)) 沿多个已有的维度重复\nrepeat(ims[0], \u0026#39;h w c -\u0026gt; (2 h) (3 w) c\u0026#39;) # np.tile(ims[0], (2, 3, 1)) 模式字符串中维度的顺序仍然很重要，可以通过改变重复次数和宽度的顺序来将每个元素重复多次\nrepeat(ims[0], \u0026#39;h w c -\u0026gt; h (w repeat) c\u0026#39;, repeat=4) # np.repeat(ims[0], 4, axis=1) ","permalink":"https://Achilles-10.github.io/posts/tech/matrix/","summary":"einsum Einsum是爱因斯坦求和约定(Einstein summation convention)，提供一种间接的方式来计算多维线性代数数组运算，可以在计算表达式中省去求和符号。 函数原型 numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe', optimize=False) 参数： subscripts: str: 将求和的下标指定为逗号分隔的下标标签列表 operands: list of array_like: 操作的数组 out: ndarray, optional: 输出数组，可选 dtype: {data-type, None}, optional: 使用指定","title":"einsum与einops"},{"content":"pathlib在功能和易用性上已经超越os库，比如以下这个获取上层目录和上上层目录的例子，pathlib的链式调用比os的嵌套调用更加灵活方便。\nos方法\nimport os # 获取上层目录 os.path.dirname(os.getcwd()) # 获取上上层目录 os.path.dirname(os.path.dirname(os.getcwd())) pathlib方法\nfrom pathlib import Path # 获取上层目录 Path.cwd().parent # 获取上上层目录 Path.cwd().parent.parent glob基础使用 基础语法，glob默认不匹配隐藏文件\n通配符 描述 示例 匹配 不匹配 * 匹配0个或多个字符，包含空串 glob* glob,glob123 glo ? 匹配1个字符 ?lob glob,flob lob [abc] 匹配括号内字符集合中的单个字符 [gf]lob glob,flob lob,hlob [a-z] 匹配括号内字符范围中的单个字符 [a-z]lob alob,zlob lob,1lob [^abc]或[!abc] 匹配不在括号内字符集合中的单个字符 [^cb]at aat at,cat [^a-z]或[!a-z] 匹配不在括号内字符范围中的单个字符 [^a-z]at 1at at,cat 在 bash 命令行中[!abc]需要转义成[\\!abc]\n扩展语法\n通配符 描述 示例 匹配 不匹配 {x,y,...} Brace Expansion，展开花括号内容，支持展开嵌套括号 a.{png,jp{e}g} a.png,a.jpg,a.jpeg ** globstar，匹配所有文件和任意层目录，若**后面紧接着/则只匹配目录，不含隐藏目录 src/** src/a.py,src/b/c.txt,src/b/ src/.hide/ ?(pattern-list) 匹配0次或1次给定模式 *(pattern-list) 匹配0次或多次给定的模式 a.*(txt|py) a., a.txt, a.txtpy a +(pattern-list) 匹配1次或多次给定的模式 @(pattern-list) 匹配给定模式 !(pattern-list) 匹配非给定的模式 pattern-list是一组以|为分隔符的模式集合\npathlib基础使用 Path为pathlib的主类，首先导入主类：\nfrom pathlib import Path 列出子目录：\n\u0026gt;\u0026gt;\u0026gt; p=Path(\u0026#39;./Research/\u0026#39;) \u0026gt;\u0026gt;\u0026gt; [x for x in p.iterdir() if x.is_dir()] [PosixPath(\u0026#39;Research/Two_Stream\u0026#39;), PosixPath(\u0026#39;Research/pytorch_wavelets\u0026#39;), PosixPath(\u0026#39;Research/FDFL\u0026#39;)] 列出当前目录树下所有.py文件\n\u0026gt;\u0026gt;\u0026gt; p=Path(\u0026#39;./DFDC/\u0026#39;) \u0026gt;\u0026gt;\u0026gt; list(p.glob(\u0026#39;**/*.py\u0026#39;)) [PosixPath(\u0026#39;DFDC/processing/__init__.py\u0026#39;), PosixPath(\u0026#39;DFDC/processing/crop_face.py\u0026#39;), PosixPath(\u0026#39;DFDC/processing/make_dataset.py\u0026#39;)] 在目录树中移动，用'/'进行路径拼接\n\u0026gt;\u0026gt;\u0026gt; p=Path(\u0026#39;.\u0026#39;) \u0026gt;\u0026gt;\u0026gt; q = p/\u0026#39;DFDC\u0026#39;/\u0026#39;processing\u0026#39; \u0026gt;\u0026gt;\u0026gt; q PosixPath(\u0026#39;DFDC/processing\u0026#39;) 使用joinpath()。\n\u0026gt;\u0026gt;\u0026gt; p=Path.cwd() \u0026gt;\u0026gt;\u0026gt; p.joinpath(\u0026#39;pathlib\u0026#39;) PosixPath(\u0026#39;/media/sda/zhy/pathlib\u0026#39;) 按照分隔符将文件路径分割\n\u0026gt;\u0026gt;\u0026gt; q.parts (\u0026#39;DFDC\u0026#39;, \u0026#39;processing\u0026#39;) 查询路径属性\n\u0026gt;\u0026gt;\u0026gt; q.exists() True \u0026gt;\u0026gt;\u0026gt; q.is_dir() Trueq \u0026gt;\u0026gt;\u0026gt; q.is_file() False 创建目录和文件\np = Path(\u0026#39;./pathlib/\u0026#39;) # parents默认为False，若父目录不存在抛出异常 # exist_ok默认为False，若目录已存在抛出异常 p.mkdir(parents=True, exist_ok=True) p = Path(\u0026#39;./pathlib/test.txt\u0026#39;) # touch创建文件，父目录必须存在否则抛出异常 p.touch(exist_ok=True) 获取文件/目录信息\n\u0026gt;\u0026gt;\u0026gt; p = Path(\u0026#39;DFDC/processing/__init__.py\u0026#39;) # 获取文件/目录名 \u0026gt;\u0026gt;\u0026gt; p.name \u0026#39;__init__.py\u0026#39; # 获取不包含后缀的文件名 \u0026gt;\u0026gt;\u0026gt; p.stem \u0026#39;__init__\u0026#39; # 获取文件后缀名 \u0026gt;\u0026gt;\u0026gt; p.suffix \u0026#39;.py\u0026#39; \u0026gt;\u0026gt;\u0026gt; p = Path.cwd() # 获取上层目录路径 \u0026gt;\u0026gt;\u0026gt; p.parent PosixPath(\u0026#39;/media/sda\u0026#39;) # 获取所有上层目录路径 \u0026gt;\u0026gt;\u0026gt; [path for path in p.parents] [PosixPath(\u0026#39;/media/sda\u0026#39;), PosixPath(\u0026#39;/media\u0026#39;), PosixPath(\u0026#39;/\u0026#39;)] # 获取文件/目录属性 \u0026gt;\u0026gt;\u0026gt; p.stat() os.stat_result(st_mode=16895, st_ino=171704321, st_dev=2048, st_nlink=16, st_uid=1018, st_gid=1019, st_size=4096, st_atime=1684207391, st_mtime=1684207367, st_ctime=1684207367) 重命名/移动文件\n重命名文件时，当新命名的文件重复时，会抛出异常。\n\u0026gt;\u0026gt;\u0026gt; p = Path(\u0026#39;pathlib/test.txt\u0026#39;) # 重命名 \u0026gt;\u0026gt;\u0026gt; new_name = p.with_name(\u0026#39;test_new.txt\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p.rename(new_name) PosixPath(\u0026#39;pathlib/test_new.txt\u0026#39;) # 修改后缀 \u0026gt;\u0026gt;\u0026gt; new_suffix = new_name.with_suffix(\u0026#39;.json\u0026#39;) \u0026gt;\u0026gt;\u0026gt; new_name.rename(new_suffix) PosixPath(\u0026#39;pathlib/test_new.json\u0026#39;) 移动文件，当新路径下文件已存在时，无法创建。\n\u0026gt;\u0026gt;\u0026gt; p = Path(\u0026#39;pathlib/test_new.json\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p.rename(\u0026#39;test.json\u0026#39;) PosixPath(\u0026#39;test.json\u0026#39;) replace()与rename()用法基本相同，但是当新命名的文件重复时，replace()不会抛出异常而是直接覆盖旧文件。\n删除文件/目录\n删除文件，missing_ok=True设置文件不存在时不会抛出异常。\n\u0026gt;\u0026gt;\u0026gt; p = Path(\u0026#39;test.json\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p.unlink(missing_ok=True) 删除目录，目录必须为空，否则抛出异常。\n\u0026gt;\u0026gt;\u0026gt; p=Path(\u0026#39;pathlib\u0026#39;) \u0026gt;\u0026gt;\u0026gt; p.rmdir() ","permalink":"https://Achilles-10.github.io/posts/tech/pathlib/","summary":"pathlib在功能和易用性上已经超越os库，比如以下这个获取上层目录和上上层目录的例子，pathlib的链式调用比os的嵌套调用更加灵活方便。 os方法 import os # 获取上层目录 os.path.dirname(os.getcwd()) # 获取上上层目录 os.path.dirname(os.path.dirname(os.getcwd())) pathlib方法 from pathlib import Path # 获取上层目录 Path.cwd().parent # 获取上上层目录 Path.cwd().parent.parent glob基础使用 基础语法，gl","title":"使用pathlib优雅操作路径"},{"content":"[paper] [code]\n1. 引言 参考NLP在大规模预训练模型上的突破，在CV中提取通用的视觉特征，可以是用于分类任务的图像级别特征，也可以是用于分割任务的像素级别特征。利用自监督的方法，在不同源、足够多的数据上训练即可生成这样的特征。\n本文工作探索了自监督学习是否在大规模数据上预训练有潜力学习通用的视觉特征。本文大部分技术贡献都是专为稳定和加速判别性自监督学习定制。\n对于预训练数据，本文构建了一个自动管道，从大量未处理的图像集合中筛选和平衡数据集。在本文工作中，使用一种朴素的聚类方法解决了在处理wild数据时重新平衡概念并避免在少数主导模式上的过拟合问题，并构建了一个小型但多样的142M图像的语料库来验证本文方法。\n本文提出了用不同ViT框架训练的DINOv2模型，下图展示了DINOv2在多种图像级和像素级CV任务中的性能（深蓝色为DINOv2，浅橙色是自监督方法，深粉色为弱监督方法），验证了与最好的开源的弱监督模型相比，自监督训练是学习可迁移冻结特征的一个很好的候选方法。\n2. 数据处理(Data Processing) 数据处理pipeline如下图所示：\n数据来源(Data selection)\n构建的LVD-142M数据集的所用的数据集如下表所示，该集合旨在为图像级和密集识别提供涵盖各种下游视觉任务的图像。总共有1.2B图像。\n图像相似度(Image similarity)\n使用余弦相似度(cosine similarity)将图像特征与下面的相似度函数m比较： $$ m(s,r)=\\text{cos-similarity}(f(s),f(r))=\\frac{f(s)\\cdot f(r)}{||f(s)||_2||f(r)||_2} $$ 其中s和r是一对比较的图像，f是模型生成的特征。\n去重(Deduplication)\n将[A self-supervised descriptor for image copy detection]的拷贝检测流程应用到未处理的数据中去除重复图像，减少冗余增加图像多样性。\nSelf-deduplication：检索每幅图像的k=64最近邻(余弦相似度)，只考虑相似度\u0026gt;0.6的邻居，通过可扩展的不相交集数据结构实现来提取关联k-NN图的连通分支，对重复图像的每个分量只保留一个代表性图像。自去重的结果有1.1B图像。 Relative deduplication：丢弃上一步骤中与评估数据集的训练和测试划分中相似的图像，采用与自去重中相似的步骤，丢弃相似度\u0026gt;0.45的所有重复图像。剩下744M数据。 Retrieval：检索相似图像来构建数据集。首先使用在ImageNet-22k预训练的ViT-H/16网络来计算Image Embedding，并使用余弦相似度来作为图像之间的距离度量。然后对未处理的数据进行k-means聚类。给定一个用于检索的查询数据集，如果它足够大，为每个查询图像检索N个(4)最近邻。如果较小，则从每个查询图像对应的簇中采样M张图像。 3. 判别性自监督预训练(Discriminative Self-supervised Pre-training) 使用一种判别性的自监督方法来学习特征，该方法可以看作是以SwAV为中心的DINO和iBOT损失的组合\nImage-level objective：考虑从学生和教师网络中提取特征之间的交叉熵损失。这两个特征来自ViT的class token，由同一张图的不同crop得到。学习学生网络的参数，通过指数异动平均(EMA)来构建教师网络。 Patch-level objective：随机mask学生网络输入图像的一些patch，然后在两个网络的每个掩码快上添加交叉熵损失，与图像级的损失结合。 Untying head weights between both objectives：将两个目标相关的权重绑定在一起会使得模型在patch-level上欠拟合，image-level上过拟合。通过解绑这些权重提高了在两个尺度上的性能。 Sinkhorn-Knopp centering：使用Sinkhorn-Knopp(SK)批归一化替代DINO和iBot的教师softmax-centering步骤。运行SK算法进行3轮迭代；对于学生，使用softmax归一化。 KoLeo regularizer：KoLeo 正则项来自于 Kozachenko-Leonenko differential entropy estimator.给定一个含有n向量的集合$(x_i,\\dots,x_n)$，$L_{\\text{koleo}}=-\\frac{1}{n}\\sum_{i=1}^{n}{\\log(d_{n,i})}$ ，其中 $d_{n,i}=\\min_{j\\neq i}||x_i-x_j||$是$x_i$与batch内其他点的最小距离。在计算koLeo正则项前还对特征进行L2正则化。 Adapting the resolution：高分辨率是分割或检测等像素级下游任务的关键，因为小物体在低分辨率下消失。然而，在高分辨率下进行训练需要更长的时间和更大内存。相反，在预训练结束的短时间内将图像的分辨率提高到518 × 518。Fixing the train-test resolution discrepancy 4. 高效实现(Efficient implementation) 相较于iBOT，DINOv2运行速度快2倍，使用1/3的内存。\nFast and memory-efficient attention：实现自己版本的FlashAttention以提高自注意力层的效率。由于GPU硬件的特性，当每个头(head)的嵌入维度为64倍数时效率最高，当全嵌入维度为256倍数时矩阵运算更高效。因此本文的ViT-g架构使用embedding dimension = 1536(24 heads, 64 dim/head)，而非embedding dimension = 1408(16 heads, 88 dim/head)。本文的ViT-g有1.1B参数。 Nested tensors in self-attention Efficient stochastic depth：本文实现了随机深度的一个高效版本，它跳过了丢弃残差计算而不是掩盖结果，以近似丢弃率的比例节省内存和计算量。本文丢弃率d=40%，显著提高计算效率和内存使用率。该实现在批维度上随机重排B个样本，并对前$(1-d)\\times B$个样本分块计算。 Fully-Sharded Data Parallel (FSDP)：使用AdamW优化器，对于ViT-g将使用16G内存。FSDP节省跨GPU的通信开销。 模型蒸馏(Model Distillation) 5. 消融研究(Ablation Studies) 设置一系列消融研究来验证本文pipeline中不同组件：技术修改、预训练数据和模型蒸馏。\n5.1 Improved Training Recipe 本文方法在iBOT基础上进行改进。本文通过在一个baseline iBOT模型中依次添加各个组件，训练了多个模型，结果如下图所示。几乎每个组件都能带来性能的提升，只有Layer Scale和Stochastic Depth在linear中降低了性能，但它们提高了训练的稳定性。\n5.2 Pretraining Data Source 预训练数据的质量直接影响到特征的质量，本实验对比LVD-142M，ImageNet-22k和未处理的原始数据。结果如下图所示。可见，在LVD-142M上预训练能够在ImageNet-1k上取得最好性能，同时在其他测试集也能取得较好的性能。于是可以得出LVD-142M数据集提供了不同类型的平衡的数据，能带来性能的提升。\n5.3 Model Size and Data 模型大小与数据量大小的重要性实验结果如下图所示。\n5.4 Loss Components 验证KoLeo Loss和masked image modeling(MIM)的影响，结果如下图所示：\n5.5 Impact of Knowledge Distillation 验证模型蒸馏的有效性，比较ViT-L/14从头训练和从ViT-g/14蒸馏的性能，结果如下图所示。可见，蒸馏得到的模型性能更高，甚至在有的benchmark上超过了教师模型。\n5.6 Impact of Resolution 衡量在预训练过程中改变分辨率对图像级和patch级特征的影响，结果如下图所示。可见，在训练结尾使用高分辨率训练10k次迭代，在增加很少计算量的同时带来和高分辨率训练几乎一样好的性能。\n6. 结果(Results) Baseline. ImageNet-1k top-1 ACC. 在其他评估中报告SSL(自监督)模型中最好的四个，以及弱监督中最好的OpenCLIP-G模型。\n与开源的SOTA自监督模型比较：MAE, DINO, SEERv2, MSN, EsViT, Mugs, iBOT.\n弱监督模型：CLIP, OpenCLIP, SWAG.\n6.1 ImageNet Classification 冻结特征层，仅训练一个线性分类器。\n能否微调编码器(Can we finetune the encoders)？\n下图是微调后的实验结果，取得了明显的性能提升，因此微调是可选的策略。\n鲁棒性分析(Robustness analysis)\n下图是泛化性(鲁棒性)的测试结果，相较于SSL模型，本文方法取得了明显更好的鲁棒性；相较于弱监督模型，仅在Im-R和Sketch上稍微落后。\n6.2 Additional Image and Video classification Benchmarks 6.3 Instance Recognition 6.4 Dense Recognition Tasks 语义分割(Semantic segmentation)\n深度估计(Depth estimation)\n6.5 定性结果(Qualitative Results) 语义分割和深度估计(Semantic Segmentation and Depth Estimation)\n分布外的泛化性(Out-of-distribution generalization)\n分布外数据的分割和深度估计例子如下图所示，展现了在不同特征域中良好的迁移性。\nPCA of patch features\n块匹配(Patch matching)\n7. Fairness and Bias Analysis 7.1 Geographical Fairness 7.2 Gender, Skintones and Age 8. Estimating the Environmental Impact of Training our Models ","permalink":"https://Achilles-10.github.io/posts/paper/dinov2/","summary":"[paper] [code] 1. 引言 参考NLP在大规模预训练模型上的突破，在CV中提取通用的视觉特征，可以是用于分类任务的图像级别特征，也可以是用于分割任务的像素级别特征。利用自监督的方法，在不同源、足够多的数据上训练即可生成这样的特征。 本文工作探索了自监督学习是否在大规模数据上预训练有潜力学习通用的视觉","title":"DINOv2: Learning Robust Visual Features without Supervision"},{"content":"46. 剑指 Offer 51. 数组中的逆序对(困难) 归并排序：\n归并排序的合并阶段，每当遇到左子数组元素\u0026gt;右子数组元素时，表示左子数组当前元素到末尾元素与右子数组当前元素构成若干逆序对。\n在归并时用一个tmp辅助数组来暂存nums[i:j]内的元素。\nclass Solution: def reversePairs(self, nums: List[int]) -\u0026gt; int: def merge(l,r): if l\u0026gt;=r: return 0 m=l+(r-l)//2 res = merge(l,m)+merge(m+1,r) tmp[l:r+1]=nums[l:r+1] i,j=l,m+1 for k in range(l,r+1): if i==m+1: nums[k]=tmp[j] j+=1 elif j==r+1 or tmp[i]\u0026lt;=tmp[j]: nums[k]=tmp[i] i+=1 else: nums[k]=tmp[j] res+= m-i+1 j+=1 return res tmp=[0]*len(nums) return merge(0,len(nums)-1) 47. 剑指 Offer 52. 两个链表的第一个公共节点(简单) 双指针：当指针遍历到链表末尾时，转移到另一链表表头，直到p1=p2。\nclass Solution: def getIntersectionNode(self, headA: ListNode, headB: ListNode) -\u0026gt; ListNode: A, B = headA, headB while A != B: A = A.next if A else headB B = B.next if B else headA return A 48. 剑指 Offer 53 - I. 在排序数组中查找数字 I(简单) 二分法：\n分别找到目标数字左边和右边的第一个元素索引left和right，答案为right-left-1。\nhelper函数旨在查找数字tar在nums中的插入位置，若存在值相同的元素，则插入右边。\nclass Solution: def search(self, nums: [int], target: int) -\u0026gt; int: def helper(tar): i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] \u0026lt;= tar: i = m + 1 else: j = m - 1 return i return helper(target) - helper(target - 1) 49. 剑指 Offer 53 - II. 0～n-1中缺失的数字(简单) 二分查找：\n数组可以划分为左子数组：nums[i]=i和右子数组：nums[i]!=i。缺失数字等于右子数组的首位元素索引。\nclass Solution: def missingNumber(self, nums: List[int]) -\u0026gt; int: i, j = 0, len(nums) - 1 while i \u0026lt;= j: m = (i + j) // 2 if nums[m] == m: i = m + 1 else: j = m - 1 return i 50. 剑指 Offer 54. 二叉搜索树的第k大节点(简单) 中序遍历：\n二叉搜索树的中序遍历的倒序为递减序列。\nclass Solution: def kthLargest(self, root: TreeNode, k: int) -\u0026gt; int: def dfs(root): if not root: return dfs(root.right) if self.k == 0: return self.k -= 1 if self.k == 0: self.res = root.val dfs(root.left) self.k = k dfs(root) return self.res 51. 剑指 Offer 55 - I. 二叉树的深度(简单) 层序遍历：\n每遍历一层深度+1\nclass Solution: def maxDepth(self, root: TreeNode) -\u0026gt; int: if not root: return 0 queue, res = [root], 0 while queue: tmp = [] for node in queue: if node.left: tmp.append(node.left) if node.right: tmp.append(node.right) queue = tmp res += 1 return res DFS：\n树的深度=max(左深度，右深度)+1\nclass Solution: def maxDepth(self, root: TreeNode) -\u0026gt; int: if not root: return 0 return max(self.maxDepth(root.left), self.maxDepth(root.right)) + 1 52. 剑指 Offer 55 - II. 平衡二叉树(简单) 树的深度=max(左深度，右深度)+1\n后序遍历+剪枝（自底向上）：当某子树不是平衡树时直接剪枝\nclass Solution: def isBalanced(self, root: TreeNode) -\u0026gt; bool: def recur(root): if not root: return 0 left = recur(root.left) if left == -1: return -1 right = recur(root.right) if right == -1: return -1 return max(left, right) + 1 if abs(left - right) \u0026lt;= 1 else -1 return recur(root) != -1 先序遍历+判断深度（自顶向下）：当所有子树都是平衡树则是平衡树，产生很多重复计算\nclass Solution: def isBalanced(self, root: TreeNode) -\u0026gt; bool: if not root: return True return abs(self.depth(root.left) - self.depth(root.right)) \u0026lt;= 1 and \\ self.isBalanced(root.left) and self.isBalanced(root.right) def depth(self, root): if not root: return 0 return max(self.depth(root.left), self.depth(root.right)) + 1 53. 剑指 Offer 56 - I. 数组中数字出现的次数(中等) 分组异或：\n对所有数字进行一次异或，得到两个出现一次的数字的异或值。\n在异或结果中找到任意为1的位。\n根据这一位对所有的数字进行分组。在每个组内进行异或操作，得到两个数字。\nclass Solution: def singleNumbers(self, nums: List[int]) -\u0026gt; List[int]: x,y,n,m=0,0,0,1 for num in nums: n^=num while n\u0026amp;m==0: m\u0026lt;\u0026lt;=1 for num in nums: if num\u0026amp;m: x^=num else: y^=num return x,y 54. 剑指 Offer 56 - II. 数组中数字出现的次数 II(中等) 遍历统计：\nclass Solution: def singleNumber(self, nums: List[int]) -\u0026gt; int: counts = [0] * 32 for num in nums: for j in range(32): counts[j] += num \u0026amp; 1 num \u0026gt;\u0026gt;= 1 res, m = 0, 3 for i in range(32): res \u0026lt;\u0026lt;= 1 res |= counts[31 - i] % m return res if counts[31] % m == 0 else ~(res ^ 0xffffffff) 有限状态自动机：\n考虑数字的二进制形式，对于出现三次的数字，各二进制位出现的次数都是3的倍数。统计所有数字的各二进制位中1的出现次数，并对3求余，结果则为只出现一次的数字。\n计算one的方法：one = one^n \u0026amp; ~two\n利用计算后的one计算two：two = two^n \u0026amp; ~one\nclass Solution: def singleNumber(self, nums: List[int]) -\u0026gt; int: ones, twos = 0, 0 for num in nums: ones = ones ^ num \u0026amp; ~twos twos = twos ^ num \u0026amp; ~ones return ones 55. 剑指 Offer 57. 和为s的两个数字(简单) 双指针：\nclass Solution: def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: i, j = 0, len(nums) - 1 while i \u0026lt; j: s = nums[i] + nums[j] if s \u0026gt; target: j -= 1 elif s \u0026lt; target: i += 1 else: return nums[i], nums[j] return [] 56. 剑指 Offer 57 - II. 和为s的连续正数序列(简单) ","permalink":"https://Achilles-10.github.io/posts/algo/offer3/","summary":"46. 剑指 Offer 51. 数组中的逆序对(困难) 归并排序： 归并排序的合并阶段，每当遇到左子数组元素\u0026gt;右子数组元素时，表示左子数组当前元素到末尾元素与右子数组当前元素构成若干逆序对。 在归并时用一个tmp辅助数组来暂存nums[i:j]内的元素。 class Solution: def reversePairs(self, nums: List[int]) -\u0026gt; int: def merge(l,r): if l\u0026gt;=r: return 0 m=l+(r-l)//2 res = merge(l,m)+merge(m+1,r) tmp[l:r+1]=nums[l:r+1] i,j=l,m+1 for k in range(l,r+1):","title":"剑指offer复习笔记(3)"},{"content":"[paper]\n1. 引言 由于绝大多数伪造视频都是以逐帧伪造的方式生成的，这样独立生成的每张人脸在时序上不可避免地导致闪烁和不连续，如下图所示。所以可以利用时序相关性来实现更泛化和鲁棒的人脸伪造视频检测。\n人脸伪造视频主要有两类伪影，一类是空间相关的融合边界(blending boundary)、棋盘格和模糊伪影等，一类是时序相关性。但通常方法都侧重于检测空间相关伪影，为了学习到更多的时序不一致性信息，本文提出了FTCN（fully temporal convolution network），关键思想是限制模型学习空间相关的伪影。另外，发现一些不相邻的帧之间也会出现不一致性，故提出用Transformer来解决时间维度上的长依赖关系。\n本文的方法可以从头开始训练而不需要预训练模型，并且可以在没有人工标注的情况下定位和可视化伪造视频中的时序不一致性。\n本文主要贡献总结如下：\n探索充分利用时间相关性进行人脸伪造检测，并提出了一种结合全时序卷积网络(FTCN)和Transformer的框架来显式检测时间不一致性。 本文的检测器可以定位和可视化人脸伪造的时间不连续部分。 在各种数据集上的实验证明了本文提出的方法在泛化能力方面的优越性。 2. 方法 为了学习到更多的时序不一致性信息，提出FTCN。具体的，将所有与时间相关的卷积核大小不变，但将所有与空间相关的卷积核大小设置为1。用ResNet-50(R50)作为backbone，通过以下实验验证：\n3D R50 2D R50 3D R50 FTCN 通过在FF++数据集上训练，t-SNE可视化结果如下图。分析可知，虽然所有分类器都能区分真假数据，但是假数据的分布完全不同。3D R50和2D R50都会将不同的人脸操纵方法产生的虚假数据分离出来，这表明它们提取的特征包含了每种伪造算法的独特伪影，这会影响泛化能力。相反，3D R50 FTCN分类器的造伪数据更多的混合在一起，这证明了时序网络是通过更一般的时序不一致来学习分类的。\n(d)是FTCN结合Transformer的结果。\n","permalink":"https://Achilles-10.github.io/posts/paper/ftcn/","summary":"[paper] 1. 引言 由于绝大多数伪造视频都是以逐帧伪造的方式生成的，这样独立生成的每张人脸在时序上不可避免地导致闪烁和不连续，如下图所示。所以可以利用时序相关性来实现更泛化和鲁棒的人脸伪造视频检测。 人脸伪造视频主要有两类伪影，一类是空间相关的融合边界(blending boundary)、棋盘","title":"Exploring Temporal Coherence for More General Video Face Forgery Detection"},{"content":"图像阈值 全局简单阈值 对于图像里的每个像素，应用相同的阈值，如果像素值小于阈值，则将其设置为0，否者设置为最大值。使用函数cv2.threshold(src, thresh, maxval, type, dst)，src是原图像，必须是单通道图像(灰度)，type表示阈值类型，所有简单的阈值类型为：\ncv2.THRESH_BINARY：小于阈值置0，大于阈值置为最大值maxval cv2.THRESH_BINARY_INV：与上相反 cv2.THRESH_TRUNC：小于阈值不变，大于阈值置为阈值thresh，即大于阈值的部分显示为白色 cv2.THRESH_TOZERO：小于阈值置0，大于阈值保持原色 cv2.THRESH_TOZERO_INV：与上相反 有两个输出，第一个是使用的阈值，第二个是阈值处理后的图像。\n示例如下，灰度图像黑色为0，白色为255：\nimg = cv2.imread(\u0026#39;face.png\u0026#39;,0) ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV) ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC) ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO) ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV) titles = [\u0026#39;Original Image\u0026#39;,\u0026#39;BINARY\u0026#39;,\u0026#39;BINARY_INV\u0026#39;,\u0026#39;TRUNC\u0026#39;,\u0026#39;TOZERO\u0026#39;,\u0026#39;TOZERO_INV\u0026#39;] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] plt.figure(figsize=(9,6)) for i in range(6): plt.subplot(2,3,i+1),plt.imshow(images[i],\u0026#39;gray\u0026#39;) plt.title(titles[i]) plt.xticks([]),plt.yticks([]) plt.show() 自适应阈值 使用cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C, dst)实现自适应阈。thresholdType有cv2.THRESH_BINARY和cv2.THRESH_BINARY_INV两种，blockSize确定邻域大小，C是减去的常数，adaptiveMethod决定如何计算阈值，有以下两种：\ncv2.ADAPTIVE_THRESH_MEAN_C：局部邻域块均值减去常数C cv2.ADAPTIVE_THRESH_GAUSSIAN_C：局部邻域块高斯加权和减去常数C； 自适应阈值可以根据像素周围的区域来确定像素的阈值；亮度较高的图像区域的二值化阈值通常会较高，而亮度低的图像区域的二值化阈值则会相适应的变小；不同亮度、对比度、纹理的局部图像区域将会拥有相对应的局部二值化阈值。\n示例如下：\nimg = cv2.imread(\u0026#39;sudoku.png\u0026#39;,0) img = cv2.medianBlur(img,5) ret,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) th2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,2) th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2) titles = [\u0026#39;Original\u0026#39;, \u0026#39;Global(v = 127)\u0026#39;,\u0026#39;Mean\u0026#39;, \u0026#39;Gaussian\u0026#39;] images = [img, th1, th2, th3] plt.figure(figsize=(8,8)) for i in range(4): plt.subplot(2,2,i+1),plt.imshow(images[i],\u0026#39;gray\u0026#39;) plt.title(titles[i]) plt.xticks([]),plt.yticks([]) plt.show() Otsu的二值化 在全局简单阈值中，我们使用任意选择的值作为阈值。Otsu的方法可以自动确定阈值，避免了必须选择一个值的情况。考虑仅具有两个不同图像值的图像（双峰图像），其中直方图将仅包含两个峰。一个好的阈值应该在这两个值的中间，Otsu的方法能从图像直方图中确定最佳全局阈值。\n仍然使用cv2.threshold函数，将cv2.THRESH_OTSU作为额外的标志位传递到函数，阈值thresh可任意选择。\n示例如下，三种情况分别是全局阈值(thresh=127)、Otsu阈值，经过高斯滤波去噪后的Otsu阈值：\nimg = cv2.imread(\u0026#39;noisy.jpeg\u0026#39;,0) ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY) ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) blur = cv2.GaussianBlur(img,(5,5),0) ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) # plot all the images and their histograms images = [img, 0, th1, img, 0, th2, blur, 0, th3] titles = [\u0026#39;Original Noisy Image\u0026#39;,\u0026#39;Histogram\u0026#39;,\u0026#39;Global Thresholding (v=127)\u0026#39;, \u0026#39;Original Noisy Image\u0026#39;,\u0026#39;Histogram\u0026#39;,\u0026#34;Otsu\u0026#39;s Thresholding\u0026#34;, \u0026#39;Gaussian filtered Image\u0026#39;,\u0026#39;Histogram\u0026#39;,\u0026#34;Otsu\u0026#39;s Thresholding\u0026#34;] plt.figure(figsize=(9,9)) for i in range(3): plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],\u0026#39;gray\u0026#39;) plt.title(titles[i*3]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256) plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],\u0026#39;gray\u0026#39;) plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([]) plt.show() 图像平滑 2D卷积（图像过滤） 可以使用各类低通滤波器(LPF)和高通滤波器(HPF)对图像进行滤波，LPF有助于消除噪声、使图像模糊等，HPF有助于在图像中寻找边缘信息。使用函数cv2.filter2D(src, ddepth, kernel, dst, anchor, delta, borderType)来将滤波核与图像进行卷积，例如，5x5的均值滤波核如下： $$ K = \\frac{1}{25} \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} $$ 使用7x7的均值滤波示例如下：\nimg = cv.imread(\u0026#39;face.png\u0026#39;) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) kernel = np.ones((7,7),np.float32)/49 dst = cv.filter2D(img,-1,kernel) plt.subplot(121),plt.imshow(img),plt.title(\u0026#39;Original\u0026#39;) plt.xticks([]), plt.yticks([]) plt.subplot(122),plt.imshow(dst),plt.title(\u0026#39;Averaging\u0026#39;) plt.xticks([]), plt.yticks([]) plt.show() 图像模糊（图像平滑） 通过将图像与低通滤波器进行卷积来实现图像模糊，这对于消除噪声很有用。它实际上从图像中消除了噪声和边缘等高频部分。因此此操作会使边缘模糊，OpenCV主要提供四种模糊类型：\n均值\n获取核区域下所有像素的均值，然后替换中心元素。可以通过cv2.blur(src, ksize, dst, anchor, borderType)或cv2.boxFilter(src, ddepth, ksize, dst, anchor, normalize, borderType)完成。示例如下：\nimg = cv2.imread(\u0026#39;face.png\u0026#39;) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) blur = cv2.blur(img,(5,5)) plt.subplot(121),plt.imshow(img),plt.title(\u0026#39;Original\u0026#39;) plt.xticks([]), plt.yticks([]) plt.subplot(122),plt.imshow(blur),plt.title(\u0026#39;Blurred\u0026#39;) plt.xticks([]), plt.yticks([]) plt.show() 若不想使用标准化的滤波器，可以使用cv2.boxFilter()并且设置normalize=False。\n高斯模糊\n使用cv2.GaussianBlur()。需要指定高斯核的宽度和高度，宽度和高度都应该为正奇整数；还需指定X和Y方向的标准差sigmaX和sigmaY，如果仅指定sigmaX，则两者相同，若两者均为零，则根据高斯核大小计算。高斯模糊对于从图像中去除高斯噪声非常有效。\n可通过cv2.getGaussianKernel()创建高斯内核。\n修改上述代码实现高斯模糊：\nblur = cv2.GaussianBlur(img,(5,5),0) 中值模糊\n将中心元素替换为和区域的中值，对于消除椒盐噪声非常有效。内核大小应该为正奇整数。示例如下：\nmedian = cv.medianBlur(img,5) 椒盐噪声就是黑白噪点，椒指黑色噪点，盐指白色噪点。\n双边滤波\n双边滤波cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace, dst, borderType)在去除噪声的同时保持边缘清晰锐利非常有效。但是该操作相较于其他滤波器速度较慢。\nd表示滤波时使用的每个像素邻域的直径 sigmaColor表示色彩空间中滤波的标准差，该值越大，滤波器在保留边缘信息上越弱 sigmaSpace表示坐标空间中滤波的标准差，该值越大，更远的颜色相近的元素会互相影响 双边滤波器在空间中也采用高斯滤波器，但是还有一个高斯滤波器，它是像素差的函数。空间的高斯函数确保仅考虑附近像素的模糊，而强度差的高斯函数确保仅考虑强度与中心像素相似的那些像素的模糊。由于边缘的像素强度变化较大，因此可以保留边缘。\nblur = cv.bilateralFilter(img,9,75,75) ","permalink":"https://Achilles-10.github.io/posts/tech/opencv4/","summary":"图像阈值 全局简单阈值 对于图像里的每个像素，应用相同的阈值，如果像素值小于阈值，则将其设置为0，否者设置为最大值。使用函数cv2.threshold(src, thresh, maxval, type, dst)，src是原图像，必须是单通道图像(灰度)，type表示阈值类型，所有简单的阈值类型为： cv2.THRESH_","title":"OpenCV-Python学习笔记(4)：图像阈值与图像平滑"},{"content":"26. 剑指 Offer 30. 包含min函数的栈(简单) 双栈：一个栈存储数据，一个栈存储最小值\nclass MinStack: def __init__(self): self.stack=[] self.minstack=[inf] def push(self, x: int) -\u0026gt; None: self.stack.append(x) self.minstack.append(min(x,self.minstack[-1])) def pop(self) -\u0026gt; None: self.stack.pop() self.minstack.pop() def top(self) -\u0026gt; int: return self.stack[-1] def min(self) -\u0026gt; int: return self.minstack[-1] 27. 剑指 Offer 31. 栈的压入、弹出序列 模拟：按顺序push，若和popped值相等，则pop，最后判断栈是否为空。\nclass Solution: def validateStackSequences(self, pushed: List[int], popped: List[int]) -\u0026gt; bool: stack,i=[],0 for num in pushed: stack.append(num) while stack and stack[-1]==popped[i]: stack.pop() i+=1 return not stack 28. 剑指 Offer 32 - II. 从上到下打印二叉树 II(简单) BFS\nclass Solution: def levelOrder(self, root: TreeNode) -\u0026gt; List[List[int]]: if not root: return [] ans,queue=[],[root] while queue: tmp=[] for i in range(len(queue)): node=queue.pop(0) tmp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) ans.append(tmp) return ans 29. 剑指 Offer 33. 二叉搜索树的后序遍历序列 递归：找到第一个大于列表末尾(根节点)的位置，即为左右子树划分的点；判断右子树序列中是否所有节点都大于根节点(p==j)\nclass Solution: def verifyPostorder(self, postorder: List[int]) -\u0026gt; bool: def recur(i,j): if i\u0026gt;=j: return True p = i while postorder[p]\u0026lt;postorder[j]: p+=1 m=p while postorder[p]\u0026gt;postorder[j]: p+=1 return p==j and recur(i,m-1) and recur(m,j-1) return recur(0,len(postorder)-1) 单调栈：\n将后序遍历的序列倒序，即为根|右子树|左子树； 此时对于升序arr[i+1]\u0026gt;arr[i]，i+1一定是i的右节点； 对于倒序arr[i+1]\u0026lt;arr[i]，i+1是某节点root的左节点，且root为0~i中大于且最接近i+1的节点。 单调栈存储升序节点 遇到降序节点时，通过出栈来更新root 若arr[i]\u0026gt;root: 返回false；若arr[i]\u0026lt;root，满足定义，继续遍历 class Solution: def verifyPostorder(self, postorder: [int]) -\u0026gt; bool: stack, root = [], float(\u0026#34;+inf\u0026#34;) for i in range(len(postorder) - 1, -1, -1): if postorder[i] \u0026gt; root: return False while(stack and postorder[i] \u0026lt; stack[-1]): root = stack.pop() stack.append(postorder[i]) return True 30. 剑指 Offer 34. 二叉树中和为某一值的路径(中等) 回溯：\nclass Solution: def pathSum(self, root: TreeNode, target: int) -\u0026gt; List[List[int]]: ans,path=[],[] def dfs(node,s): if not node: return s+=node.val path.append(node.val) if s==target and not node.left and not node.right: ans.append(path[:]) dfs(node.left,s) dfs(node.right,s) path.pop() dfs(root,0) return ans 31. 剑指 Offer 35. 复杂链表的复制(中等) 拼接链表：\n第一次遍历构建原节点1--\u0026gt;新节点1--\u0026gt;原节点2--\u0026gt;新节点2的链表； 第二次遍历构建新链表的random指向：old.next.random = old.random.next 第三次遍历拆分原/新链表 class Solution: def copyRandomList(self, head: \u0026#39;Node\u0026#39;) -\u0026gt; \u0026#39;Node\u0026#39;: if not head: return None p=head while p: node = Node(p.val) p.next,node.next=node,p.next p=p.next.next p=head while p: if p.random: p.next.random = p.random.next p=p.next.next ans=p=head.next while p.next: p.next = p.next.next p=p.next return ans 哈希表：构建原链表节点和新链表节点的映射关系，\ndic[old]=new dic[old].next = dic[old.next] dic[old].random = dic[old.random] class Solution: def copyRandomList(self, head: \u0026#39;Node\u0026#39;) -\u0026gt; \u0026#39;Node\u0026#39;: if not head: return None dic,p={},head while p: dic[p]=Node(p.val) p=p.next p=head while p: dic[p].next = dic.get(p.next,None) dic[p].random = dic.get(p.random,None) p=p.next return dic[head] 32. 剑指 Offer 36. 二叉搜索树与双向链表(中等) 中序遍历：记录前驱节点和当前节点，self.pre.right,cur.left=cur,self.pre\nclass Solution: def treeToDoublyList(self, root: \u0026#39;Node\u0026#39;) -\u0026gt; \u0026#39;Node\u0026#39;: def dfs(node): if not node: return None dfs(node.left) if self.pre: self.pre.right,node.left=node,self.pre else: self.head=node self.pre=node dfs(node.right) if not root: return None self.pre=None dfs(root) self.pre.right,self.head.left=self.head,self.pre return self.head 33. 剑指 Offer 37. 序列化二叉树(困难) 层序遍历：反序列化时采用BFS和一个idx指针，idx指针指向的分别是当前节点的左节点和右节点\nclass Codec: def serialize(self, root): if not root: return \u0026#34;None\u0026#34; res = [] queue=[root] while queue: node = queue.pop(0) if node: res.append(str(node.val)) queue.append(node.left) queue.append(node.right) else: res.append(\u0026#39;N\u0026#39;) return \u0026#39;,\u0026#39;.join(res) def deserialize(self, data): if data==\u0026#39;None\u0026#39;: return None data,i = data.split(\u0026#39;,\u0026#39;),1 root=TreeNode(int(data[0])) queue=[root] while queue: node=queue.pop(0) if data[i]!=\u0026#39;N\u0026#39;: node.left=TreeNode(int(data[i])) queue.append(node.left) i+=1 if data[i]!=\u0026#39;N\u0026#39;: node.right=TreeNode(int(data[i])) queue.append(node.right) i+=1 return root 34. 剑指 Offer 38. 字符串的排列 回溯：dfs(idx)表示固定idx位置的字符；用dic=set()记录当前位置已经使用过的字符，达到剪枝的目的；遍历idx到len(lst)-1位置，依次将其固定至idx位置。\nclass Solution: def permutation(self, s: str) -\u0026gt; List[str]: lst,ans=list(s),[] def dfs(idx): if idx==len(lst)-1: ans.append(\u0026#39;\u0026#39;.join(lst)) return dic=set() for i in range(idx,len(lst)): if lst[i] in dic: continue dic.add(lst[i]) lst[i],lst[idx]=lst[idx],lst[i] dfs(idx+1) lst[i],lst[idx]=lst[idx],lst[i] dfs(0) return ans 35. 剑指 Offer 39. 数组中出现次数超过一半的数字(简单) 哈希表统计：用哈希表统计各数字出现的次数\n数组排序法：排序后的数组中点为众数\n摩尔投票法：票数正负抵消\nclass Solution: def majorityElement(self, nums: List[int]) -\u0026gt; int: ans,cnt=-1,0 for num in nums: if cnt==0: ans=num cnt+=1 if ans==num else -1 return ans 36. 剑指 Offer 40. 最小的k个数(简单) 排序或者冒泡k次\n堆：时间复杂度O(nlogk)，空间复杂度O(k)\nimport heapq class Solution: def getLeastNumbers(self, arr: List[int], k: int) -\u0026gt; List[int]: if k==0: return [] heap=[-x for x in arr[:k]] heapq.heapify(heap) for i in range(k,len(arr)): if -arr[i]\u0026gt;heap[0]: heapq.heappushpop(heap,-arr[i]) return [-x for x in heap] 基于快排：每次划分得到哨兵的下标idx，若刚好等于k，则返回arr[:k]，否测递归划分\nimport random class Solution: def getLeastNumbers(self, arr: List[int], k: int) -\u0026gt; List[int]: def partition(nums,l,r): idx = l+random.randint(0,r-l) nums[idx],nums[l]=nums[l],nums[idx] ll,rr=l+1,r while True: while ll\u0026lt;=rr and nums[ll]\u0026lt;nums[l]: ll+=1 while ll\u0026lt;=rr and nums[rr]\u0026gt;nums[l]: rr-=1 if ll\u0026gt;rr: break nums[ll],nums[rr]=nums[rr],nums[ll] ll,rr=ll+1,rr-1 nums[l],nums[rr]=nums[rr],nums[l] if rr==k: return arr[:k] elif rr\u0026lt;k: return partition(arr,rr+1,r) else: return partition(arr,l,rr-1) if k\u0026gt;=len(arr): return arr return partition(arr,0,len(arr)-1) 37. 剑指 Offer 41. 数据流中的中位数(困难) 堆：用一个最小堆和最小堆存储数据，最大堆存储数据流左半部分数据，最小堆存储右半部分数据，保证len(heapL)-len(heapR)\u0026lt;=1。当两个堆长度相等时，中位数为两个堆顶的均值，否则为最大堆的堆顶值。\nimport heapq class MedianFinder: def __init__(self): self.heapl=[] self.heapr=[] def addNum(self, num: int) -\u0026gt; None: if not self.heapl or -num\u0026gt;=self.heapl[0]: heapq.heappush(self.heapl,-num) if len(self.heapl)-len(self.heapr)\u0026gt;1: tmp = -heapq.heappop(self.heapl) heapq.heappush(self.heapr,tmp) else: heapq.heappush(self.heapr,num) if len(self.heapl)\u0026lt;len(self.heapr): tmp = heapq.heappop(self.heapr) heapq.heappush(self.heapl,-tmp) def findMedian(self) -\u0026gt; float: if len(self.heapl)\u0026gt;len(self.heapr): return -self.heapl[0] return (-self.heapl[0]+self.heapr[0])/2 38. 剑指 Offer 42. 连续子数组的最大和(简单) 动态规划：dp[i]为以i结尾的最大子数组和，最终返回max(dp)\nclass Solution: def maxSubArray(self, nums: List[int]) -\u0026gt; int: s,n=0,len(nums) dp=[] for num in nums: dp.append(s+num) s=max(0,dp[-1]) return max(dp) 可以压缩存储空间如下：\nclass Solution: def maxSubArray(self, nums: List[int]) -\u0026gt; int: s,n=0,len(nums) ans = -inf for num in nums: ans = max(ans,s+num) s=max(0,s+num) return ans 39. 剑指 Offer 43. 1～n 整数中 1 出现的次数(困难) 数学，枚举：将数字n分为高位high，当前位cur，低位low，以及位因子digit。统计某位中1出现的次数，分类讨论如下：\n当cur=0：hight x digit，以2304为例，其出现1的范围为0010~2219，即229-0+1=230次； 当cur=1：high x digit + low + 1，以2314为例，其出现1的范围为0010~2314，即234-0+1=235次； 当cur=2,3,...,9：(high+1) x digit，以2374为例，出现1的范围为0010~2319，即239-0+1=240次。 class Solution: def countDigitOne(self, n: int) -\u0026gt; int: ans=0 high,cur,low,digit=n//10,n%10,0,1 while high or cur: if cur==0: ans+=high*digit elif cur==1: ans+=high*digit+low+1 else: ans+=(high+1)*digit high,cur,low,digit=high//10,high%10,low+cur*digit,digit*10 return ans 40. 剑指 Offer 44. 数字序列中某一位的数字(中等) 迭代+求整/求余：\n确定n所在的数字的位数digit：循环执行n减去一位数，两位数。。。的数位量count，直到n\u0026lt;=count； 确定n所在的数字num：num=start+(n-1)//digit； 确定n是num种的哪一数位，返回结果：srt(num)[(n-1)%digit] class Solution: def findNthDigit(self, n: int) -\u0026gt; int: start,digit,cnt=1,1,9 while n\u0026gt;cnt: n-=cnt start,digit=start*10,digit+1 cnt=start*digit*9 num = start+(n-1)//digit return int(str(num)[(n-1)%digit]) 41. 剑指 Offer 46. 把数字翻译成字符串(中等) 动态规划：若s[i-1:i+1]位于10~25之间，则可以翻译在一起，此时dp[i]=dp[i-1]+dp[i-2]，否则dp[i]=dp[i-1]\nclass Solution: def translateNum(self, num: int) -\u0026gt; int: s = str(num) n=len(s) dp=[1,1]+[0]*(n-1) for i in range(1,n): if s[i-1] in [\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;] and int(s[i-1:i+1])\u0026lt;26: dp[i+1]=dp[i]+dp[i-1] else: dp[i+1]=dp[i] return dp[-1] 42. 剑指 Offer 47. 礼物的最大价值(中等) 动态规划\nclass Solution: def maxValue(self, grid: List[List[int]]) -\u0026gt; int: m,n=len(grid),len(grid[0]) dp=[[0 for _ in range(n+1)]for _ in range(m+1)] for i in range(1,m+1): for j in range(1,n+1): dp[i][j]=max(dp[i-1][j],dp[i][j-1])+grid[i-1][j-1] return dp[-1][-1] 43. 剑指 Offer 48. 最长不含重复字符的子字符串(中等) 双指针+哈希表：哈希表记录字符最右的位置，当遍历到哈希表中存在的字符时，更新i为max(i,vis[s[j]])，每次计算最长长度j-i\nclass Solution: def lengthOfLongestSubstring(self, s: str) -\u0026gt; int: vis,ans,i={},0,-1 for j in range(len(s)): if s[j] in vis: i=max(i,vis[s[j]]) vis[s[j]]=j ans=max(ans,j-i) return ans 44. 剑指 Offer 49. 丑数(中等) 最小堆：每次取出堆顶元素x，然后将2x,3x,5x加入堆中，用哈希表去重，最后返回堆顶元素\nimport heapq class Solution: def nthUglyNumber(self, n: int) -\u0026gt; int: factors = [2,3,5] heap,vis=[1],{1} for i in range(n-1): curr = heapq.heappop(heap) for factor in factors: nxt = curr*factor if nxt not in vis: vis.add(nxt) heapq.heappush(heap,nxt) return heapq.heappop(heap) 动态规划：设置p2,p3,p5三个索引，dp[i]=min(dp[p2]*2,dp[p3]*3,dp[p5]*5)，独立判断dp[i]与这三个值的大小关系，若相等则将相应的索引+1\nclass Solution: def nthUglyNumber(self, n: int) -\u0026gt; int: dp=[1]*n p2,p3,p5=0,0,0 for i in range(1,n): n2,n3,n5=dp[p2]*2,dp[p3]*3,dp[p5]*5 dp[i]=min(n2,n3,n5) if dp[i]==n2: p2+=1 if dp[i]==n3: p3+=1 if dp[i]==n5: p5+=1 return dp[-1] 45. 剑指 Offer 50. 第一个只出现一次的字符(简单) 有序哈希表：OrderedDict()\nclass Solution: def firstUniqChar(self, s: str) -\u0026gt; str: dic = collections.OrderedDict() for c in s: dic[c] = not c in dic for k,v in dic.items(): if v: return k return \u0026#39; \u0026#39; ","permalink":"https://Achilles-10.github.io/posts/algo/offer2/","summary":"26. 剑指 Offer 30. 包含min函数的栈(简单) 双栈：一个栈存储数据，一个栈存储最小值 class MinStack: def __init__(self): self.stack=[] self.minstack=[inf] def push(self, x: int) -\u0026gt; None: self.stack.append(x) self.minstack.append(min(x,self.minstack[-1])) def pop(self) -\u0026gt; None: self.stack.pop() self.minstack.pop() def top(self) -\u0026gt; int: return self.stack[-1] def min(self) -\u0026gt; int: return self.minstack[-1] 27. 剑指 Offer 31. 栈的压入、弹出序列 模拟：按顺序push，若和popped值相等，则pop，最后判断栈是否为空。 class Solution: def validateStackSequences(self, pushed: List[int], popped: List[int]) -\u0026gt; bool: stack,i=[],0 for num in pushed: stack.append(num) while stack and","title":"剑指offer复习笔记(2)"},{"content":"[paper] [code]\n引言 由于通道注意力在特征建模上的简单性和有效性，成为深度学习领域流行的工具。而全局平均池化（GAP）由于其简单性成为了默认选择，但它的简单性也使得它很难很好地捕获各种输入的复杂信息。\n本文将信道的标量表示看作一个压缩问题。也就是说，一个通道的信息应该被一个标量紧凑地编码，同时尽可能地保留整个通道的表示能力。而如何有效地压缩具有标量的信道是一大难点。\n由于DCT的高压缩比可以满足用标量表示信道注意力的需求，以及它可微的性质可以简单地集成到CNN中，选择DCT定制通道注意力。本文主要贡献如下：\n把通道注意力看作一个压缩问题，并在通道注意力中引入DCT。证明了传统GAP是DCT的一个特例。基于这一证明，在频域推广了通道注意力，并提出了多谱通道注意力框架（Multi-Spectral Channel Attention , MSCA），称为FcaNet； 我们提出了三种频率成分选择标准（LF低频选择，TS两步选择，NAS神经架构搜索选择）以及所提出的多光谱通道注意框架来实现FcaNet。 在ImageNet和COCO数据集上达到SOTA水平。 方法 回顾DCT和通道注意力 DCT： $$ f^{2d}_{h,w}=\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1}{x^{2d}_{i,j}B^{i,j}_{h,w}}\\\\ B^{i,j}_{h,w}=\\cos(\\frac{\\pi h}{H}(i+\\frac{1}{2}))\\cos(\\frac{\\pi w}{W}(j+\\frac{1}{2}))\\tag{1} $$\n通道注意力：通道注意力用标量来表示和评估每个通道的重要性，可以写成如下形式： $$ att=sigmoid(fc(compress(X))) $$ compress: $\\mathbb {R}^{C\\times H\\times W}\\rightarrow\\mathbb{R}^C$ 是压缩方法。全局平均池化（GAP）可以视作一种压缩方法。\n多谱通道注意力（Multi-Spectral Channel Attention，MSCA） 通道注意力的理论分析，定理1：GAP是2D DCT的特例，其结果与2D DCT的最低频率成分成正比。\n证明：在公式(1)中，令h和w为0，有\n$$ \\begin{align} f^{2d}_{0,0} \u0026amp;=\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1}{x^{2d}_{i,j}B^{i,j}_{0,0}}\\\\ \u0026amp;=\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1}x^{2d}_{i,j}\\\\ \u0026amp;=GAP(x^{2d})\\cdot HW \\end{align}\\tag{2} $$\n在公式(2)中，$f^{2d}_{0,0}$表示2D DCT的最低频率成分，与GAP成正比，定理1得证。\n多谱通道注意力模块：\n通过定理1可知，在使用通道注意力中使用GAP意味着只保留了最低频的信息，为了更好地压缩信道并引入更多信息，需要利用DCT中更多的频率信息。\n首先，将输入X沿通道方向分块为$[X^0,X^1,\\dots,X^{n-1}]$，其中$X^i\\in\\mathbb{R}^{C\u0026rsquo;\\times H\\times W},\\ C\u0026rsquo;=\\frac{C}{n}$，C能被n整除。对每部分进行2D DCT操作，有： $$ Freq^i=2\\text{DDCT}(X^i) $$ 最终的压缩向量可以表示如下： $$ \\begin{align} Freq\u0026amp;=compress(X)\\ \u0026amp;=cat([Freq^0,Freq^1,\\dots,Freq^{n-1}]) \\end{align} $$ 最终的多谱注意力表示如下： $$ ms_att=sigmoid(fc(Freq)) $$\n频率成分选择标准（Criteria for Choosing Frequency Components）\n提出三种选择频率成分的标准：\nFcaNet-LF：选择低频成分 FceNet-TS：通过两步选择方案确定，首先确定每个频率分量的重要性，然后评估不同频率分量数量的效果 FcaNet-NAS：通过神经架构搜索来搜索通道的最佳频率成分 实验 消融实验 单体频率分量的影响（The effects of individual frequency components）\nImageNet上的最小特征图大小为7x7，因此将2D DCT的频率空间划分为7x7，测试每部分的性能如下图（TOP-1 准确率）：\n可以看出，低频有更好的表现，也验证了SENet的成功和深度网络偏好低频信息的结论。然而几乎所有的频率成分(除最高频率外)与最低频率成分之间的差距非常小(\u0026lt;=0.5% Top-1准确率)。这说明其他频率成分也能很好地应对通道注意力机制，在频域上泛化通道注意力是有效的。\n频率分量的数量的影响（The effects of different numbers of frequency components）\n对于TS，选取上图中Top-K性能最高的频率成分；对于LF，选取K个最低频率成分的结果。结果如下图所示，可以发现：\n使用多谱注意力的性能比仅使用通道注意力中的GAP都有提高 当k=2和16时效果最好 与完全可学习的通道注意力相比：2D DCT的基函数可以看做是包含DCT系数的张量\nFR：Fixed tensor with Random initialization，随机初始化固定张量\nLR：Learned tensor with Random initialization，随机初始化可学习张量\nLD：Learned tensor with DCT initialization，DCT初始化可学习张量\nFD：Fixed tensor with DCT initialization，DCT初始化固定张量\n讨论 多谱框架（multi-spectrum framework）如何压缩和嵌入更多信息：\n由于深度网络是冗余的，若两个通道是冗余的，则通过GAP只能得到相同的信息；而在多谱注意力中，不同的频率分量包含不同的信息，因此可以从冗余通道中提取更多的信息。\n复杂度分析：\nDCT权重是预定义的常数，没有额外参数；额外计算成本与SENet相当。\n代码实现：\n多谱注意力与SENet的区别仅在于信道压缩方法（GAP vs. multi-spectrum 2D DCT），2D DCT可以看做是输入的加权和，因此该方法可以很容易地集成到任意通道注意力方法中。\n在ImageNet上的图像分类 在COCO上的目标检测 ","permalink":"https://Achilles-10.github.io/posts/paper/fcanet/","summary":"[paper] [code] 引言 由于通道注意力在特征建模上的简单性和有效性，成为深度学习领域流行的工具。而全局平均池化（GAP）由于其简单性成为了默认选择，但它的简单性也使得它很难很好地捕获各种输入的复杂信息。 本文将信道的标量表示看作一个压缩问题。也就是说，一个通道的信息应该被一个标量紧凑地编码，同时尽","title":"FcaNet: Frequency Channel Attention Networks"},{"content":"变换 OpenCV提供了cv2.warpAffine和cv2.warpPerspective两个转换函数，cv2.warpAffine采用2x3的转换矩阵，cv2.warpPerspective采用3x3转换矩阵。\n缩放 使用cv2.resize实现图像的缩放，可以指定缩放尺寸或缩放比例，以及插值方法。首选的插值方法是用于缩小的 cv2.INTER_AREA 和用于缩放的 cv2.INTER_CUBIC（慢）和 cv2.INTER_LINEAR。cv2.INTER_LINEAR是默认的缩放插值方法。可以用一下两种方法实现：\nimport numpy as np import cv2 img = cv2.imread(\u0026#39;face.png\u0026#39;) res = cv2.resize(img, None,fx=2, fy=2, interpolation = cv2.INTER_CUBIC) # OR height, width = img.shape[:2] res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC) 平移 如果在(x,y)方向上的平移量为$(t_x,t_y)$，则可以得到转换矩阵M: $$ M=\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; t_x \\\\ 0 \u0026amp; 1 \u0026amp; t_y \\end{bmatrix} $$ 将其转换为np.float32的numpy数组并传入cv2.warpAffine函数，以平移(100,50)为例：\nrows,cols,_ = img.shape M = np.float32([[1,0,100],[0,1,50]]) dst = cv2.warpAffine(img,M,(cols,rows)) cv2.warpAffine的第三个参数是输出图像的大小，形式为(width,height)\n旋转 图像旋转角度为$\\theta$是通过以下变换矩阵实现的： $$ M = \\begin{bmatrix} \\cos\\theta \u0026amp; -\\sin\\theta \\\\ \\sin\\theta \u0026amp; \\cos\\theta \\end{bmatrix} $$ OpenCV提供了可缩放的旋转和可调整的旋转中心，修改后的变换矩阵为： $$ \\begin{bmatrix} \\alpha \u0026amp; \\beta \u0026amp; (1- \\alpha ) \\cdot center.x - \\beta \\cdot center.y \\\\ - \\beta \u0026amp; \\alpha \u0026amp; \\beta \\cdot center.x + (1- \\alpha ) \\cdot center.y \\end{bmatrix} $$ 其中： $$ \\alpha=scale\\cdot\\cos\\theta,\\\\\\beta=scale\\cdot\\sin\\theta $$ 为了得到该变换矩阵，OpenCV提供了cv2.getRotationMatrix2D函数，以将图像相对于中心旋转逆时针90度缩放比例为1：\nrows,cols,_ = img.shape # cols-1 和 rows-1 是坐标限制 M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),90,1) dst = cv2.warpAffine(img,M,(cols,rows)) 仿射变换（Affine Transformation） 在仿射转换中，原始图像中的所有并行线仍将在输出图像中平行。为了得到转换矩阵，需要从输入图像中的三个点及其在输出图像中的对应位置。通过cv2.getAffineTransform函数创建一个2x3的矩阵，并传递给cv2.warpAffine。\nrows,cols,ch = img.shape pts1 = np.float32([[100,100],[100,400],[400,100]]) pts2 = np.float32([[50,50],[100,400],[350,50]]) M = cv2.getAffineTransform(pts1,pts2) dst = cv2.warpAffine(img,M,(cols,rows)) 透视变换（Perspective Transformation） 透视转换需要一个3x3转换矩阵。即使在转换后，直线也将保持直线。需要在输入图像上有四个点，在输出图像中需要对应的四个点，其中三个点不共线。可通过cv2.getPersperctiveTransform得到变换矩阵，并传递给cv2.warpPerspective。\nrows,cols,ch = img.shape pts1 = np.float32([[40,100],[400,100],[0,400],[360,400]]) pts2 = np.float32([[0,0],[500,0],[0,500],[500,500]]) M = cv2.getPerspectiveTransform(pts1,pts2) dst = cv2.warpPerspective(img,M,(cols,rows)) ","permalink":"https://Achilles-10.github.io/posts/tech/opencv3/","summary":"变换 OpenCV提供了cv2.warpAffine和cv2.warpPerspective两个转换函数，cv2.warpAffine采用2x3的转换矩阵，cv2.warpPerspective采用3x3转换矩阵。 缩放 使用cv2.resize实现图像的缩放，可以指定缩放尺寸或缩放比","title":"OpenCV-Python学习笔记(3)：几何变换"},{"content":"1. 剑指 Offer 03. 数组中重复的数字(简单) 哈希表：用哈希表（Set）记录遍历到的数字，若找到重复的数字则返回。\n原地交换：数组元素的索引和值是一对多的关系。因此，可遍历数组并通过交换操作，使元素的索引与值一一对应（即$nums[i]=i$）。\n算法流程\n遍历数组，索引初始值i=0; 若nums[i]=i：说明该数字已在对应的索引处，无需交换，跳过； 若nums[nums[i]]=nums[i]：说明索引nums[i]处和索引i处的值均为nums[i]，即找到一组重复，返回nums[i]； 否则交换nums[nums[i]]与nums[i]； 若遍历完未返回，返回-1。 复杂度：时间O(N)，空间O(1)\n代码：\nclass Solution: def findRepeatNumber(self, nums: List[int]) -\u0026gt; int: n=len(nums) i=0 while i\u0026lt;n: if nums[i]==i: i+=1 continue if nums[nums[i]]==nums[i]: return nums[i] nums[nums[i]],nums[i]=nums[i],nums[nums[i]] Python中a,b=c,d的原理是暂存元组(c,d)，然后按左右顺序赋值，在此处需要先给nums[nums[i]]赋值。\n2. 剑指 Offer 04. 二维数组中的查找(中等) 暴力法：时间复杂度O(MN)，未利用到数组的排序信息\n二分法：时间复杂度O(M+N)\n如下图，考虑将二维数组旋转45°，类似一棵二叉搜索树：故我们可以从数组的左下角(n-1,0)或右上角(0,m-1)开始，运用二分的思想进行搜索。\nclass Solution: def findNumberIn2DArray(self, matrix: List[List[int]], target: int) -\u0026gt; bool: i,j=len(matrix)-1,0 while 0\u0026lt;=i and j\u0026lt;len(matrix[0]): if target==matrix[i][j]: return True elif target\u0026gt;matrix[i][j]: j+=1 else: i-=1 return False 3. 剑指 Offer 05. 替换空格(简单) 库函数:return s.replace(' ','%20')\n遍历添加：初始化一个新的str，时间空间复杂度O(N)\n原地修改(python str是不可修改，无法实现)：\n遍历得到空格数cnt 修改s长度为len+2*cnt 倒序遍历，i指向原字符串末尾，j指向新字符串末尾，当i=j时跳出（左方已没有空格）； s[i]=' '：s[j-2:j]=\u0026rsquo;%20\u0026rsquo;，j-=2 s[i]!=' '：s[j]=s[i] string replaceSpace(string s) { int count = 0, len = s.size(); for (char c : s) { if (c == \u0026#39; \u0026#39;) count++; } s.resize(len + 2 * count); for(int i = len - 1, j = s.size() - 1; i \u0026lt; j; i--, j--) { if (s[i] != \u0026#39; \u0026#39;) s[j] = s[i]; else { s[j - 2] = \u0026#39;%\u0026#39;; s[j - 1] = \u0026#39;2\u0026#39;; s[j] = \u0026#39;0\u0026#39;; j -= 2; } } return s; } 4. 剑指 Offer 06. 从尾到头打印链表(简单) 辅助栈：遍历链表，将各节点入栈，返回倒序列表。时间空间复杂度O(N)\n递归：时间空间复杂度O(N)\nclass Solution: def reversePrint(self, head: ListNode) -\u0026gt; List[int]: return self.reversePrint(head.next) + [head.val] if head else [] 5. 剑指 Offer 07. 重建二叉树(中等) 递归：\npreorder=[root,L,R], inorder=[L,root,R]\n找到root在inorder中的下标，构建root的左右子树\nclass Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -\u0026gt; TreeNode: if not preorder: return None root=TreeNode(preorder[0]) rootidx=inorder.index(preorder[0]) root.left=self.buildTree(preorder[1:rootidx+1],inorder[:rootidx]) root.right=self.buildTree(preorder[rootidx+1:],inorder[rootidx+1:]) return root 迭代：待续\n6. 剑指 Offer 09. 用两个栈实现队列(简单) 双栈：将一个栈当作输入栈，用于将数据入队；另一个栈当作输出栈，用于数据出队。每次出队时，若输出栈不为空，则直接从输出栈弹出；否则现将所有数据从输入栈弹出并压入输出栈，再从输出栈弹出。 class CQueue: def __init__(self): self.stack1, self.stack2=[],[] def appendTail(self, value: int) -\u0026gt; None: self.stack1.append(value) def deleteHead(self) -\u0026gt; int: if self.stack2: return self.stack2.pop() if not self.stack1: return -1 while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop() 7. 剑指 Offer 10- I. 斐波那契数列(简单) 动态规划：\nclass Solution: def fib(self, n: int) -\u0026gt; int: a,b=0,1 while n: a,b=b,a+b n-=1 return a%(10**9+7) 矩阵快速幂：时间复杂度O(log n)，空间复杂度O(1)\n$$ \\left[\\begin{matrix} 1\u0026amp;1 \\\\ 1\u0026amp;0 \\end{matrix} \\right] \\left[\\begin{matrix} F(n) \\\\ F(n-1) \\end{matrix} \\right] = \\left[ \\begin{matrix} F(n)+F(n-1)\\\\ F(n) \\end{matrix} \\right] =\\left[\\begin{matrix} F(n+1) \\\\ F(n) \\end{matrix}\\right] $$\n$$ \\left[\\begin{matrix} F(n+1) \\\\ F(n) \\end{matrix}\\right] = \\left[\\begin{matrix} 1\u0026amp;1 \\\\ 1\u0026amp;0 \\end{matrix} \\right]^n \\left[\\begin{matrix} F(1) \\\\ F(0) \\end{matrix}\\right] $$\n令： $$ M=\\left[\\begin{matrix} 1\u0026amp;1 \\\\ 1\u0026amp;0 \\end{matrix} \\right] $$ 关键在于快速计算矩阵M的n次幂。\nclass Solution: def fib(self, n: int) -\u0026gt; int: MOD = 10 ** 9 + 7 if n \u0026lt; 2: return n def multiply(a: List[List[int]], b: List[List[int]]) -\u0026gt; List[List[int]]: c = [[0, 0], [0, 0]] for i in range(2): for j in range(2): c[i][j] = (a[i][0] * b[0][j] + a[i][1] * b[1][j]) % MOD return c def matrix_pow(a: List[List[int]], n: int) -\u0026gt; List[List[int]]: ret = [[1, 0], [0, 1]] while n \u0026gt; 0: if n \u0026amp; 1: ret = multiply(ret, a) n \u0026gt;\u0026gt;= 1 a = multiply(a, a) return ret res = matrix_pow([[1, 1], [1, 0]], n - 1) return res[0][0] 8. 剑指 Offer 11. 旋转数组的最小数字(简单) 二分法：\n如下图，考虑数组最后一个元素x，最小值右侧的元素一定小于等于x，最小值左侧的元素一定大于等于x。\n可以分为三种情况：\nnums[mid]\u0026gt;x：left=mid+1 nums[mid]\u0026lt;x：right=mid nums[mid]=x：此时无法判断nums[mid]在最小值左侧还是右侧，但可以确定nums[right]有nums[mid]这个替代值，故可以忽略右端点。right-=1 class Solution: def minArray(self, numbers: List[int]) -\u0026gt; int: n=len(numbers) l,r=0,n-1 while l\u0026lt;r: mid = l+(r-l)//2 if numbers[mid]\u0026gt;numbers[r]: l=mid+1 elif numbers[mid]\u0026lt;numbers[r]: r=mid else: r-=1 return numbers[l] 9. 剑指 Offer 12. 矩阵中的路径(中等) 回溯(DFS)：时间复杂度$O(MN3^L)$，空间复杂度O(MN)\n用backtracking(i,j,idx)表示从位置(i,j)出发能否匹配字符串word[idx:]，执行步骤如下：\n若board[i][j]!=word[idx]，不匹配返回False 若当前字符匹配且到了字符串末尾，返回True 否则，遍历当前相邻位置 class Solution: def exist(self, board: List[List[str]], word: str) -\u0026gt; bool: m,n=len(board),len(board[0]) vis=set() directions=[(1,0),(-1,0),(0,1),(0,-1)] def backtracking(i,j,idx): if board[i][j]!=word[idx]: return False if idx==len(word)-1: return True vis.add((i,j)) for di,dj in directions: ii,jj=i+di,j+dj if 0\u0026lt;=ii\u0026lt;m and 0\u0026lt;=jj\u0026lt;n and (ii,jj) not in vis: if backtracking(ii,jj,idx+1): return True vis.remove((i,j)) return False for i in range(m): for j in range(n): if backtracking(i,j,0): return True return False 10. 剑指 Offer 14- I. 剪绳子(中等) 动态规划：时间空间复杂度O(n)\ndp[1]=1,dp[2]=1,状态转移方程： $$ dp[i]=max(2*dp[i-2],3*dp[i-3],2*(i-2),3*(i-3)) $$\nclass Solution: def cuttingRope(self, n: int) -\u0026gt; int: dp=[0]*(n+1) dp[1]=dp[2]=1 for i in range(3,n+1): dp[i]=max(2*dp[i-2],3*dp[i-3],2*(i-2),3*(i-3)) return dp[n] 数学推导（贪心）:\nclass Solution: def cuttingRope(self, n: int) -\u0026gt; int: if n\u0026lt;4: return n-1 a,b=n//3,n%3 if b==1: return int(math.pow(3,a-1)*4) elif b==2: return int(math.pow(3,a)*2) return int(math.pow(3,a)) 11. 剑指 Offer 15. 二进制中1的个数(简单) 循环遍历\n位运算优化：时间复杂度O(log n)。每次将n与n-1做与操作，可以将n的最低位的1变为0。例如6(110)\u0026amp;5(101)=4(100)。\nclass Solution: def hammingWeight(self, n: int) -\u0026gt; int: ans=0 while n: n\u0026amp;=(n-1) ans+=1 return ans 12. 剑指 Offer 16. 数值的整数次方(中等) 快速幂乘法：递归和迭代\nclass Solution: def myPow(self, x: float, n: int) -\u0026gt; float: # 迭代 def quickMul(n): ans=1.0 xx=x while n: if n\u0026amp;1: ans*=xx xx*=xx n\u0026gt;\u0026gt;=1 return ans return quickMul(n) if n\u0026gt;=0 else 1/quickMul(-n) class Solution: def myPow(self, x: float, n: int) -\u0026gt; float: def quickMul(n): if n==0: return 1.0 y=quickMul(n//2) return y*y if n\u0026amp;1==0 else y*y*x return quickMul(n) if n\u0026gt;=0 else 1/quickMul(-n) 13. 剑指 Offer 17. 打印从1到最大的n位数(简单) DFS：用字符串来正确表示大数（本题不需要），依次遍历长度1~n的数，第一位只能为1~9，其他位为0~9。\nclass Solution: def printNumbers(self, n: int) -\u0026gt; List[int]: ans=[] def dfs(k,n,s): if k==n: ans.append(int(s)) return for i in range(10): dfs(k+1,n,s+str(i)) for i in range(1,n+1): for j in range(1,10): dfs(1,i,str(j)) return ans 15. 剑指 Offer 18. 删除链表的节点(简单) 前驱结点遍历：考虑要删除节点为头结点的特殊情况。\nclass Solution: def deleteNode(self, head: ListNode, val: int) -\u0026gt; ListNode: if head.val==val: return head.next pre,p=head,head.next while p and p.val!=val: pre,p=p,p.next if p: pre.next = p.next return head 16. 剑指 Offer 19. 正则表达式匹配(困难) 动态规划：\ndp[i][j]表示s[:i]与p[:j]是否匹配，考虑以下情况：\np[j]='*': 若s[i]与p[j-1]不匹配，则p[j-1]匹配零次即为dp[i][j]=dp[i][j-2]；否则p[j-1]匹配1次或多次，即dp[i][j]=dp[i-1][j] or dp[i][j-2] p[j]!='*'：若s[i]与p[j]匹配，则dp[i][j]=dp[i-1[j-1] s[i]与p[j]匹配时满足，s[i]=p[j] or p[j]=='.' 初始化时，dp[0][0]=True，考虑s为空数组，只有p的偶数位为*时能够匹配 class Solution: def isMatch(self, s: str, p: str) -\u0026gt; bool: m,n=len(s),len(p) dp = [[False for _ in range(n+1)]for _ in range(m+1)] dp[0][0]=True for j in range(2,n+1,2): if p[j-1]==\u0026#39;*\u0026#39;: dp[0][j]=dp[0][j-2] for i in range(1,m+1): for j in range(1,n+1): if p[j-1]==\u0026#39;*\u0026#39;: if s[i-1]==p[j-2] or p[j-2]==\u0026#39;.\u0026#39;: dp[i][j]=dp[i-1][j] or dp[i][j-2] else: dp[i][j]=dp[i][j-2] else: if s[i-1]==p[j-1] or p[j-1]==\u0026#39;.\u0026#39;: dp[i][j]=dp[i-1][j-1] return dp[-1][-1] 17. 剑指 Offer 20. 表示数值的字符串(中等) 模拟\n有限状态机：定义状态-\u0026gt;画状态转移图-\u0026gt;编写代码\n字符类型：空格 ，数字1-9，正负号+-，小数点.，幂符号eE。\n状态定义：\n开始的空格 幂符号前的正负号 小数点前的数字 小数点，小数点后的数字 当小数点前为空格时，小数点和小数点后的数字 幂符号 幂符号后的正负号 幂符号后的数字 结尾的空格 状态转移图：\nclass Solution: def isNumber(self, s: str) -\u0026gt; bool: states = [ {\u0026#39; \u0026#39;:0,\u0026#39;s\u0026#39;:1,\u0026#39;d\u0026#39;:2,\u0026#39;.\u0026#39;:4}, # 0. start with \u0026#39;blank\u0026#39; {\u0026#39;d\u0026#39;:2,\u0026#39;.\u0026#39;:4}, # 1. \u0026#39;sign\u0026#39; before \u0026#39;e\u0026#39; {\u0026#39;d\u0026#39;:2,\u0026#39;.\u0026#39;:3,\u0026#39;e\u0026#39;:5,\u0026#39; \u0026#39;:8}, # 2. \u0026#39;digit\u0026#39; before \u0026#39;dot\u0026#39; {\u0026#39;d\u0026#39;:3,\u0026#39;e\u0026#39;:5,\u0026#39; \u0026#39;:8}, # 3. \u0026#39;digit\u0026#39; after \u0026#39;dot\u0026#39; {\u0026#39;d\u0026#39;:3}, # 4. \u0026#39;digit\u0026#39; after \u0026#39;dot\u0026#39; (‘blank’ before \u0026#39;dot\u0026#39;) {\u0026#39;s\u0026#39;:6,\u0026#39;d\u0026#39;:7}, # 5. \u0026#39;e\u0026#39; {\u0026#39;d\u0026#39;:7}, # 6. \u0026#39;sign\u0026#39; after \u0026#39;e\u0026#39; {\u0026#39;d\u0026#39;:7,\u0026#39; \u0026#39;:8}, # 7. \u0026#39;digit\u0026#39; after \u0026#39;e\u0026#39; {\u0026#39; \u0026#39;:8} # 8. end with \u0026#39;blank\u0026#39; ] p = 0 # start with state 0 for c in s: if \u0026#39;0\u0026#39;\u0026lt;=c\u0026lt;=\u0026#39;9\u0026#39;: t = \u0026#39;d\u0026#39; # digit elif c in \u0026#34;+-\u0026#34;: t = \u0026#39;s\u0026#39; # sign elif c in \u0026#34;eE\u0026#34;: t = \u0026#39;e\u0026#39; # e or E elif c in \u0026#34;. \u0026#34;: t = c # dot, blank else: t = \u0026#39;?\u0026#39; # unknown if t not in states[p]: return False p = states[p][t] return p in (2, 3, 7, 8) 18. 剑指 Offer 21. 调整数组顺序使奇数位于偶数前面(简单) 双指针交换\nclass Solution: def exchange(self, nums: List[int]) -\u0026gt; List[int]: i,j=0,len(nums)-1 while i\u0026lt;j: while i\u0026lt;j and nums[i]%2: i+=1 while i\u0026lt;j and nums[j]%2==0: j-=1 nums[i],nums[j]=nums[j],nums[i] return nums 19. 剑指 Offer 22. 链表中倒数第k个节点(简单) 双指针：让快指针先走k个节点\nclass Solution: def getKthFromEnd(self, head: ListNode, k: int) -\u0026gt; ListNode: former, latter = head, head for _ in range(k): former = former.next while former: former, latter = former.next, latter.next return latter 20. 剑指 Offer 24. 反转链表(简单) 迭代（双指针）：\nclass Solution: def reverseList(self, head: ListNode) -\u0026gt; ListNode: pre,cur=None,head while cur: cur.next,pre,cur=pre,cur,cur.next return pre 递归：\nclass Solution: def reverseList(self, head: ListNode) -\u0026gt; ListNode: def recur(pre,cur): if not cur: return pre res = recur(cur,cur.next) cur.next=pre return res return recur(None,head) 21. 剑指 Offer 25. 合并两个排序的链表(简单) 递归：\nclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -\u0026gt; ListNode: if not l1 or not l2: return l1 or l2 if l1.val\u0026lt;l2.val: l1.next = self.mergeTwoLists(l1.next,l2) return l1 else: l2.next = self.mergeTwoLists(l1,l2.next) return l2 22. 剑指 Offer 26. 树的子结构(中等) 先序遍历：\n分为两步，先序遍历树A中的每个节点$n_A$；判断以$n_A$为根节点的子树是否包含树B。\nsame函数判断以$n_A$为根节点的子树是否包含树B，若B为空，则表示树B匹配完成，返回True；若A为空，则说明越过A，返回False；若A和B值不同，则不匹配，返回False。\nclass Solution: def isSubStructure(self, A: TreeNode, B: TreeNode) -\u0026gt; bool: def same(A,B): if not B: return True if not A: return False return A.val==B.val and same(A.left,B.left) and same(A.right,B.right) return bool(A and B) and (same(A,B) or self.isSubStructure(A.left,B) or self.isSubStructure(A.right,B)) 23. 剑指 Offer 27. 二叉树的镜像(简单) 递归：注意同时赋值或者临时保存子树\nclass Solution: def mirrorTree(self, root: TreeNode) -\u0026gt; TreeNode: if not root: return None root.left,root.right = self.mirrorTree(root.right),self.mirrorTree(root.left) return root 迭代：用栈保存节点\nclass Solution: def mirrorTree(self, root: TreeNode) -\u0026gt; TreeNode: if not root: return None stack=[root] while stack: node = stack.pop() if node.left: stack.append(node.left) if node.right: stack.append(node.right) node.left,node.right=node.right,node.left return root 24. 剑指 Offer 28. 对称的二叉树(简单) 递归：\nclass Solution: def isSymmetric(self, root: TreeNode) -\u0026gt; bool: def recur(L,R): if not L and not R: return True if not L or not R: return False return L.val==R.val and recur(L.left,R.right) and recur(L.right,R.left) return not root or recur(root.left,root.right) 迭代：\nclass Solution: def isSymmetric(self, root: TreeNode) -\u0026gt; bool: if not root or not (root.left or root.right): return True queue=[root.left,root.right] while queue: l=queue.pop(0) r=queue.pop(0) if not l and not r: continue if not l or not r or l.val!=r.val: return False queue.append(l.left) queue.append(r.right) queue.append(l.right) queue.append(r.left) return True 25. 剑指 Offer 29. 顺时针打印矩阵(简单) 设置边界：设置top，bottom，left，right，遍历\nclass Solution: def spiralOrder(self, matrix: List[List[int]]) -\u0026gt; List[int]: if not matrix: return [] top,bottom,left,right,ans=0,len(matrix)-1,0,len(matrix[0])-1,[] while True: for j in range(left,right+1): ans.append(matrix[top][j]) top+=1 if top\u0026gt;bottom: break for i in range(top,bottom+1): ans.append(matrix[i][right]) right-=1 if right\u0026lt;left: break for j in range(right,left-1,-1): ans.append(matrix[bottom][j]) bottom-=1 if top\u0026gt;bottom: break for i in range(bottom,top-1,-1): ans.append(matrix[i][left]) left+=1 if right\u0026lt;left: break return ans ","permalink":"https://Achilles-10.github.io/posts/algo/offer1/","summary":"1. 剑指 Offer 03. 数组中重复的数字(简单) 哈希表：用哈希表（Set）记录遍历到的数字，若找到重复的数字则返回。 原地交换：数组元素的索引和值是一对多的关系。因此，可遍历数组并通过交换操作，使元素的索引与值一一对应（即$nums[i]=i$）。 算法流程 遍历数组，索引初始值i=0; 若nums[","title":"剑指offer复习笔记(1)"},{"content":"[paper]\n1. 介绍 使用结合CNN和Transformer的双流网络来融合伪造人脸的局部和全局伪造信息，进而提高模型的泛化能力。同时使用高频信息在CNN和Transformer之间交互，实现更通用和鲁棒的伪造检测。\n低频信息和中高频信息通常表现出不同的特征，其中低频分量主要包括图像的自然内容信息，中高频分量则主要包含混合边界、模糊伪影和棋盘格(checkboard)等细粒度信息。\n主要贡献如下：\n提出了一种用于人脸伪造检测的双流网络，分层频域辅助交互网络（Hierarchical Frequency-assisted Interactive Networks, HFI-Net）。 我们设计了一个新颖的频域特征改进模块（Frequency-based Feature Refinement，FFR）来提取RGB特征上的中高频信息，充分利用更通用和鲁棒的频域特征，避免了空间伪影的脆弱性。 我们提出了一种共享和频域辅助的全局局部交互模块（Global Local Interaction, GLI），该模块放置在HFI-Net的多级层中，以在全局上下文和局部特征之间进行有效的交互。 2. 方法 2.1 概述 HFI-Net是由CNN分支和Transformer分支组成的双流网络，旨在捕获全局上下文信息和局部细节信息。\nTransformer分支的backbone是ViT。CNN分支有一个瓶颈卷积(Bottleneck)和四个可分离阶段，其中瓶颈卷积包含两个3x3卷积层用于提取边缘和纹理等初始局部特征；每个可分离阶段有三个可分离卷积块组成，以及阶段输入和输出之间的残差连接。\nGLI模块放置在HFI-Net的每个阶段，由两个FFR模块组成，用于融合全局和局部信息，并在学习中高频伪造痕迹的同时抑制共享的高级语义特征。\nTransformer的输入为$x^g\\in\\mathbb{R}^{T\\times D},\\ T=196,\\ D=768$，bottleneck输入为$x^l\\in\\mathbb{R}^{C\\times H\\times W},\\ C=768,\\ H=W=14$。在CNN分支中没有下采样操作，故特征维度不会改变。\n最后，每个分支使用一个分类器进行训练。在训练阶段，采用交叉熵损失作为损失函数。在测试期间，将两个分类器的输出的均值作为最终预测结果。\n2.2 FFR (Frequency-based Feature Refinement) DCT\n频率选择准则（Frequency Selection Criterion, FSC）：\n与高频信息相比，CNN倾向于强调低频通道，其中包含图像的背景和真实部分；同时，已有许多研究证实了频域中的真实人脸和伪造人脸之间存在差异。如下图，伪造人脸和真实人脸在RGB图像，RGB特征图和低频特征图上几乎一致，但在中高频特征图上存在关键差异。\n所以本文在RGB特征图上使用2D DCT提取中高频分量，如下图，假设n表示提取的中高频基数，将这n个频域特征拼接在一起得到$\\tilde{X}_{u,v}=cat[F_i,\\dots,F_n]$。其中用到了FcaNet: Frequency Channel Attention Networks。\nFrequency-Based Feature Refinement（FFR）：\n聚合特征T： $$ T=\\sum_{h=0}^{H-1}\\sum_{w=0}^{W-1}\\tilde{X}_{h,w} $$ 用一个MLP层（2个全连接层）和一个sigmoid函数$\\sigma$来生成注意力权重： $$ W=\\sigma(MLP(T)) $$\n2.3 Global-Local Interaction Module（GLC）： Frequency-Assisted Global-Local Interaction (GLI) Module：\nGLI 模块利用FFR模块得到频域注意力权重，通过使用注意力权重和残差连接特征获得最终输出： $$ X^g_{freq}=X^g+W^l\\odot X^g\\\\\\ X^l_{freq}=X_l+W^g\\odot X^l $$\nMulti-Level Frequency Feature Extraction：\n浅层特征可以捕获局部区域的细微差异，但随着模型越来越深，人脸的高级语义特征仍然包含原始人脸和操纵人脸的许多共同特征。高级语义信息对人脸检测有负面影响，可能导致模型过拟合而导致泛化性能差。所以将GLI模块插入双流网络的每个阶段来增强局部伪造线索并抑制RGB特征图上共享的高级语义信息。\n3. 实验 3.1 数据集和设置 类内实验：FF++ 未知数据集实验： Celeb-DF(V2)：590+5639 TIMIT：320+640 DFDCp：1131+4113 UADFV：49+49 未知伪造方法实验：GID-DF/GID-FF，在DF/FF上训练，其余部分测试 未知扰动实验：DeeperForensics-1.0 (DFo)，60000+ 实现细节： backbone：ViT(ImageNet-1K), CNN(randomly init). optimizer: Adam; lr:2e-5; weight decay=1e-7 trainset: FF++(C40) 3.2 模块分析 在FF++(C40)上训练，测试指标为帧级AUC。\n消融实验：测试结果和可视化结果如下图，w/o FSC表示使用所有频域信息进行训练而非中高频信息；w/o dual-branch表示只使用ViT；w/o GLI表示不使用GLI交互模块。可视化结果中(a), (b), (c)和(d)分别对应消融实验的四种情况，可见HFI-Net能够对操纵区域有较高的响应。\n不同层级的交互作用分析：下图分别是将模型分割为n个阶段和在哪些阶段设置GLI模块的消融实验。可见，将模型划分为4个阶段，并在每个阶段都设置GLI模块能达到最佳性能。\n不同频率成分分析：如下图，FCS方法提取中高频的方法由于其他频率选择方法，对于未知的测试场景，中高频的特征比单频（低，中，高）更具一般性。\n对2D DCT的分析：实验测试不同U(V)参数的性能。\n$$ F^{2d}_{u,v}=\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1}{x^{2d}_{i,j}B^{i,j}_{u,v}}\\\\\\ B^{i,j}_{u,v}=\\cos(\\frac{\\pi u}{U}(i+\\frac{1}{2}))\\cos(\\frac{\\pi v}{U}(j+\\frac{1}{2})) $$\n不同频率变换方式的分析：对比DWT（小波换换）和DCT，DCT更适合这个网络架构。DWT捕获频率信息的同时会对特征图进行下采样，可能会损害与伪造区域相关的注意力权重。\n思考：使用DWT时将FFR中的FcaNet修改为WaveNets?\n不同特征细化方式的分析（FFR）：对比SE-Net和CBAM，本文的FFR模块强调中高频的伪造线索，抑制图像在空域上的原始部分和在RGB特征上的高阶语义信息，提高了泛化性。\n不同融合方式的分析（fusion）：比较特征图fusion和最后输出的fusion。\n3.3 和近期工作的比较 和SOTA对比： 未知数据集泛化性测试：下图评价指标为Image-level AUC 未知伪造方法泛化性测试：Image-based Video-level AUC 未知扰动的鲁棒性测试：Image-based Video-level AUC 计算复杂度对比：Image-level AUC ","permalink":"https://Achilles-10.github.io/posts/paper/hfinet/","summary":"[paper] 1. 介绍 使用结合CNN和Transformer的双流网络来融合伪造人脸的局部和全局伪造信息，进而提高模型的泛化能力。同时使用高频信息在CNN和Transformer之间交互，实现更通用和鲁棒的伪造检测。 低频信息和中高频信息通常表现出不同的特征，其中低频分量主要包括图像的自然内容信","title":"Hierarchical Frequency-Assisted Interactive Networks for Face Manipulation Detection"},{"content":"颜色空间 RGB颜色空间 RGB（红绿蓝）是依据人眼识别的颜色定义出的空间，可表示大部分颜色。但在科学研究一般不采用RGB颜色空间，因为它的细节难以进行数字化的调整。它将色调，亮度，饱和度三个量放在一起表示，很难分开。它是最通用的面向硬件的彩色模型。RGB颜色空间适合于显示系统，不适合于图像处理。\nHSV颜色空间 HSV表达彩色图像的方式由三个部分组成：\nHue（色调，色相） Saturation（饱和度，色彩纯净度） Value（明度） 在HSV颜色空间下，比RGB更容易跟踪某种颜色的物体，常用与分割指定颜色的物体。\n用下面这个圆柱体来表示HSV颜色空间，圆柱体的横截面可以看做是一个极坐标系 ，H用极坐标的极角表示，S用极坐标的极轴长度表示，V用圆柱中轴的高度表示。\n在RGB颜色空间中，颜色由三个值共同决定，比如黄色为(255,255,0)，在HSV颜色空间中，黄色只有一个值决定，Hue=60。\n饱和度表示颜色接近光谱色的程度：饱和度越高，说明颜色越深，越接近光谱色；饱和度为0表示纯白色。\n明度决定颜色空间中颜色的明暗程度：明度越高，表示颜色越明亮；明度为0表示纯黑色（此时颜色最暗）。\nHLS 颜色空间 HLS颜色空间和HSV颜色空间比较类似，区别在于最后一个分量不同。HLS中的L表示Lightness（亮度），亮度为100表示白色，亮度为0表示黑色。HSV中的V表示明度，明度为100表示光谱色，明度为0表示黑色。\n提取白色物体时，使用HLS更方便，因为HSV中的H没有白色，需要由S和V共同决定（S=0，V=100）；在HLS中白色仅有亮度L一个分量决定。\nYUV/YCbCr YUV是通过亮度-色差来描述颜色的颜色空间。Y是亮度信号，色度信号由两个互相独立的信号组成，根据颜色系统和格式不同，色度信号被称作UV/PbPr/CbCr。在DVD中，色度信号被存储为Cb和Cr（C表示颜色，b蓝色，r红色）。\n改变颜色空间 颜色空间 考虑BGR$\\leftrightarrow$Gray，BGR$\\leftrightarrow$HSV和BGR$\\leftrightarrow$YCrCB颜色空间的转换。\ncv2.cvtColor(input_image, flag)函数用于颜色空间转换，flag决定转换的类型：\ncv2.COLOR_BGR2GRAY cv2.COLOR_BGR2HSV cv2.COLOR_BGR2YCR_CB 可以用以下命令获取其他标记：\nflags = [i for i in dir(cv2) if i.startswith(\u0026#39;COLOR_\u0026#39;)] print(flags) HSV的色相范围为[0,179]，饱和度为[0,255]，值域为[0,255]。不同软件使用不同的规模，若要将OpenCV的值和它们比较，需要做标准化操作。\n对象追踪 HSV比BGR颜色空间更容易表示颜色，可以使用HSV来提取有颜色的对象。以下代码尝试提取一个蓝色对象，步骤：截取视频的每一帧$\\rightarrow$转换到HSV颜色空间$\\rightarrow$设置蓝色范围的阈值$\\rightarrow$单独提取蓝色对象。\nimport cv2 import numpy as np cap = cv2.VideoCapture(0) while(1): # 读取帧 _, frame = cap.read() # 转换颜色空间 BGR 到 HSV hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 定义HSV中蓝色的范围 lower_blue = np.array([110,50,50]) upper_blue = np.array([130,255,255]) # 设置HSV的阈值使得只取蓝色 mask = cv2.inRange(hsv, lower_blue, upper_blue) # 将掩膜和图像逐像素相加 res = cv2.bitwise_and(frame,frame, mask= mask) cv2.namedWindow(\u0026#39;frame\u0026#39;, cv2.WINDOW_NORMAL) cv2.imshow(\u0026#39;frame\u0026#39;,frame) cv2.namedWindow(\u0026#39;mask\u0026#39;, cv2.WINDOW_NORMAL) cv2.imshow(\u0026#39;mask\u0026#39;,mask) cv2.namedWindow(\u0026#39;res\u0026#39;, cv2.WINDOW_NORMAL) cv2.imshow(\u0026#39;res\u0026#39;,res) k = cv2.waitKey(5) \u0026amp; 0xFF if k == 27: break cv2.destroyAllWindows() 在上面的HLS颜色空间示意图中测试白色的检测：\nimport cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(\u0026#34;hls.jpeg\u0026#34;) # Convert BGR to HLS imgHLS = cv2.cvtColor(img, cv2.COLOR_BGR2HLS) # range of white color in L channel # mask = cv2.inRange(imgHLS[:,:,1], lowerb=250, upperb=255) mask = cv2.inRange(imgHLS, np.array([0,250,0]), np.array([255,255,255])) # Apply Mask to original image white_mask = cv2.bitwise_and(img, img, mask=mask) 找到要追踪的HSV值 使用cv2.cvtColor(color,cv2.COLOR_BGR2HSV)，传递颜色而非图像。示例如下：\nimport cv2 import numpy as np red = np.uint8([[[0,0,255 ]]]) hsv_red = cv2.cvtColor(red,cv2.COLOR_BGR2HSV) print( hsv_red ) 如果想要同时追踪多种颜色，可以将多种颜色的掩码经过按位或操作得到新的掩码：\nmask = cv2.bitwise_or(mask_blue,mask_green,mask_red) ","permalink":"https://Achilles-10.github.io/posts/tech/opencv2/","summary":"颜色空间 RGB颜色空间 RGB（红绿蓝）是依据人眼识别的颜色定义出的空间，可表示大部分颜色。但在科学研究一般不采用RGB颜色空间，因为它的细节难以进行数字化的调整。它将色调，亮度，饱和度三个量放在一起表示，很难分开。它是最通用的面向硬件的彩色模型。RGB颜色空间适合于显示系统，不适","title":"OpenCV-Python学习笔记(2)：颜色空间"},{"content":"1. 说一下了解的激活函数和各自的应用场景 sigmoid: $$ \\sigma(x)=\\frac{1}{1+e^{-x}} $$ sigmoid是第一个被广泛应用于神级网络的激活函数，其值域为$[0,1]$，但是它存在输出均值不为0和梯度消失的问题，在深层网络中被其他激活函数替代。在逻辑回归中使用该激活函数用于输出分类。\ntanh: $$ tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2\\sigma(2x)-1 $$ tanh函数解决了sigmoid函数均值不为0的情况，值域为$[-1,1]$，但仍然存在梯度消失的问题。在LSTM中使用了tanh。\nReLU: $$ ReLU(x)= \\begin{cases} x,\\ x\\geq0\\\\\\ 0,\\ x\u0026lt;0\\ \\end{cases} $$ ReLU函数能有效避免梯度消失的问题，但在负值区域处于饱和状态（“死区”）。Alex-Net使用了ReLU，在使用深层网络时最好使用ReLU而不是sigmoid。\nLeaky ReLU： $$ Leaky\\ ReLU(x)= \\begin{cases} x,\\ x\\geq0\\\\\\ \\alpha\\cdot x,\\ x\u0026lt;0 \\end{cases} $$ Leaky ReLU在负值区添加了一个斜率参数，缓解了饱和性问题（“死区”）。但缺点是超参数$\\alpha$的合适值不好设定，当我们想让神经网络学习到负值区的信息时可以使用该函数。\n参数化ReLU(P-ReLU)：解决超参数$\\alpha$不好设定的问题，将其作为模型参数融入到模型的训练过程中，在反向传播时更新参数。\n随机化ReLU(R-ReLU)：随机化超参数$\\alpha$，使不同的层学习不同的参数。其随机化参数的分布符合均匀分布或高斯分布。\nELU： $$ ELU(x)= \\begin{cases} x,\\ x\\geq0\\\\\\ \\lambda\\cdot(e^x-1),\\ x\u0026lt;0 \\end{cases} $$ 解决饱和性问题，但缺点是指数计算量大。\nGELU： $$ GELU(x)=x\\text{P}(\\text{X}\\leq x)=x\\Phi(x) $$ ​\t其中$\\Phi(x)$是正态分布的概率函数，计算时近似计算的数学公式如下： $$ GELU(x)=\\frac{1}{2}x(1+tanh[\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3)]) $$\n2. 为什么需要激活函数？ 在线性模型中引入非线性激活函数，可以使线性模型非线性化，提高模型的非线性表达能力，也就是拟合能力。\n3. 激活函数的特征？ 非线性性 几乎处处可微 计算简单 单调性：符号不变容易收敛 非饱和性：饱和指在某些区间的梯度接近零，即梯度消失，使得参数无法继续更新 输出范围有限 接近恒等变换 参数少 4. Leaky ReLU相对于ReLU的优势在哪？ Leaky ReLU在负值增加了一个斜率$\\alpha$，缓解了ReLU在$x\u0026lt;0$时的饱和性问题(\u0026ldquo;死区\u0026rdquo;，梯度消失)，但Leaky ReLU得超参数$\\alpha$的合适值不好设定。\n当我们想让神经网络能够学到负值信息时可以使用该激活函数。\n5. 什么是ReLU6？ ReLU的值域为$[0,\\infty]$，在实际应用中需要限定输出的最大值，将输出在6处截断，即为ReLU6。\n6. Sigmoid函数有什么缺点？怎么解决？ 缺点：输出均值不为0，存在梯度消失的情况。\n解决办法：\n用ReLU，Leaky ReLU等其他激活函数代替 采用适合的权重初始化方法，如He_init 在分类问题中，sigmoid作为激活函数时，用交叉熵损失函数替代MSE 加入BN层 分层训练权重 7. ReLU在零点可导吗？如何进行反向传播？ 不可导，可以人为的将零点梯度规定为0。\ncaffe源码~/caffe/src/caffe/layers/relu_layer.cpp倒数第十行代码如下：\nbottom_diff[i] = top_diff[i] * ((bottom_data[i] \u0026gt; 0)+ negative_slope * (bottom_data[i] \u0026lt;= 0)); 可见，间断点（$\\leq0$）处的导数为negtive_slope（默认为0）。\n8. Softmax的溢出问题怎么解决？ 由于Softmax的指数运算，可能导致溢出问题。\n令$M=\\max(x_i)$，将计算$f(x_i)$转换为计算$f(x_i-M)$的值，就可以解决溢出问题了，且理论上计算结果与计算$f(x_i)$保持一致，该操作类似与Min-Max归一化。\n9. 推导Sigmoid的求导公式 sigmoid公式如下：\n$$ \\sigma(z)=\\frac{1}{1+e^{-z}} $$\n求导公式推导如下：\n$$ \\begin{align}\\sigma'(z)\u0026=(\\frac{1}{1+e^{-z}})'\\\\\u0026=(e^{-z})\\cdot\\frac{1}{(1+e^{-z})^2}\\\\\u0026=\\frac{1}{1+e^{-z}}\\cdot\\frac{e^{-z}}{1+e^{-z}}\\\\\u0026=\\frac{1}{1+e^{-z}}\\cdot(1-\\frac{1}{1+e^{-z}})\\\\\u0026=\\sigma(z)\\cdot(1-\\sigma(z))\\end{align} $$ 10. 推导Softmax的求导公式 softmax公式如下： $$ s(z_i)=\\frac{e^{z_i}}{\\sum_{k=1}^{n}{e^{z_k}}} $$ 求导公式推导如下：\n当$j=i$时： $$ \\begin{align} \\frac{\\partial s_i}{\\partial z_i} \u0026=\\frac{\\partial(\\frac{e^{z_i}}{\\sum_{k=1}^{n}{e^{z_k}}})}{\\partial z_i}\\\\ \u0026=\\frac{e^{z_i}\\cdot\\sum{e^{z_k}}-(e^{z_i})^2}{(\\sum{e^{z_k}})^2}\\\\ \u0026=\\frac{e^{z_i}}{\\sum{e^{z_k}}}\\cdot\\frac{\\sum{e^{z_k}}-e^{z_i}}{\\sum{e^{z_k}}}\\\\ \u0026=\\frac{e^{z_i}}{\\sum{e^{z_k}}}\\cdot(1-\\frac{e^{z_i}}{\\sum{e^{z_k}}})\\\\ \u0026=s_i(1-s_i) \\end{align} $$ 当$j\\neq i$时： $$ \\begin{align} \\frac{\\partial s_j}{\\partial z_i} \u0026=\\frac{\\partial(\\frac{e^{z_j}}{\\sum_{k=1}^{n}{e^{z_k}}})}{\\partial z_i}\\\\ \u0026=e^{z_j}\\cdot-(\\frac{1}{\\sum{e^{z_k}}})^2\\cdot e^{z_i}\\\\ \u0026=-\\frac{e^{z_j}}{\\sum{e^{z_k}}}\\cdot\\frac{e^{z_i}}{\\sum{e^{e_k}}}\\\\ \u0026=-s_j\\cdot s_i \\end{align} $$ p.s.基本初等函数的求导公式与法则 ","permalink":"https://Achilles-10.github.io/posts/tech/activation/","summary":"1. 说一下了解的激活函数和各自的应用场景 sigmoid: $$ \\sigma(x)=\\frac{1}{1+e^{-x}} $$ sigmoid是第一个被广泛应用于神级网络的激活函数，其值域为$[0,1]$，但是它存在输出均值不为0和梯度消失的问题，在深层网络中被其他激活函数替代。在逻辑回归中使用该激活函数用于输出分类。 tanh: $$ tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2\\sigma(2x)-1 $$ tanh函数解决了sigmoid函","title":"深度学习面试题：激活函数"},{"content":"[paper] [code]\n1. 引言 Transformer 取代了以往递归神经网络为主导的骨干架构，随着ViT的引入，彻底改变了网络架构设计的格局。但ViT的全局注意力机制对输入大小的复杂度过高，难以处理高分辨率的输入。\n层级Transformer采用混合方法来解决这个问题，例如Swin Transformer采用了“滑动窗口”策略，也说明了卷积仍然非常受欢迎。本文目标是为卷积网络弥补前ViT时代和后ViT时代的差距，并测试纯卷积网络可以达到的极限。\n2. Modernizing a ConvNet: a Roadmap（研究路线图） 以ResNet-50作为baseline，考虑以下几种设计决策：\nmarco design（宏观设计） ResNeXt inverted bottlenect（倒置瓶颈） large kernel size（更大的卷积核） various layer-wise micro designs（多样的分层微设计） 2.1 训练技巧 epoch: 90-\u0026gt;300 optimizer: AdamW data augmentation: Mixup, Cutmix, RandAugment, RandomErasing\u0026hellip; regularization: Stochastic Depth, Label Smoothing 2.2 Marco Design（宏观设计） 改变阶段计算比：Swin-T的阶段计算比为1:1:3:1，更大型的Swin的阶段计算比为1:1:9:1。对此，将ResNet-50中的(3,4,6,3)改为 (3,3,9,3)，使模型准确率从78.8%提升至79.4%。 将stem改为\u0026quot;Patchify\u0026quot;（非重叠的卷积）：标准的ResNet中stem为(k=7,p=3,s=2)的卷积后跟一个(k=3,p=1,s=2)的最大池化，这导致输入图像的4倍下采样。将其更换为 (k=4,s=4)的卷积，模型准确率从79.4%提升至79.5%。 2.3 ResNeXt-ify 采用深度可分离卷积，使得每个操作单独混合空间或通道的信息。使用分组卷积(depthwise conv)能够降低网络的FLOPs，但也会降低准确率(78.3%)。将网络宽度从64扩展到96，准确率提升到80.5%。\n2.4 Inverted Bottlenect（倒置瓶颈） Transformer中的MLP的隐藏维度比输入维度大4倍（384:96），这就是倒置瓶颈。对倒置瓶颈的探索如下图(a)(b)，这使得准确率提升(80.5%-\u0026gt;80.6%)的同时降低了FLOPs(下采样残差1x1卷积的FLOPs减少)。\n2.5 Large Kernel Sizes（大卷积核） VGG推广的黄金标准是堆叠3x3的小卷积核，这在现代化GPU上更高效，但Swin中的窗口大小至少为7x7。\n上移分组卷积层：如上图(b)(c)，使复杂低效的模块(MSA)有更少的通道数，降低FLOPS至4.1G，性能暂时下降到79.9%。 增大卷积核：将卷积核大小从3x3增大到7x7，FLOPs大致保持不变，准确率提升至80.6%。当继续增大卷积核时并没有带来更大准确率增益。 2.6 Micro Design（微观设计） 将ReLU更换为GELU：准确率不变 更少的激活函数：如下图所示，复制Swin的样式，将残差块中的激活函数去掉，去掉两个卷积层中的一个激活函数，准确度提升至81.3%。 更少的归一化层：去掉两个归一化层，在1x1卷积前只留下一个BN层，准确率提升到81.4%，超过Swin。 将BN替换为LN：BN能够加速收敛并减少过拟合，但BN错综复杂，可能对模型的性能产生不利影响。在ResNet中直接将BN替换为LN会导致性能不佳，但随着对网络结构和训练技巧的修改，使用LN将准确率提升至81.5%。 可分离的下采样层：ResNet中的下采样是通过每个阶段开始时的残差块实现的。Swin中添加了一个单独的下采样层。本文用单独的(k=2,s=2)卷积实现下采样，后续实验发现在分辨率变化的地方添加归一化层有助于稳定训练，这时准确率达到82.0%。 3. 在ImageNet上的评估 构建了不同的ConvNeXt变体：\nConvNeXt-T: C =(96, 192, 384, 768), B =(3, 3, 9, 3) ConvNeXt-S: C =(96, 192, 384, 768), B =(3, 3, 27, 3) ConvNeXt-B: C =(128, 256, 512, 1024), B =(3, 3, 27, 3) ConvNeXt-L: C =(192, 384, 768, 1536), B =(3, 3, 27, 3) ConvNeXt-XL: C =(256, 512, 1024, 2048), B =(3, 3, 27, 3) 3.1 结果 ImageNet-1K：\nImageNet-22K 预训练，ImageNet-1K 微调：\n3.2 Isotropic ConvNeXt vs. ViT（同质性比较） 同质架构（Isotropic architecture）：同质架构模型没有下采样层，在所有深度都保持相同的特征图分辨率，只需要用特征大小（即patch embedding的维度）和网络深度（即blocks数量）两个参数定义。\nConvNeXt的性能同ViT相当，说明ConvNeXt块设计在用于非层级模型时具有竞争力。\n4. 在下游任务上的评估 4.1 COCO数据集上的目标检测和分割 4.2 ADE20K上的语义分割 4.3 关于模型效率的评论 5. 总结 ConvNeXt模型本身不是全新的，里面的许多设计都被单独测试过，但没有放在一起测试过。ConvNeXt的实验结果是优秀的，在多个计算机视觉基准测试中与最先进的层级Transformer竞争的同时，还保留着标准卷积网络的简单性和效率。\n","permalink":"https://Achilles-10.github.io/posts/paper/convnext/","summary":"[paper] [code] 1. 引言 Transformer 取代了以往递归神经网络为主导的骨干架构，随着ViT的引入，彻底改变了网络架构设计的格局。但ViT的全局注意力机制对输入大小的复杂度过高，难以处理高分辨率的输入。 层级Transformer采用混合方法来解决这个问题，例如Swin Transformer采用了“滑动窗口”策","title":"A ConvNet for the 2020s"},{"content":"图像入门 读取图像 使用cv2.imread(filename,flags)函数读取图像，参数说明如下：\nfilename - 待读取图像的路径\nflags - 读取图像的方式\ncv2.IMREAD_COLOR - 加载彩色图像，图像的透明度会被忽略，默认标志 cv2.IMREAD_GRAYSCALE - 以灰度模式加载图像 cv2.IMREAD_UNCHANGED - 加载图像，不会忽略透明度 可以分别传递整数1，0，-1\nimport numpy as np import cv2 img = cv2.imread(\u0026#39;face.png\u0026#39;,0) 即使图像路径错误，也不会报错，但print(img)会输出None\n写入图像 使用cv2.imwrite(filename,image)函数保存图像，参数说明如下：\nfilename - 保存的文件名 image - 要保存的图像 cv2.imwrite(\u0026#39;save.png\u0026#39;, img) 使用Matplotlib显示图像 from matplotlib import pyplot as plt plt.imshow(img, cmap = \u0026#39;gray\u0026#39;, interpolation = \u0026#39;bicubic\u0026#39;) plt.xticks([]), plt.yticks([]) # 隐藏 x 轴和 y 轴上的刻度值 plt.show() 图像的基本操作 访问和修改像素值 可以通过行列坐标来访问和修改像素值。对于BGR图像，返回一个[BLUE值 GREEN值 RED值]数组；对于灰度图像，只返回对应的灰度。\n\u0026gt;\u0026gt;\u0026gt; px=img[100,100] \u0026gt;\u0026gt;\u0026gt; print(px) [237 217 186] # 访问BLUE值 \u0026gt;\u0026gt;\u0026gt; blue = img[100,100,0] \u0026gt;\u0026gt;\u0026gt; print(blue) 237 \u0026gt;\u0026gt;\u0026gt; img[100,100]=[255,255,255] \u0026gt;\u0026gt;\u0026gt; print(img[100,100]) [255 255 255] 上述方法用于选择数组的区域。对于单像素访问，Numpy数组方法array.item()和array.itemset()返回标量，相对更好。\n\u0026gt;\u0026gt;\u0026gt; img.item(100,100,2) 255 \u0026gt;\u0026gt;\u0026gt; # 修改RED值 \u0026gt;\u0026gt;\u0026gt; img.itemset((10,10,2),99) \u0026gt;\u0026gt;\u0026gt; img.item(10,10,2) 99 访问图像属性 图像属性包括行列通道数，数据类型和像素数等。\n图像的形状由img.shape访问，返回行列通道数的元组，如果图像是灰度的，返回值仅包括行列数。\n像素总数由img.size访问，图像数据类型由img.dtype访问。\n\u0026gt;\u0026gt;\u0026gt; print(\u0026#39;shape:\u0026#39;,img.shape) shape: (512, 512, 3) \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;size:\u0026#39;,img.size) size: 786432 \u0026gt;\u0026gt;\u0026gt; print(\u0026#39;dtype:\u0026#39;,img.dtype) dtype: uint8 img.dtype在调试时很重要，因为OpenCV代码中的大量错误是由无效的数据类型引起的。\n图像感兴趣区域(Region of Interest, ROI) 用Numpy获取ROI，例如将图像中的人脸复制到图像的另一个区域\n\u0026gt;\u0026gt;\u0026gt; face = img[10:300,210:430] \u0026gt;\u0026gt;\u0026gt; img[0:290,0:220]=face 拆分和合并图像通道 有时候需要分别处理图像的B, G, R通道，或者将单独的通道加入到BGR图像，在这种情况下需要拆分或合并图像通道。\n\u0026gt;\u0026gt;\u0026gt; b,g,r=cv2.split(img) \u0026gt;\u0026gt;\u0026gt; img=cv2.merge((b,g,r)) 或者采用Numpy索引，例如将所有红色值设置为零：\n\u0026gt;\u0026gt;\u0026gt; img[:,:,2]=0 cv2.split()是一个耗时的操作，非必要时使用Numpy索引。\n为图像设置边框（填充） 可以使用copyMakeBorder(src, top, bottom, left, right, borderType[, dst[, value]]) -\u0026gt; dst在图像周围创建边框，该函数在卷积运算，零填充等方面有更多应用。参数说明如下：\nsrc - 输入图像 top, bottom, left, right - 边界宽度 borderType - 边框类型标志，可以是一下类型： cv2.BORDER_CONSTANT - 添加恒定的彩色边框，该值由下一个参数给出 cv2.BORDER_REFLECT - 边框是边框元素的镜像 cv2.BORDER_REFLECT_101或cv2.BORDER_DEFAULT与上述相同，但略有变化 cv2.BORDER_REPLICATE - 最后一个元素被复制 cv2.BORDER_WRAP value - 边框的颜色，如果边框类型为cv2.BORDER_CONSTANT import cv2 import numpy as np from matplotlib import pyplot as plt BLUE = [255,0,0] img1 = cv2.imread(\u0026#39;face.png\u0026#39;) img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) replicate = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REPLICATE) reflect = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REFLECT) reflect101 = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_REFLECT_101) wrap = cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_WRAP) constant= cv2.copyMakeBorder(img1,10,10,10,10,cv2.BORDER_CONSTANT,value=BLUE) plt.subplot(231),plt.imshow(img1,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;ORIGINAL\u0026#39;) plt.subplot(232),plt.imshow(replicate,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REPLICATE\u0026#39;) plt.subplot(233),plt.imshow(reflect,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REFLECT\u0026#39;) plt.subplot(234),plt.imshow(reflect101,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;REFLECT_101\u0026#39;) plt.subplot(235),plt.imshow(wrap,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;WRAP\u0026#39;) plt.subplot(236),plt.imshow(constant,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;CONSTANT\u0026#39;) plt.show() 图像上的运算 图像加法 可以通过cv2.add()或Numpy操作res=img1+img2完成图像加法操作。相加的图像应该具有相同的深度和类型，或者第二个图像是一个标量值。\nOpenCV加法是饱和运算，Numpy加法是模运算。\n\u0026gt;\u0026gt;\u0026gt; x=np.uint8([250]) \u0026gt;\u0026gt;\u0026gt; y=np.uint8([10]) \u0026gt;\u0026gt;\u0026gt; print(cv2.add(x,y)) [[255]] \u0026gt;\u0026gt;\u0026gt; print(x+y) [4] 当添加两个图像时，尽量使用OpenCV的功能，能提供更好的结果。\n图像融合 这也是图像加法，但是对相加的图像赋予给定的权重，使其具有融合或透明的感觉。\n$$ G(x)=(1-\\alpha)f_0(x)+\\alpha f_1(x) $$\n将$\\alpha$从$0\\rightarrow1$更改，可以实现图像过渡的效果。cv2.addWeighted()在图像上应用以下公式：\n$$ dst=\\alpha\\cdot img_1+\\beta\\cdot img_2+\\gamma $$\n在这里，$\\gamma$ 被视为零。\nimg1 = cv2.imread(\u0026#39;face1.png\u0026#39;) img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2 = cv2.imread(\u0026#39;face2.png\u0026#39;) img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) dst = cv2.addWeighted(img1,0.6,img2,0.4,0) plt.figure(figsize=(15, 9)) plt.subplot(131),plt.imshow(img1),plt.title(\u0026#39;IMG1\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(132),plt.imshow(img2),plt.title(\u0026#39;IMG2\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(133),plt.imshow(dst),plt.title(\u0026#39;DST\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() 按位运算 按位运算包括AND,OR,NOT,XOR操作。这些操作在提取图像的任意部分、定义和处理非矩形ROI方面非常有用。下面的例子是想把OpenCV的标志放到一个图像上。\n如果我们直接使用图像加法，它会改变颜色，无法达到我们想要的效果；如果使用图像融合，会得到一个透明的效果，也不是我们想要的效果。如果是一个矩形区域，我们可以使用ROI，但OpenCV的标志并不是矩形的，故可以用按位操作来实现：\n# 加载两张图片 img1 = cv2.cvtColor(cv2.imread(\u0026#39;shuyi.png\u0026#39;),cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(\u0026#39;logo.jpg\u0026#39;),cv2.COLOR_BGR2RGB) # 我想把logo放在左上角，所以我创建了ROI rows,cols,channels = img2.shape roi = img1[0:rows, 0:cols] # 现在创建logo的掩码，并同时创建其相反掩码 img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY) ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY) mask_inv = cv2.bitwise_not(mask) # 现在将ROI中logo的区域涂黑 img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv) # 仅从logo图像中提取logo区域 img2_fg = cv2.bitwise_and(img2,img2,mask = mask) # 将logo放入ROI并修改主图像 dst = cv2.add(img1_bg,img2_fg) img1[0:rows, 0:cols ] = dst plt.figure(figsize=(9, 6)) plt.subplot(121),plt.imshow(mask,\u0026#39;gray\u0026#39;),plt.title(\u0026#39;MASK\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.subplot(122),plt.imshow(img1),plt.title(\u0026#39;RESULT\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() 在计算mask时用到了cv2.bitwise_not(src,dst=None,mask=None)函数，在计算前景和背景区域时用到了cv2.bitwise_and(src1,src2,dst=None,mask=None)，参数说明如下：\nsrc1 - 参与运算的图像 src2 - 参与运算的图像 dst - 可选运算结果输出数组 mask - 可选操作掩码 threshold(src, thresh, maxval, type[, dst])-\u0026gt;ret,dst函数的作用是将一幅灰度图二值化，参数说明如下：\nsrc - 输入的灰度图\nthresh - 阈值\nmaxval - 最大值\ntype - 阈值类型\n阈值类型 灰度值大于阈值(val\u0026gt;threshold) 其他情况 cv2.THRESH_BINARY maxval 0 cv2.THRESH_BINARY_INV 0 maxval cv2.THRESH_TRUNC thresh 当前灰度值 cv2.THRESH_TOZERO 当前灰度值 0 cv2.THRESH_TOZERO_INV 0 当前灰度值 性能衡量和提升技术 使用OpenCV衡量性能 cv2.getTickCount()函数返回从参考时间到调用此函数时的时钟周期数，可以在函数执行前后调用它获得执行函数的时钟周期数。\ncv2.getTickFrequency()函数返回时钟周期的频率或者每秒的时钟周期数。下列代码可以获得执行函数所用时间（以秒为单位）。\ne1 = cv2.getTickCount() # 你的执行代码 e2 = cv2.getTickCount() time = (e2 - e1)/ cv2.getTickFrequency() 也可以使用两次time.time()函数，取两次的差来获得函数所用时间。\nOpenCV中的默认优化 OpenCV默认运行优化的代码。可以使用cv2.useOptimized()来检查是否启用优化，使用cv2.setUseOptimized(bool)来启用/禁用优化。\ncv2.setUseOptimized(True) print(cv2.useOptimized()) %timeit res = cv2.medianBlur(img1,59) cv2.setUseOptimized(False) print(cv2.useOptimized()) %timeit res = cv2.medianBlur(img1,59) True 35 ms ± 2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) False 36.5 ms ± 2.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 在IPython或者Jupyter中衡量性能 使用%timeit。实例如下：\nx=5 z=np.uint8([5]) %timeit y=x**2 %timeit y=z*z %timeit y=x**x %timeit y=np.square(z) 273 ns ± 13.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 659 ns ± 37.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 334 ns ± 10.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 646 ns ± 48.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 可以看到，Python的标量操作比Numpy的标量操作快，对于包含一两个元素的运算，Python标量比Numpy数组好，当数组大小稍大时Numpy会占优势。\n下面测试cv2.countNonZero()函数和np.count_nonzero()函数对于同一张图片的性能。\nimg = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY) %timeit z=cv2.countNonZero(img) %timeit z=np.count_nonzero(img) 13.5 µs ± 481 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 23.2 µs ± 970 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) 可以看到OpenCV函数比Numpy函数快。\n性能优化技术 尽量避免在Python中使用循环，特别是双重/三重循环等，因为它们本质上速度较慢。 最大程度地向量化算法/代码，因为Numpy和OpenCV针对向量运算进行了优化。 利用缓存一致性。 尽量不要复制数组，因为数组复制是一项代价昂贵的操作。 参考资源 Python优化技术 Scipy讲义-高级Numpy IPython中的时序和性能分析 ","permalink":"https://Achilles-10.github.io/posts/tech/opencv1/","summary":"图像入门 读取图像 使用cv2.imread(filename,flags)函数读取图像，参数说明如下： filename - 待读取图像的路径 flags - 读取图像的方式 cv2.IMREAD_COLOR - 加载彩色图像，图像的透明度会被忽略，默认标志 cv2.IMREAD_GRAYSCALE - 以灰度模式加载图像 cv2.IMREAD_UNCHANGED - 加载图像，不会忽略透明度 可以分别传递整数1，0，-1 import numpy as np import cv2 img =","title":"OpenCV-Python学习笔记(1)：核心操作"},{"content":"[paper] [code]\n动机与介绍 已有方法对跨域数据集和高压缩高曝光数据的检测能力大幅下降(泛化性差)；\n难以识别的fake样本通常包含更一般伪造痕迹，故要学习更通用和鲁棒的面部伪造表征；\n定义了四种常见的伪影(artifacts)： 主要贡献 提出了source-target generator (STG) and mask generator (MG)来学习更一般鲁棒的人脸伪造表征 通过自换脸而非寻找最接近的landmark换脸，降低了计算成本 在cross-dataset和cross-maniputation测试中都取得了SOTA 方法 学习伪造人脸与背景的不一致分为下列三个模块\nSource-Target Generator(STG):\n对source和target进行数据增强以产生不一致，并且对source进行resize和translate以再现边界混合和landmarks不匹配； 首先对Target和Source之一做图像增强 (color：RGB channels, hue, saturation, value, brightness, and contrast；frequency：downsample or sharpen)； 然后对source进行裁剪：$H_r=u_hH,\\quad W_r=u_wW$,其中$\\ u_h和u_w$是一组均分分布中的随机值，再对裁剪后的图像zero-padded 或者 center-cropped还原回初始大小； 最后对source做变形(translate)：traslate vector$\\ t=[t_h,t_w]$,$\\ t_h=v_hH,t_w=v_wW$，$v_h和v_w$是一组均分分布中的随机值。 Mask Generator: 生成变形的灰度mask图\n计算面部landmarks的convex hull来初始化mask，然后对mask变形(elastic deformation)，在用两个不同参数的高斯滤波器(gaussian filter)对mask进行平滑处理。最后在{0.25, 0.5, 0.75, 1, 1, 1}中选取混合指数(blending ration)； Blending: 用Mask来混合source和target图得到SBI\n$$I_{SB}=I_s\\odot M+I_t\\odot(1-M)$$\nTrain with SBIs: 将target而非原图作为”REAL“，使得模型集中在伪造痕迹上\n实验 实现细节 预处理：Dlib和RetinaFace裁帧，面部区域裁剪：4~20%(训练),12.5%(推理)； Source-Target Augmentation：RGBShift, HueSaturationValue, RandomBrightnessContrast, Downscale, and Sharpen 推理策略：如果在一帧中检测到两个或多个人脸，则将分类器应用于所有人脸，并将最高的虚假置信度用作该帧的预测置信度。 实验设定：各类baseline 跨数据集评估 跨操作评估 定量分析 消融实验 定性分析 局限性 缺乏时序信息、无法解决GAN生成的伪造图像\n","permalink":"https://Achilles-10.github.io/posts/paper/sbi/","summary":"[paper] [code] 动机与介绍 已有方法对跨域数据集和高压缩高曝光数据的检测能力大幅下降(泛化性差)； 难以识别的fake样本通常包含更一般伪造痕迹，故要学习更通用和鲁棒的面部伪造表征； 定义了四种常见的伪影(artifacts)： 主要贡献 提出了source-target generator (STG) and mask generator (MG)来学习更一般","title":"Detecting Deepfakes with Self-Blended Images"},{"content":" 英文名: Achilles Zhang 职业: 学生 爱好: 篮球、健身、Dota2 个性签名: 垒山不止就是幸福 ","permalink":"https://Achilles-10.github.io/about/","summary":"英文名: Achilles Zhang 职业: 学生 爱好: 篮球、健身、Dota2 个性签名: 垒山不止就是幸福","title":"关于"}]