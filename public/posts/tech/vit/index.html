<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>速通 Vision Transformer (ViT) | 烈烈风中、的博客</title>
<meta name="keywords" content="学习笔记, 模型">
<meta name="description" content="图解 ViT 网络结构与计算，以及面试常见问题">
<meta name="author" content="Achilles">
<link rel="canonical" href="https://Achilles-10.github.io/posts/tech/vit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5f0dd219ed8bdc295d1cda8e0687931360db45762f55d74830834defe744a8a6.css" integrity="sha256-Xw3SGe2L3CldHNqOBoeTE2DbRXYvVddIMINN7&#43;dEqKY=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
        onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Achilles-10.github.io/img/profile.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://Achilles-10.github.io/img/profile.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://Achilles-10.github.io/img/profile.jpg">
<link rel="apple-touch-icon" href="https://Achilles-10.github.io/img/profile.jpg">
<link rel="mask-icon" href="https://Achilles-10.github.io/img/profile.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script defer src="https://unpkg.com/mermaid@8.8.1/dist/mermaid.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<meta property="og:title" content="速通 Vision Transformer (ViT)" />
<meta property="og:description" content="图解 ViT 网络结构与计算，以及面试常见问题" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Achilles-10.github.io/posts/tech/vit/" />
<meta property="og:image" content="https://Achilles-10.github.io/posts/tech/vit/vit.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-25T19:21:03+08:00" />
<meta property="article:modified_time" content="2023-07-25T19:21:03+08:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://Achilles-10.github.io/posts/tech/vit/vit.png" />
<meta name="twitter:title" content="速通 Vision Transformer (ViT)"/>
<meta name="twitter:description" content="图解 ViT 网络结构与计算，以及面试常见问题"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "📚 文章",
          "item": "https://Achilles-10.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  3 ,
          "name": "👨🏻‍💻 技术",
          "item": "https://Achilles-10.github.io/posts/tech/"
        }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "速通 Vision Transformer (ViT)",
      "item": "https://Achilles-10.github.io/posts/tech/vit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "速通 Vision Transformer (ViT)",
  "name": "速通 Vision Transformer (ViT)",
  "description": "图解 ViT 网络结构与计算，以及面试常见问题",
  "keywords": [
    "学习笔记", "模型"
  ],
  "articleBody": "引言 将图像拆分为块（patch），并将这些图像块的线性嵌入序列（patch token）作为 Transformer 输入。\n当在没有强正则化的中型数据集（如 ImageNet）上进行训练时，这些模型产生的准确率比同等大小的 ResNet 低几个百分点。 这种看似令人沮丧的结果可能是意料之中的：Transformers 缺乏 CNN 固有的一些归纳偏置 (inductive biases) ，如平移等效性和局部性 （translation equivariance and locality），因此在数据量不足时，训练不能很好地泛化。\n但若在更大的数据集（14M-300M图像）上训练，情况就会发生变化，我们发现大规模训练胜过归纳偏置。\n归纳偏置：学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合\n例如，深度神经网络偏好性地认为，层次化处理信息有更好效果；卷积神经网络认为信息具有空间局部性，可用滑动卷积共享权重的方式降低参数空间；循环神经网络则将时序信息纳入考虑，强调顺序重要性；图网络则认为中心节点与邻居节点的相似性会更好地引导信息流动。\nCNN 平移不变性：卷积+最大池化约等于平移不变性\n如下面两图，输入图像的左下角有一个人脸，经过卷积，人脸的特征（眼睛，鼻子）也位于特征图的左下角。假如人脸特征在图像的左上角，那么卷积后对应的特征也在特征图的左上角。\n最大池化，它返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。\nCNN 局部性：如果我们想要识别出与物体相对应的图案，如天空中的一架飞机，我们可能需要看看附近的像素是如何排列的，但我们对那些彼此相距很远的组合的像素是如何出现的并不那么感兴趣。\n方法 图像块嵌入（Patch Embeddings） 图像 $X\\in\\mathbb{R}^{H\\times W\\times C}$经过分块得到 2D patch 序列 $x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，其中 $N=\\frac{H\\cdot W}{P^2}$ 为 ViT 有效输入序列长度。\nViT 在所有层中使用相同维度的隐向量大小 D，将 $x_p$经过 FC 层将 $P^2\\cdot C$ 映射到 D，同时 N 保持不变。\n上述即为图像块嵌入，本质即为对每一个图像块向量做一个线性变换，将其映射到 D 维，得到 $x_pE\\in\\mathbb{R}^{N\\times D}$。\nnum_patches = (image_height // patch_height) * (image_width // patch_width) patch_dim = channels * patch_height * patch_width to_patch_embedding = nn.Sequential( Rearrange('b c (h p1) (w p2) -\u003e b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width), nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim), nn.LayerNorm(dim), ) x = to_patch_embedding(x) 可学习的嵌入（Learnable Embedding） 为图像块嵌入序列手动预设一个可学习的嵌入 cls token，最后取追加的 cls token 作为输出。最终输入 ViT 的嵌入向量总长度为 N+1，cls token 在训练时随机初始化。\ncls_token = nn.Parameter(torch.randn(1, 1, dim)) cls_tokens = repeat(cls_token, '1 1 d -\u003e b 1 d', b = b) x = torch.cat((cls_tokens, x), dim=1) 位置嵌入（Position Embeddings） 位置嵌入 $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$被加入图像块嵌入以保留输入图像块之间的空间位置关系，这是由于自注意力的扰动不变性（Permutation-invariant），即打乱 Sequence 中 tokens 的顺序并不会改变结果。若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。\n论文中比较了以下几种不同的位置编码方案：\n无位置嵌入 1-D 位置编码嵌入：将 2D 图像块视作 1D 序列 2-D 位置编码嵌入：考虑图像块 2-D 位置 (x,y) 相对位置编码嵌入：考虑图像块的相对位置 其中无位置嵌入效果很差，其他效果接近。Transformer 中采用固定位置编码，ViT 采用标准可学习的 1-D 位置编码嵌入。\npos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) dropout = nn.Dropout(emb_dropout) x += self.pos_embedding[:, :(n + 1)] x = self.dropout(x) 最终输入： $$ x_0=[x_{cls};x_p^1E;x_p^2E;\\dots;x_p^NE]+E_{pos},\\quad x_0\\in\\mathbb{R}^{(N+1)\\times D} $$\nTransformer 编码器 Transformer 编码器由交替的多头自注意力层和多层感知机块构成，每个块前应用 LayerNorm，每个块后残差连接。\n多头自注意力（MSA）机制中的 QKV 矩阵由嵌入向量 X 与三个不同权重的矩阵相乘得到。\nQ 与 K 转置相乘，然后使用 Softmax 计算每一个单词对于其他单词的注意力系数，公式中的 Softmax 是对每一行计算，即每一行的和为 1。\n将 Softmax 矩阵与 V 矩阵相乘得到输出 $Z_i$，最后将每一头的输出 Z 拼接在一起，经过一个线性层得到最终输出 Z。\n公式表达如下： $$ \\begin{align*} \u0026Q,K,V=MatMul(X,(W^Q,W^K,W^V))\\\\ \u0026\\text{head}_i=\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})\\cdot V\\\\ \u0026\\text{MultiHead}(Q,K,V)=Cat(\\text{head}_1,\\text{head}_2,\\dots,\\text{head}_h)\\cdot W^o \\end{align*} $$\nMSA 后面跟一个多层感知机（MLP），将特征维度先增大再恢复，其中使用 GELU 激活函数。\nclass PreNorm(nn.Module): def __init__(self, dim, fn): super().__init__() self.norm = nn.LayerNorm(dim) self.fn = fn def forward(self, x, **kwargs): return self.fn(self.norm(x), **kwargs) class FeedForward(nn.Module): def __init__(self, dim, hidden_dim, dropout = 0.): super().__init__() self.net = nn.Sequential( nn.Linear(dim, hidden_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout) ) def forward(self, x): return self.net(x) class Attention(nn.Module): def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.): super().__init__() inner_dim = dim_head * heads project_out = not (heads == 1 and dim_head == dim) self.heads = heads self.scale = dim_head ** -0.5 self.attend = nn.Softmax(dim = -1) self.dropout = nn.Dropout(dropout) self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False) self.to_out = nn.Sequential( nn.Linear(inner_dim, dim), nn.Dropout(dropout) ) if project_out else nn.Identity() def forward(self, x): qkv = self.to_qkv(x).chunk(3, dim = -1) q, k, v = map(lambda t: rearrange(t, 'b n (h d) -\u003e b h n d', h = self.heads), qkv) dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale attn = self.attend(dots) attn = self.dropout(attn) out = torch.matmul(attn, v) out = rearrange(out, 'b h n d -\u003e b n (h d)') return self.to_out(out) class Transformer(nn.Module): def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.): super().__init__() self.layers = nn.ModuleList([]) for _ in range(depth): self.layers.append(nn.ModuleList([ PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)) ])) def forward(self, x): for attn, ff in self.layers: x = attn(x) + x x = ff(x) + x return x 最后提取可学习的嵌入 cls token 对应的特征用于模型输出。\n常见问题 ViT是为了解决什么问题？\n处理大规模图像数据。传统 CNN 在面对大规模数据集时面临着计算和内存方面的挑战。Transformer 高度并行化，适合大规模数据集训练。 跨越空间信息。传统 CNN 使用局部感受野的卷积操作，无法充分捕捉全局语义信息。ViT 能够解决这个问题。 Transformer 为什么用 LayerNorm 不用 BatchNorm？为什么 ViT 定长也用 LayerNorm 不用 BatchNorm？\nBN 是对一个 Batch 内的不同样本的同意特征去做归一化操作，LN 是对单个样本的不同特征做操作，不收样本数的限制。在 NLP 中，每个样本的长度可能不同，无法组合成一个批次进行处理。 LN 是在样本内计算均值方差，这与自注意力机制的计算一致。而 BN 是在一个批次内跨样本计算均值方差，可能会破坏数据原有的数据分布。 为什么 $QK^T$ 除以 $\\sqrt{n}$ 而不是 n?\n点积结果需要除以一个数防止 softmax 结果过大使得梯度趋于 0。假设 Q 和 K 每一维都满足 0 均值 1 方差的分布，那么 Q*K 的方差为 n，除以 $\\sqrt{n}$ 使结果满足方差为 1 的分布，达到归一化的效果。$aX\\sim(0,a^2),X\\sim(0,1)$\n自注意力计算复杂度为 $O(n^2)$，怎么优化？\n[Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection]\n一个元素可能只与部分元素有关，但标准自注意力会给每个元素都分配权重，可通过 top-k 显示稀疏化，只关注部分元素。\n如下图，最左边为标准自注意力计算流程，中间为稀疏的流程，最右边为执行示意图。简而言之，在 Softmax 之前，先通过 top-k 选择少数重要的元素，将其他元素设置为负无穷。通过这种操作可以使注意力更加集中。\n位置编码的作用是什么\n自注意力计算是无向的，若没有位置编码，模型则无法知道 token 各自的位置信息。\n位置编码可以使模型在注意力机制中考虑到输入的绝对和相对位置信息，从而能够更好地捕捉到序列数据中的顺序关系。\ncls token 为什么能用来分类？还有哪些方法来进行最后的分类？\ncls token 随机初始化，并随着网络的训练不断更新，它能够编码整个数据集的统计特性； cls token 可以对全局特征进行聚合，并且不基于图像内容，避免对某一 token 有倾向性； cls token 固定位置为 0，能够避免输出受到位置编码的干扰； 除了将 cls token 用于最后 MLP 分类的输入，还可以对输出特征进行池化或直接将所有特征输出 MLP 分类层。\n",
  "wordCount" : "2692",
  "inLanguage": "en",
  "image":"https://Achilles-10.github.io/posts/tech/vit/vit.png","datePublished": "2023-07-25T19:21:03+08:00",
  "dateModified": "2023-07-25T19:21:03+08:00",
  "author":[{
    "@type": "Person",
    "name": "Achilles"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Achilles-10.github.io/posts/tech/vit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "烈烈风中、的博客",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Achilles-10.github.io/img/profile.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Achilles-10.github.io" accesskey="h" title="烈烈风中、的个人博客 (Alt + H)">
            <img src="https://Achilles-10.github.io/img/profile.jpg" alt="logo" aria-label="logo"
                 height="36">烈烈风中、的个人博客</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Achilles-10.github.io/" title="🏠 主页">
                <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://Achilles-10.github.io/posts" title="📚 文章">
                <span>📚 文章</span>
                </a>
            </li>
            <li>
                <a href="https://Achilles-10.github.io/tags" title="🧩 标签">
                <span>🧩 标签</span>
                </a>
            </li>
            <li>
                <a href="https://Achilles-10.github.io/archives/" title="⏱️ 时间轴">
                <span>⏱️ 时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://Achilles-10.github.io/about" title="🙋🏻‍♂️ 关于">
                <span>🙋🏻‍♂️ 关于</span>
                </a>
            </li>
            <li>
                <a href="https://Achilles-10.github.io/search" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                <span>🔍 搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://Achilles-10.github.io">🏠 主页</a>&nbsp;»&nbsp;<a href="https://Achilles-10.github.io/posts/">📚 文章</a>&nbsp;»&nbsp;<a href="https://Achilles-10.github.io/posts/tech/">👨🏻‍💻 技术</a></div>
            <h1 class="post-title">
                速通 Vision Transformer (ViT)
            </h1>
            <div class="post-description">
                图解 ViT 网络结构与计算，以及面试常见问题
            </div>
            <div class="post-meta"><style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2023 年 7 月 25 日
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>2692字
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>6分钟
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Achilles
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://Achilles-10.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="color: var(--secondary)!important;">学习笔记</a>
                &nbsp;<a href="https://Achilles-10.github.io/tags/%E6%A8%A1%E5%9E%8B/" style="color: var(--secondary)!important;">模型</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo//twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://Achilles-10.github.io"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> 
<figure class="entry-cover1"><img style="zoom:;" loading="lazy" src="https://Achilles-10.github.io/posts/tech/vit/vit.png" alt="">
    
</figure><aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">文章目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%bc%95%e8%a8%80" aria-label="引言">引言</a></li>
                <li>
                    <a href="#%e6%96%b9%e6%b3%95" aria-label="方法">方法</a><ul>
                        
                <li>
                    <a href="#%e5%9b%be%e5%83%8f%e5%9d%97%e5%b5%8c%e5%85%a5patch-embeddings" aria-label="图像块嵌入（Patch Embeddings）">图像块嵌入（Patch Embeddings）</a></li>
                <li>
                    <a href="#%e5%8f%af%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%b5%8c%e5%85%a5learnable-embedding" aria-label="可学习的嵌入（Learnable Embedding）">可学习的嵌入（Learnable Embedding）</a></li>
                <li>
                    <a href="#%e4%bd%8d%e7%bd%ae%e5%b5%8c%e5%85%a5position-embeddings" aria-label="位置嵌入（Position Embeddings）">位置嵌入（Position Embeddings）</a></li>
                <li>
                    <a href="#transformer-%e7%bc%96%e7%a0%81%e5%99%a8" aria-label="Transformer 编码器">Transformer 编码器</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98" aria-label="常见问题">常见问题</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h2 id="引言">引言<a hidden class="anchor" aria-hidden="true" href="#引言">#</a></h2>
<p>将图像拆分为<strong>块（patch）</strong>，并将这些图像块的<strong>线性嵌入序列</strong>（patch token）作为 Transformer 输入。</p>
<p>当在没有强正则化的中型数据集（如 ImageNet）上进行训练时，这些模型产生的准确率比同等大小的 ResNet 低几个百分点。 这种看似令人沮丧的结果可能是意料之中的：Transformers 缺乏 CNN 固有的一些<code>归纳偏置 (inductive biases)</code> ，如<strong>平移等效性和局部性</strong> （translation equivariance and locality），因此在<strong>数据量不足</strong>时，训练不能很好地泛化。</p>
<p>但若在更大的数据集（14M-300M图像）上训练，情况就会发生变化，我们发现<strong>大规模训练胜过归纳偏置</strong>。</p>
<blockquote>
<p>归纳偏置：学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合</p>
<p>例如，<strong>深度神经网络</strong>偏好性地认为，层次化处理信息有更好效果；<strong>卷积神经网络</strong>认为信息具有空间局部性，可用滑动卷积共享权重的方式降低参数空间；<strong>循环神经网络</strong>则将时序信息纳入考虑，强调顺序重要性；<strong>图网络</strong>则认为中心节点与邻居节点的相似性会更好地引导信息流动。</p>
</blockquote>
<blockquote>
<p>CNN 平移不变性：卷积+最大池化约等于平移不变性</p>
<p>如下面两图，输入图像的左下角有一个人脸，经过卷积，人脸的特征（眼睛，鼻子）也位于特征图的左下角。假如人脸特征在图像的左上角，那么卷积后对应的特征也在特征图的左上角。</p>
<p>最大池化，它返回感受野中的最大值，如果最大值被移动了，但是仍然在这个感受野中，那么池化层也仍然会输出相同的最大值。</p>
<div align=center><img src="trans1.png" style="zoom:40%"/></div>
<div align=center><img src="trans2.png" style="zoom:40%"/></div>
</blockquote>
<blockquote>
<p>CNN 局部性：如果我们想要识别出与物体相对应的图案，如天空中的一架飞机，我们可能需要看看附近的像素是如何排列的，但我们对那些彼此相距很远的组合的像素是如何出现的并不那么感兴趣。</p>
</blockquote>
<h2 id="方法">方法<a hidden class="anchor" aria-hidden="true" href="#方法">#</a></h2>
<h3 id="图像块嵌入patch-embeddings">图像块嵌入（Patch Embeddings）<a hidden class="anchor" aria-hidden="true" href="#图像块嵌入patch-embeddings">#</a></h3>
<p>图像 $X\in\mathbb{R}^{H\times W\times C}$经过分块得到 2D patch 序列 $x_p\in\mathbb{R}^{N\times(P^2\cdot C)}$，其中 $N=\frac{H\cdot W}{P^2}$ 为 ViT 有效输入序列长度。</p>
<p>ViT 在所有层中使用相同维度的隐向量大小 D，将 $x_p$经过 FC 层将 $P^2\cdot C$ 映射到 D，同时 N 保持不变。</p>
<p>上述即为图像块嵌入，本质即为对每一个图像块向量做一个线性变换，将其映射到 D 维，得到 $x_pE\in\mathbb{R}^{N\times D}$。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_patches <span style="color:#f92672">=</span> (image_height <span style="color:#f92672">//</span> patch_height) <span style="color:#f92672">*</span> (image_width <span style="color:#f92672">//</span> patch_width)
</span></span><span style="display:flex;"><span>patch_dim <span style="color:#f92672">=</span> channels <span style="color:#f92672">*</span> patch_height <span style="color:#f92672">*</span> patch_width
</span></span><span style="display:flex;"><span>to_patch_embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            Rearrange(<span style="color:#e6db74">&#39;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#39;</span>, p1 <span style="color:#f92672">=</span> patch_height, p2 <span style="color:#f92672">=</span> patch_width),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LayerNorm(patch_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(patch_dim, dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LayerNorm(dim),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> to_patch_embedding(x)
</span></span></code></pre></div><h3 id="可学习的嵌入learnable-embedding">可学习的嵌入（Learnable Embedding）<a hidden class="anchor" aria-hidden="true" href="#可学习的嵌入learnable-embedding">#</a></h3>
<p>为图像块嵌入序列手动预设一个可学习的嵌入 cls token，最后取追加的 cls token 作为输出。最终输入 ViT 的嵌入向量总长度为 N+1，cls token 在训练时随机初始化。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cls_token <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, dim))
</span></span><span style="display:flex;"><span>cls_tokens <span style="color:#f92672">=</span> repeat(cls_token, <span style="color:#e6db74">&#39;1 1 d -&gt; b 1 d&#39;</span>, b <span style="color:#f92672">=</span> b)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((cls_tokens, x), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><h3 id="位置嵌入position-embeddings">位置嵌入（Position Embeddings）<a hidden class="anchor" aria-hidden="true" href="#位置嵌入position-embeddings">#</a></h3>
<p>位置嵌入 $E_{pos}\in\mathbb{R}^{(N+1)\times D}$被加入图像块嵌入以保留输入图像块之间的空间位置关系，这是由于自注意力的<strong>扰动不变性</strong>（Permutation-invariant），即打乱 Sequence 中 tokens 的顺序并不会改变结果。若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。</p>
<p>论文中比较了以下几种不同的位置编码方案：</p>
<ol>
<li>无位置嵌入</li>
<li>1-D 位置编码嵌入：将 2D 图像块视作 1D 序列</li>
<li>2-D 位置编码嵌入：考虑图像块 2-D 位置 (x,y)</li>
<li>相对位置编码嵌入：考虑图像块的相对位置</li>
</ol>
<p>其中无位置嵌入效果很差，其他效果接近。Transformer 中采用固定位置编码，ViT 采用标准可学习的 1-D 位置编码嵌入。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pos_embedding <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1</span>, num_patches <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, dim))
</span></span><span style="display:flex;"><span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(emb_dropout)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>pos_embedding[:, :(n <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)]
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
</span></span></code></pre></div><p>最终输入：
$$
x_0=[x_{cls};x_p^1E;x_p^2E;\dots;x_p^NE]+E_{pos},\quad x_0\in\mathbb{R}^{(N+1)\times D}
$$</p>
<h3 id="transformer-编码器">Transformer 编码器<a hidden class="anchor" aria-hidden="true" href="#transformer-编码器">#</a></h3>
<div align=center><img src="encoder.png" style="zoom:50%"/></div>
<p>Transformer 编码器由交替的多头自注意力层和多层感知机块构成，每个块前应用 LayerNorm，每个块后残差连接。</p>
<p>多头自注意力（MSA）机制中的 QKV 矩阵由嵌入向量 X 与三个不同权重的矩阵相乘得到。</p>
<div align=center><img src="qkvw.png" style="zoom:40%"/></div>
<p>Q 与 K 转置相乘，然后使用 Softmax 计算每一个单词对于其他单词的注意力系数，公式中的 Softmax 是对每一行计算，即每一行的和为 1。</p>
<div align=center><img src="softmax.png" style="zoom:35%"/></div>
<p>将 Softmax 矩阵与 V 矩阵相乘得到输出 $Z_i$，最后将每一头的输出 Z 拼接在一起，经过一个线性层得到最终输出 Z。</p>
<div align=center><img src="v.png" style="zoom:30%"/></div>
<div align=center><img src="mhaout.png" style="zoom:30%"/></div>
<p>公式表达如下：
$$
\begin{align*}
&amp;Q,K,V=MatMul(X,(W^Q,W^K,W^V))\\
&amp;\text{head}_i=\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})\cdot V\\
&amp;\text{MultiHead}(Q,K,V)=Cat(\text{head}_1,\text{head}_2,\dots,\text{head}_h)\cdot W^o
\end{align*}
$$</p>
<div align=center><img src="msa.png" style="zoom:50%"/></div>
<p>MSA 后面跟一个多层感知机（MLP），将特征维度先增大再恢复，其中使用 GELU 激活函数。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PreNorm</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, fn):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fn <span style="color:#f92672">=</span> fn
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>fn(self<span style="color:#f92672">.</span>norm(x), <span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FeedForward</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, hidden_dim, dropout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(dim, hidden_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>GELU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(dropout),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>net(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Attention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, heads <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>, dim_head <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, dropout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        inner_dim <span style="color:#f92672">=</span> dim_head <span style="color:#f92672">*</span>  heads
</span></span><span style="display:flex;"><span>        project_out <span style="color:#f92672">=</span> <span style="color:#f92672">not</span> (heads <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">and</span> dim_head <span style="color:#f92672">==</span> dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>heads <span style="color:#f92672">=</span> heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scale <span style="color:#f92672">=</span> dim_head <span style="color:#f92672">**</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attend <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Softmax(dim <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>to_qkv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(dim, inner_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">3</span>, bias <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>to_out <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(inner_dim, dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">if</span> project_out <span style="color:#66d9ef">else</span> nn<span style="color:#f92672">.</span>Identity()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        qkv <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>to_qkv(x)<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">3</span>, dim <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        q, k, v <span style="color:#f92672">=</span> map(<span style="color:#66d9ef">lambda</span> t: rearrange(t, <span style="color:#e6db74">&#39;b n (h d) -&gt; b h n d&#39;</span>, h <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>heads), qkv)
</span></span><span style="display:flex;"><span>        dots <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(q, k<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>scale
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attend(dots)
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(attn)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(attn, v)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> rearrange(out, <span style="color:#e6db74">&#39;b h n d -&gt; b n (h d)&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>to_out(out)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Transformer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(depth):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>                PreNorm(dim, Attention(dim, heads <span style="color:#f92672">=</span> heads, dim_head <span style="color:#f92672">=</span> dim_head, dropout <span style="color:#f92672">=</span> dropout)),
</span></span><span style="display:flex;"><span>                PreNorm(dim, FeedForward(dim, mlp_dim, dropout <span style="color:#f92672">=</span> dropout))
</span></span><span style="display:flex;"><span>            ]))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> attn, ff <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> attn(x) <span style="color:#f92672">+</span> x
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> ff(x) <span style="color:#f92672">+</span> x
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>最后提取可学习的嵌入 cls token 对应的特征用于模型输出。</p>
<h2 id="常见问题">常见问题<a hidden class="anchor" aria-hidden="true" href="#常见问题">#</a></h2>
<ul>
<li>
<p>ViT是为了解决什么问题？</p>
<ol>
<li>处理大规模图像数据。传统 CNN 在面对大规模数据集时面临着计算和内存方面的挑战。Transformer 高度并行化，适合大规模数据集训练。</li>
<li>跨越空间信息。传统 CNN 使用局部感受野的卷积操作，无法充分捕捉全局语义信息。ViT 能够解决这个问题。</li>
</ol>
</li>
<li>
<p>Transformer 为什么用 LayerNorm 不用 BatchNorm？为什么 ViT 定长也用 LayerNorm 不用 BatchNorm？</p>
<ul>
<li>BN 是对一个 Batch 内的不同样本的同意特征去做归一化操作，LN 是对单个样本的不同特征做操作，不收样本数的限制。在 NLP 中，每个样本的长度可能不同，无法组合成一个批次进行处理。</li>
<li>LN 是在样本内计算均值方差，这与自注意力机制的计算一致。而 BN 是在一个批次内跨样本计算均值方差，可能会破坏数据原有的数据分布。</li>
</ul>
</li>
<li>
<p>为什么 $QK^T$ 除以 $\sqrt{n}$ 而不是 n?</p>
<p>点积结果需要除以一个数防止 softmax 结果过大使得梯度趋于 0。假设 Q 和 K 每一维都满足 0 均值 1 方差的分布，那么 Q*K 的方差为 n，除以 $\sqrt{n}$ 使结果满足方差为 1 的分布，达到归一化的效果。$aX\sim(0,a^2),X\sim(0,1)$</p>
<div align=center><img src="qkv.png" style="zoom:40%"/></div>
</li>
<li>
<p>自注意力计算复杂度为 $O(n^2)$，怎么优化？</p>
<p><a href="https://arxiv.org/abs/1912.11637">[Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection]</a></p>
<p>一个元素可能只与部分元素有关，但标准自注意力会给每个元素都分配权重，可通过 top-k 显示稀疏化，只关注部分元素。</p>
<p>如下图，最左边为标准自注意力计算流程，中间为稀疏的流程，最右边为执行示意图。简而言之，在 Softmax 之前，先通过 top-k 选择少数重要的元素，将其他元素设置为负无穷。通过这种操作可以使注意力更加集中。</p>
<div align=center><img src="sparse.png" style="zoom:30%"/></div>
</li>
<li>
<p>位置编码的作用是什么</p>
<p>自注意力计算是无向的，若没有位置编码，模型则无法知道 token 各自的位置信息。</p>
<p>位置编码可以使模型在注意力机制中考虑到输入的绝对和相对位置信息，从而能够更好地捕捉到序列数据中的顺序关系。</p>
</li>
<li>
<p>cls token 为什么能用来分类？还有哪些方法来进行最后的分类？</p>
<ul>
<li>cls token 随机初始化，并随着网络的训练不断更新，它能够编码整个数据集的统计特性；</li>
<li>cls token 可以对全局特征进行聚合，并且不基于图像内容，避免对某一 token 有倾向性；</li>
<li>cls token 固定位置为 0，能够避免输出受到位置编码的干扰；</li>
</ul>
<p>除了将 cls token 用于最后 MLP 分类的输入，还可以对输出特征进行池化或直接将所有特征输出 MLP 分类层。</p>
</li>
</ul>


        </div>

        

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://Achilles-10.github.io/posts/tech/interview1/">
    <span class="title">« 上一页</span>
    <br>
    <span>百面深度学习：卷积神经网络</span>
  </a>
  <a class="next" href="https://Achilles-10.github.io/posts/tech/kd/">
    <span class="title">下一页 »</span>
    <br>
    <span>知识蒸馏速览</span>
  </a>
</nav>

        </footer>
    </div><div class="comments">
    <script>
    function loadComment() {
        let theme = localStorage.getItem('pref-theme') === 'dark' ? 'dark' : 'light';
        let s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'Achilles-10\/Achilles-10.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOJODJBA');
        s.setAttribute('data-category', 'Announcements');
        s.setAttribute('data-category-id', 'DIC_kwDOJODJBM4CVIpZ');
        s.setAttribute('data-mapping', 'title');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '1');
        s.setAttribute('data-input-position', 'top');
        s.setAttribute('data-lang', 'zh-CN');
        s.setAttribute('data-theme', theme);
        s.setAttribute('crossorigin', 'anonymous');
        
        s.setAttribute('async', '');
        document.querySelector('div.comments').innerHTML = '';
        document.querySelector('div.comments').appendChild(s);
    }
    loadComment();
    </script>
</div>

</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        -2023
        <a href="https://Achilles-10.github.io" style="color:#939393;">烈烈风中、的博客</a>
        All Rights Reserved
    </span>
    
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        let theme = 'light';
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
        } else {
            document.body.classList.add('dark');
            theme = 'dark';
            }
        localStorage.setItem("pref-theme", theme);
        const message = {'giscus': {'setConfig': {'theme': theme}}};
        const iframe = document.querySelector('iframe.giscus-frame');
        iframe.contentWindow.postMessage(message, 'https://giscus.app');
    })
</script>



<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"烈烈风中、的博客"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"烈烈风中、的博客"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '📄复制';

        function copyingDone() {
            copybutton.innerText = '👌🏻已复制!';
            setTimeout(() => {
                copybutton.innerText = '📄复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\n————————————————\r\n' +
                    '版权声明：本文为「'+"烈烈风中、的博客"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>

<script>
    $("code[class^=language] ").on("mouseover", function () {
        if (this.clientWidth < this.scrollWidth) {
            $(this).css("width", "135%")
            $(this).css("border-top-right-radius", "var(--radius)")
        }
    }).on("mouseout", function () {
        $(this).css("width", "100%")
        $(this).css("border-top-right-radius", "unset")
    })
</script>
</body>

</html>
